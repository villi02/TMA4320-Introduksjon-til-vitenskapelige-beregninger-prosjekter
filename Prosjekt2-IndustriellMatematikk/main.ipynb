{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppgave 1 - Forstå hvordan datasettene og transformermodellen er strukturert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1\n",
    "Gi et eksempel (som i likning (10)) på hvordan et datasett {x, y} ville sett ut for å trene en transformermodell for å predikere et heltall d gitt d = a · b + c der a, c\n",
    "er tosifrede heltall, mens b er et ettsifret heltall, altså 9 ≥ b ∈ Z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et sett av treningsdata kan genereres ved å la x bestå av sifrene i $a, b, c$ og alle sifrene i $d$ med unntak av det siste og lar $y = d$. Dermed vil formen for x være gitt ved x = [$a_0$ , ..., $a_{r-1}$, $b_0$, ..., $b_{r-1}$, $c_0$, ..., $c_{r-1}$, $d_0$, ..., $d_{r-1}$]. Gitt betingelsene i oppgaven over, la $r$ = 2, $a$ = 24, $b$ = 4, $c$ = 15 og dermed <br> $d$ = 111.  som gir oss x = $[2, 4, 4, 1, 5, 1, 1]$ og $y = [1, 1, 1]$. Merk at siste siffer i $d$ ikke er del av datasettet i x.  Modellen skal da gi $\\hat{z}$. Lengden av $\\hat{z}$, $n$, vil være gitt av lengden av x som har med lengden $n$. $\\hat{z}$ = [$\\hat{z}_0$, ..., $\\hat{z}_5$] =  $f_{\\theta}([2, 4, 4, 1, 5, 1, 1])$. Ideelt er $\\theta$ optimert til en slik grad at <Br> $\\hat{y} = [\\hat{z}_3, \\hat{z}_4, \\hat{z}_5] = [1, 1, 1] = y$ er korrekt predikert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy er gitt ved $L(θ, D) = -\\frac{1}{D \\cdot n} \\sum_{i=0}^{D-1} \\sum_{j=0}^{n-1} \\log \\hat{Y}_{k,j}^{(i)}$ hvor D er datapunktene, $\\theta $\n",
    "er parameterne, og $\\hat{Y}$ er sannsynlighetsfordelingen til den predikterte modellen, samt er $j$ og $i$ dimensjonene til $\\hat{Y}$. Det objektfunksjonen gjør er å sammenligne onehot(y) med $\\hat{Y}$. Hvis $L(θ, D) = 0$ vil den optimerte modellen og onehot(y) være identiske. Når dette inntreffer vil $argmax_{\\text{col}}(\\hat{Y})$ = $\\hat{y}$ som igjen er lik $y$. I dette tilfellet er $y = [4,3,2,1]$, som også vil være lik $\\hat{y}$.\n",
    "$\\hat{Y}$ vil være gitt av den diskrete sannsynlighetsfordelingen: $\\hat{Y}$ =\n",
    "$\\left[\\begin{array}{ccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 0 & 0\\\\\n",
    "\\end{array}\\right]$, som er lik onehot([4,3,2,1]). Dette betyr i praksis at paramtetrene i transformenmodellen klarer å prediktere hva som kommer videre i sekvensen og vi ender opp med samme antatt løsning ($\\hat{y}$) som faktisk løsning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med gitte variabler for $d, m, n_{max}, k, p$ og  $L$ er antall enkeltparametre mulig å bestemme. Enkeltparametre er gitt med $w \\in \\mathbb{R}$ noe som uttrykkes med å se på dimensjoner for ulike parametermatriser i transformermodellen.\n",
    "\n",
    "$W_E$ og $W_P$ har henholdsvis dimensjonene $W_E \\in \\mathbb{R}^{d \\times m}$ og $ W_P \\in \\mathbb{R}^{d \\times n_{max}} $ som representerer parametermatrisen til en sekvens for x med lengde n, som skrives som $z_0$. I tilegg ønskes det å gjøre $L$ paramtriserte trasformasjoner på $z_0$, så man ender opp med $L \\cdot (d \\times m + d \\times n_{max})$ for embedding delen av enkeltparamtrene. Under unenbeddingen oppstår en ny parametermatrise $W_U$ som er en sekvens med lengde $n$ med heltall opp til $m$, den har dimensjonene $ W_U \\in \\mathbb{R}^{d \\times m} $. Attention-lag bidrar også til antall enkeltparamtre for transformmodellen, der har man 4 parametermatriser; $W_O, W_V, W_Q, W_K$ alle med samme dimensjon $\\mathbb{R}^{k \\times d} $. Transformermodellen har også en $feed$-$forward $ del som bidrar med to paramtermatriser $W_1$ og $W_2$ begge med dimensjoner $\\mathbb{R}^{p \\times d} $\n",
    "\n",
    "\n",
    "Hvis man tar disse parametermatrisene i betrakning og antar at $k < d < p$ vil man ha: \n",
    "$w = d \\times m+L\\cdot (d \\times m + d \\times n_{max}) + 4 \\cdot k \\times d + 2 \\cdot p \\times d $, enkeltparametre. (siden k og p er heltall man bestemmer selv er dette en rimelig antagelse å ta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L=n = n_{max}= x = 1$ og $m=d=k=p=d = 2$ og alle parametermatrisene lik \n",
    "$\\left[\\begin{array}{ccc}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{array}\\right]$ = $ I_{2\\times 2}$ utenom $W_E$ som er $\\left[\\begin{array}{ccc}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{array}\\right]$ og $W_P$ = $\\left[\\begin{array}{ccc}\n",
    "1 \\\\\n",
    "0 \n",
    "\\end{array}\\right]$\n",
    "\n",
    "Med dette oppgitt vil  $ X = onehot(x) = \\left[\\begin{array}{ccc}0 \\\\1 \n",
    "\\end{array}\\right]$ som resulterer i en $z_0 = \\left[\\begin{array}{ccc}0 \\\\ \\alpha \n",
    "\\end{array}\\right]+ \\left[\\begin{array}{ccc}1 \\\\ 0 \n",
    "\\end{array}\\right]$ =$\\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$. For å videre bestemme et uttrykk for $\\hat{z}$ må vi se på hva transformermodellen gjør med $z_0$. videre er \n",
    "\n",
    "$z_{1/2}$ = $z_0 + W_O^T  W_V  z_0 A(z_0)$ hvor $A(z_0)$ = $softmax_{col}(z_0^T W_Q^T W_K z_0+D)$ hvor D sørger for at den strengt nedre delen av A er 0.\n",
    "Ved å løse $A(z_0)$ får man utrykket $(1+ \\alpha ^2)$ i softmax funksjonen.\n",
    "\n",
    "$z_{1/2} = \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right] + I_{2 \\times 2} I_{2 \\times 2} \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right] softmax(1+ \\alpha ^2)$ = $ 2 \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$ fordi softmax av et utrykk blir å dele på seg selv i e-potens som blir 1.\n",
    "\n",
    "for $z_1$ får vi et uttrykk som er $z_{1/2} + W_2^T \\sigma (W_1 z_{1/2})$, $\\sigma$ er en aktiveringsfunskjon, i dette tilfelle kan man bruke $relu(W_1 z_{1/2})$.\n",
    "Utrykket blir da:\n",
    "\n",
    "$z_1 = 2  \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]+ I_{2 \\times 2} max(0,I_{2 \\times 2} 2 \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]) $ = $ 4  \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$\n",
    " \n",
    "Ved hjelp av $z_1$ kan man ta i bruk likning (8) for å finne sannsynlighetsfunksjonen $Z$. $Z = softmax_{col}(W_U^T Z_1)$, argumentet $W_U^T z_1$ blir lik $z_1$ og softmax gir oss $Z = \\frac{1}{e^4 + e^{4 \\alpha}} \\left[\\begin{array}{ccc}e^4 \\\\ e^{4 \\alpha }\n",
    "\\end{array}\\right]$\n",
    "\n",
    "for å få $\\hat{z} = [1]$ må $argmax(Z)$ bli 1, og dette krever at verdien på indeks [1] må være større enn den på indeks [0], da må $e^4 < e^{4 \\alpha}$ og dette impliserer at $\\alpha >1$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppgave 2 - Objektorientert programmering for transformermodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I layers classen fungerer layers som en base klasse for alle andre typer layers i nettverket. Her implementeres en basis versjon av metoder som forward(), backward() og step_gd(). Dette vil si at alle layers som arver fra Layers base klassen implementerer eller overskriver disse metodene. Dette vil si at hvis vi vet at et objekt arver fra Layers klassen så vet vi at dette objektet har metodene forward(), backward() og step_gd(). Denne strukturen tillater også at et lag har sin egen spesifisert step_gd() med at det kan overskrive metoden til å være mest hensiktsmessig for det spesifikke laget, dette gjør at neural_network kan operere på et høyere abstraksjonsnivå og kan implementere metoder som step_gd() uten å trenge å vite de spesifikke detaljene til hvert lag. \n",
    "\n",
    "Mer spesifikt her bruker neural_network arv til å kunne behandle alle sine layers på samme måte, selv om de kan ha forskjellig implementerte step_gd() metoder. Polymorfisme lar da neural_network kalle samme funksjon (step_gd()) på samme måte for hvert lag uten å vite hvilken subklasse hvert layer tilhører."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
