{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "from data_generators import get_train_test_sorting\n",
    "from data_generators import get_train_test_addition\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppgave 1 - Forstå hvordan datasettene og transformermodellen er strukturert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.1 Gi et eksempel (som i likning $(10)$) på hvordan et datasett ${x, y}$ ville sett ut for å trene en transformermodell for å predikere et heltall $d$ gitt $d = a · b + c$ der $a, c$ er tosifrede heltall, mens $b$ er et ettsifret heltall, altså $9 ≥ b ∈ Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et sett av treningsdata kan genereres ved å la x bestå av sifrene i $a, b, c$ og alle sifrene i $d$ med unntak av det siste og lar $y = d$. Dermed vil formen for x være gitt ved $x = [a_0 , \\cdot \\cdot \\cdot, a_{r-1}, b_0, \\cdot \\cdot \\cdot, b_{r-1}, c_0, \\cdot \\cdot \\cdot, c_{r-1}, d_0, \\cdot \\cdot \\cdot, d_{r-1}]$. Gitt betingelsene i oppgaven over, la $r$ = 2, $a$ = 24, $b$ = 4, $c$ = 15 og dermed <br> $d$ = 111.  som gir oss x = $[2, 4, 4, 1, 5, 1, 1]$ og $y = [1, 1, 1]$. Merk at siste siffer i $d$ ikke er del av datasettet i x.  Modellen skal da gi $\\hat{z}$. Lengden av $\\hat{z}$, $n$, vil være gitt av lengden av x som har med lengden $n$. $\\hat{z}$ = [$\\hat{z}_0$, \\cdot \\cdot \\cdot, $\\hat{z}_5$] =  $f_{\\theta}([2, 4, 4, 1, 5, 1, 1])$. Ideelt er $\\theta$ optimert til en slik grad at <Br> $\\hat{y} = [\\hat{z}_3, \\hat{z}_4, \\hat{z}_5] = [1, 1, 1] = y$ er korrekt predikert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.2) Når optimeringen er ferdig, hvordan kan vi bruke modellen $f_{\\theta}$  til å predikere $d$ gitt $a, b, c$? Vis dette med et eksempel, på samme måte som i likning $(11)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gitt at optmeringen er ferdig, kan modellen korrekt predikere $d$. Denne prossesen av å predikere de neste sifferene i seqvensen gjøres fra å lære av de oppgitte datasettene. Følgende tabell viser hvordan dette fungerer. La verdiene være det samme som i forrige oppgave, $r = 2, a = 24, b = 4,$<Br> $c = 15$\n",
    "\n",
    "| Iterasjoner | Modell |\n",
    "|----------|----------|\n",
    "| $x^{(0)} = [2, 4, 0, 4, 1, 5]$ | $[\\hat{z}_0^{(0)}, \\hat{z}_1^{(0)}, \\hat{z}_2^{(0)}, \\hat{z}_3^{(0)}] = f_{\\theta}(x^{(0)})$|\n",
    "| $x^{(1)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}]$ | $[\\hat{z}_0^{(1)}, \\cdot \\cdot \\cdot, \\hat{z}_4^{(1)}] = f_{\\theta}(x^{(1)})$ |\n",
    "| $x^{(2)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}]$ | $[\\hat{z}_0^{(2)}, \\cdot \\cdot \\cdot, \\hat{z}_5^{(2)}] = f_{\\theta}(x^{(2)})$  |\n",
    "| $x^{(3)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}, \\hat{z}_5^{(2)}]$ |  |\n",
    "\n",
    "Disse predikasjonene hentes ut og returneres som $\\hat{y} = [\\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}, \\hat{z}_5^{(2)}]$ som bør være likt $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.3) Anta at vi bruker cross-entropy som objektfunksjon, at $m = 5$ og $y = [4, 3, 2, 1]$. Hvilke diskret sannsynlighetsfordeling $\\hat{Y}$ ville gitt en objektfunksjon $L(θ, D) = 0$? Hva ville $\\hat{y}$ vært i dette tilfellet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy er gitt ved $L(θ, D) = -\\frac{1}{D \\cdot n} \\sum_{i=0}^{D-1} \\sum_{j=0}^{n-1} \\log \\hat{Y}_{k,j}^{(i)}$ hvor $D$ er datapunktene, $\\theta $\n",
    "er parameterne, og $\\hat{Y}$ er sannsynlighetsfordelingen til den predikterte modellen, samt er $j$ og $i$ dimensjonene til $\\hat{Y}$. Det objektfunksjonen gjør er å sammenligne onehot(y) med $\\hat{Y}$. Hvis $L(θ, D) = 0$ vil den optimerte modellen og onehot(y) være identiske. Når dette inntreffer vil $argmax_{\\text{col}}(\\hat{Y})$ = $\\hat{y}$ som igjen er lik $y$. I dette tilfellet er $y = [4,3,2,1]$, som også vil være lik $\\hat{y}$.\n",
    "$\\hat{Y}$ vil være gitt av den diskrete sannsynlighetsfordelingen:<Br><Br> $\\hat{Y}$ =\n",
    "$\\left[\\begin{array}{ccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 0 & 0\\\\\n",
    "\\end{array}\\right]$ , <Br><Br> som er lik onehot($[4,3,2,1]$). Dette betyr i praksis at paramtetrene i transformenmodellen klarer å prediktere hva som kommer videre i sekvensen og vi ender opp med samme antatt løsning ($\\hat{y}$) som faktisk løsning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.4) Gitt $d, m, n_{max}, k, p$ og $L$. Hvor mange enkeltparametre har en transformermodell? Med enkeltparametre mener vi hvor mange tall $w ∈ R$ vi må bestemme ved optimering. En matrise $W ∈ R^{m×n}$ består av $m · n$ tall eller enkeltparametre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med gitte variabler for $d, m, n_{max}, k, p$ og  $L$ er antall enkeltparametre mulig å bestemme. Enkeltparametre er gitt med $w \\in \\mathbb{R}$ noe som uttrykkes med å se på dimensjoner for ulike parametermatriser i transformermodellen.\n",
    "\n",
    "$W_E$ og $W_P$ har henholdsvis dimensjonene $W_E \\in \\mathbb{R}^{d \\times m}$ og $ W_P \\in \\mathbb{R}^{d \\times n_{max}} $ som representerer parametermatrisen til en sekvens for x med lengde n, som skrives som $z_0$. I tilegg ønskes det å gjøre $L$ paramtriserte trasformasjoner på $z_0$, så man ender opp med $L \\cdot (d \\times m + d \\times n_{max})$ for embedding delen av enkeltparamtrene. Under unenbeddingen oppstår en ny parametermatrise $W_U$ som er en sekvens med lengde $n$ med heltall opp til $m$, den har dimensjonene $ W_U \\in \\mathbb{R}^{d \\times m} $. Attention-lag bidrar også til antall enkeltparamtre for transformmodellen, der har man 4 parametermatriser; $W_O, W_V, W_Q, W_K$ alle med samme dimensjon $\\mathbb{R}^{k \\times d} $. Transformermodellen har også en $feed$-$forward $ del som bidrar med to paramtermatriser $W_1$ og $W_2$ begge med dimensjoner $\\mathbb{R}^{p \\times d} $\n",
    "\n",
    "\n",
    "Hvis man tar disse parametermatrisene i betrakning og antar at $k < d < p$ vil man ha: \n",
    "$w = d \\times m+L\\cdot (d \\times m + d \\times n_{max}) + 4 \\cdot k \\times d + 2 \\cdot p \\times d $, enkeltparametre. (siden k og p er heltall man bestemmer selv er dette en rimelig antagelse å ta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.5 Transformermodellen er gitt i likningene $(4) - (9)$. La $n = n_{max} = 1,$  $m = d = k = p=2$ og $L=1$. Anta videre at $W_O = W_V = W_Q = W_K = W_1 = W_2 = W_U = I_{2×2}$ og at $σ(x) = Relu(x) = max(0, x)$. Dersom <Br> $W_E = \\left[\\begin{array}{ccc} 1 & 0 \\\\ 0 & \\alpha \\end{array} \\right]$ , og $W_P$ = $\\left[\\begin{array}{ccc} 1 \\\\ 0 \\end{array} \\right]$ vis at vi må ha $\\alpha > 1$ for å få  $\\hat{z} = [1]$ som output når input er $x = [1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med variablene oppgitt i oppgaven $L=n = n_{max}= x = 1$ og $m=d=k=p=d = 2$ og alle parametermatrisene lik\n",
    "\n",
    "$\\left[\\begin{array}{ccc}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{array}\\right]$ = $ I_{2\\times 2}$ , utenom $W_E = \\left[\\begin{array}{ccc}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{array}\\right]$ , og $W_P = \\left[\\begin{array}{ccc}\n",
    "1 \\\\\n",
    "0 \n",
    "\\end{array}\\right]$\n",
    "\n",
    "Med dette oppgitt vil  $ X = onehot(x) = \\left[\\begin{array}{ccc}0 \\\\1 \n",
    "\\end{array}\\right]$ som resulterer i en $z_0 = \\left[\\begin{array}{ccc}0 \\\\ \\alpha \n",
    "\\end{array}\\right]+ \\left[\\begin{array}{ccc}1 \\\\ 0 \n",
    "\\end{array}\\right]$ =$\\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$. For å videre bestemme et uttrykk for $\\hat{z}$ må vi se på hva transformermodellen gjør med $z_0$. \n",
    "\n",
    "Videre er $z_{1/2}$ = $z_0 + W_O^T  W_V  z_0 A(z_0)$, hvor $A(z_0)$ = $softmax_{col}(z_0^T W_Q^T W_K z_0+D)$ og D sørger for at den strengt nedre delen av A er 0.\n",
    "Ved å løse $A(z_0)$ får man utrykket $(1+ \\alpha ^2)$ i softmax funksjonen.\n",
    "\n",
    "$z_{1/2} = \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right] + I_{2 \\times 2} I_{2 \\times 2} \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right] softmax(1+ \\alpha ^2)$ = $ 2 \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$ ettersom softmax av et utrykk tilsvarer å dele på seg selv i e-potens, som resulterer at utrykket blir lik 1.\n",
    "\n",
    "for $z_1$ får vi et uttrykk som er $z_{1/2} + W_2^T \\sigma (W_1 z_{1/2})$, $\\sigma$ er en aktiveringsfunskjon, i dette tilfelle kan man bruke $relu(W_1 z_{1/2})$.\n",
    "Utrykket blir da:\n",
    "\n",
    "$z_1 = 2  \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]+ I_{2 \\times 2} max(0,I_{2 \\times 2} 2 \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]) $ = $ 4  \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$\n",
    " \n",
    "Ved hjelp av $z_1$ kan man ta i bruk likning $(8)$ for å finne sannsynlighetsfunksjonen $Z$.\n",
    "\n",
    " $Z = softmax_{col}(W_U^T Z_1)$ $,$ her vil argumentet $W_U^T z_1$ bli lik $z_1$, og softmax vil returnere $Z = \\frac{1}{e^4 + e^{4 \\alpha}} \\left[\\begin{array}{ccc}e^4 \\\\ e^{4 \\alpha }\n",
    "\\end{array}\\right]$\n",
    "\n",
    "for å få $\\hat{z} = [1]$ må $argmax(Z)$ bli 1, og dette krever at verdien på indeks [1] må være større enn den på indeks [0], da må $e^4 < e^{4 \\alpha}$ og dette impliserer at $\\alpha >1$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "## Oppgave 2 - Objektorientert programmering for transformermodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 I den utdelte koden layers.py og neural network.py finnes en objektorientert implementering av et nevralt nettverk som kan ha lineære lag og en Relu- aktiveringsfunksjon. I tillegg er embedding og posisjonsenkoding samt feed-forward lag implementert.<Br> Forklar hvordan NeuralNetwork bruker arv, eller inheritance, for å utføre en iterasjon av gradient descent (stepgd()) hvis vi antar det er initiert med minst ett LinearLayer i listen layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et nevralt nettverk lærer gjennom mange små gradevis forbedringer gjort gjennom å prøve og feile. Denne prossen består av mange ulike funksjoner og operasjoner, som beskrevet i forrige oppgave. Dette gjøres med formålet om å utføre en gradient descent. Før at det nevralenettverket skal fungere må layers initieres.\n",
    " \n",
    "Layers er en klasse som fungerer som en base klasse for alle andre typer layers i nettverket, som representerer de ulike prossesene det nevralenettveket utfører. Her implementeres en basis versjon av metoder som forward(), backward() og step_gd(). Dette vil si at hvis et objekt arver fra Layers klassen så vil det objektet har metodene forward(), backward() og step_gd(). Derved vil alle layers som arver fra Layers base klassen implementere eller overskrive disse metodene med kode tilrettet hver individuelle layer. Denne strukturen tillater også at et lag har en egen spesifiserte step_gd() med at det kan overskrive metoden til å være mest hensiktsmessig for det spesifikke laget. Resultatet av dette gjør at neural_network kan operere på et høyere abstraksjonsnivå og kan implementere \"universelle\" metoder som step_gd() uten å ta hensyn til de ulike spesifikke detaljene til hvert lag. \n",
    "\n",
    "Mer spesifikt bruker neural_network arv til å kunne behandle alle sine layers på samme måte, selv om de kan ha forskjellig implementerte step_gd() metoder. Polymorfisme lar da neural_network kalle samme funksjon (step_gd()) på samme måte for hvert lag uten å vite hvilken subklasse hvert layer tilhører. Dermed kan hver operasjon som nural_network utfører kalles gjennom bruk av forward(), for å finne objektfunksjonen, backwards(), for å resette og oppdatere verdiene i nettverket, og step_gd() for å optimalisere vektingen og biasene i treningen av nettverket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "## Oppgave 3 - Objektorientert programmering for transformermodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 0.6957311136810689\n",
      "Epoch 2/10, Average Loss: 0.6954316390301116\n",
      "Epoch 3/10, Average Loss: 0.6951961024680549\n",
      "Epoch 4/10, Average Loss: 0.6950106509827151\n",
      "Epoch 5/10, Average Loss: 0.6948580912584632\n",
      "Epoch 6/10, Average Loss: 0.6947275407778352\n",
      "Epoch 7/10, Average Loss: 0.6946123048522714\n",
      "Epoch 8/10, Average Loss: 0.694508149698596\n",
      "Epoch 9/10, Average Loss: 0.694412290880418\n",
      "Epoch 10/10, Average Loss: 0.6943228230619043\n"
     ]
    }
   ],
   "source": [
    "#training the module\n",
    "r = 5\n",
    "m = 2\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2*r - 1\n",
    "n_iter = 10\n",
    "alpha = 0.00001\n",
    "num_of_samples = 250\n",
    "num_train_batches = 10\n",
    "num_test_batches = 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, num_of_samples, num_train_batches,num_test_batches)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1, feed_forward1,attention2, feed_forward2, un_embed_pos, softmax]\n",
    "nueralnetsort = NeuralNetwork(layers)\n",
    "\n",
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "x_t = data['x_test'][0]\n",
    "y_t = data['y_test'][0]\n",
    "\n",
    "\n",
    "arr2 = algorithm_4(x, y, n_iter, alpha, m, nueralnetsort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,len(arr2),len(arr2)),np.log(arr2))\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Logaritmen av losset')\n",
    "plt.title('Plot av feil')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prosent av antall riktige sorteringer før trening er 0.0%\n",
      "loss for batch 1 is 0.7010071496351208\n",
      "loss for batch 1 is 0.7009056359733078\n",
      "loss for batch 1 is 0.7008441230855066\n",
      "loss for batch 1 is 0.7007447983219827\n",
      "loss for batch 1 is 0.7006368169094347\n",
      "loss for batch 1 is 0.7005097055508396\n",
      "loss for batch 1 is 0.7003061191960271\n",
      "loss for batch 1 is 0.7002608730836754\n",
      "loss for batch 1 is 0.7002567549196111\n",
      "loss for batch 1 is 0.7002698104230599\n",
      "Epoch 1/300, Average Loss: 0.7005741787098565\n",
      "loss for batch 2 is 0.6999037536831816\n",
      "loss for batch 2 is 0.6997240939090462\n",
      "loss for batch 2 is 0.699634687436283\n",
      "loss for batch 2 is 0.6994942289109802\n",
      "loss for batch 2 is 0.6993482801261797\n",
      "loss for batch 2 is 0.6992293902599614\n",
      "loss for batch 2 is 0.6990245084338006\n",
      "loss for batch 2 is 0.698975379682844\n",
      "loss for batch 2 is 0.6989626485977531\n",
      "loss for batch 2 is 0.6989876740392519\n",
      "Epoch 2/300, Average Loss: 0.6993284645079282\n",
      "loss for batch 3 is 0.6986403289825743\n",
      "loss for batch 3 is 0.6984565728967148\n",
      "loss for batch 3 is 0.6983946914500428\n",
      "loss for batch 3 is 0.6982551226204354\n",
      "loss for batch 3 is 0.6981036604987956\n",
      "loss for batch 3 is 0.698017977075393\n",
      "loss for batch 3 is 0.6978305753660572\n",
      "loss for batch 3 is 0.6977923662161265\n",
      "loss for batch 3 is 0.6977840342324744\n",
      "loss for batch 3 is 0.6978294340791307\n",
      "Epoch 3/300, Average Loss: 0.6981104763417745\n",
      "loss for batch 4 is 0.6975055493912323\n",
      "loss for batch 4 is 0.6973241781669397\n",
      "loss for batch 4 is 0.6972887698811027\n",
      "loss for batch 4 is 0.697155518655441\n",
      "loss for batch 4 is 0.6969994808757871\n",
      "loss for batch 4 is 0.6969461236131703\n",
      "loss for batch 4 is 0.6967742294172702\n",
      "loss for batch 4 is 0.6967448261764654\n",
      "loss for batch 4 is 0.6967410147919176\n",
      "loss for batch 4 is 0.6968040961027026\n",
      "Epoch 4/300, Average Loss: 0.6970283787072029\n",
      "loss for batch 5 is 0.6964981759807211\n",
      "loss for batch 5 is 0.6963178784491476\n",
      "loss for batch 5 is 0.6963005703128369\n",
      "loss for batch 5 is 0.6961740247944787\n",
      "loss for batch 5 is 0.6960088957482479\n",
      "loss for batch 5 is 0.6959840608569235\n",
      "loss for batch 5 is 0.6958227293272772\n",
      "loss for batch 5 is 0.6957982549063365\n",
      "loss for batch 5 is 0.6957970960106847\n",
      "loss for batch 5 is 0.6958736657410991\n",
      "Epoch 5/300, Average Loss: 0.6960575352127754\n",
      "loss for batch 6 is 0.6955806957728962\n",
      "loss for batch 6 is 0.6953997471005595\n",
      "loss for batch 6 is 0.6953934356005872\n",
      "loss for batch 6 is 0.6952738537376425\n",
      "loss for batch 6 is 0.6950954285974423\n",
      "loss for batch 6 is 0.6950966193453627\n",
      "loss for batch 6 is 0.6949420527847741\n",
      "loss for batch 6 is 0.6949195745165071\n",
      "loss for batch 6 is 0.6949197485424935\n",
      "loss for batch 6 is 0.6950071529896362\n",
      "Epoch 6/300, Average Loss: 0.6951628308987902\n",
      "loss for batch 7 is 0.6947236477146951\n",
      "loss for batch 7 is 0.6945411055610875\n",
      "loss for batch 7 is 0.694540640797823\n",
      "loss for batch 7 is 0.6944288037899531\n",
      "loss for batch 7 is 0.6942338707501647\n",
      "loss for batch 7 is 0.6942598222551055\n",
      "loss for batch 7 is 0.6941093995497449\n",
      "loss for batch 7 is 0.6940868994512881\n",
      "loss for batch 7 is 0.6940877151155748\n",
      "loss for batch 7 is 0.6941843160881165\n",
      "Epoch 7/300, Average Loss: 0.6943196221073553\n",
      "loss for batch 8 is 0.6939078483169222\n",
      "loss for batch 8 is 0.6937232270840266\n",
      "loss for batch 8 is 0.693724954394756\n",
      "loss for batch 8 is 0.6936216098776704\n",
      "loss for batch 8 is 0.6934077630629741\n",
      "loss for batch 8 is 0.6934578378251066\n",
      "loss for batch 8 is 0.6933095897582385\n",
      "loss for batch 8 is 0.6932855748163911\n",
      "loss for batch 8 is 0.6932865163851811\n",
      "loss for batch 8 is 0.6933909162437718\n",
      "Epoch 8/300, Average Loss: 0.6935115837765038\n",
      "loss for batch 9 is 0.6931195122191762\n",
      "loss for batch 9 is 0.69293220528975\n",
      "loss for batch 9 is 0.6929333500908195\n",
      "loss for batch 9 is 0.692838869428273\n",
      "loss for batch 9 is 0.6926042143767773\n",
      "loss for batch 9 is 0.692677827479957\n",
      "loss for batch 9 is 0.6925304634936408\n",
      "loss for batch 9 is 0.6925041185265525\n",
      "loss for batch 9 is 0.6925047763678372\n",
      "loss for batch 9 is 0.6926160823201891\n",
      "Epoch 9/300, Average Loss: 0.6927261419592972\n",
      "loss for batch 10 is 0.6923485361475264\n",
      "loss for batch 10 is 0.6921581805314087\n",
      "loss for batch 10 is 0.6921573469916109\n",
      "loss for batch 10 is 0.6920718335959195\n",
      "loss for batch 10 is 0.691815423315345\n",
      "loss for batch 10 is 0.691912197511281\n",
      "loss for batch 10 is 0.6917648809836672\n",
      "loss for batch 10 is 0.6917353544405206\n",
      "loss for batch 10 is 0.6917352849398709\n",
      "loss for batch 10 is 0.6918530071942569\n",
      "Epoch 10/300, Average Loss: 0.6919552045651407\n",
      "loss for batch 11 is 0.6915886924613242\n",
      "loss for batch 11 is 0.6913948265411214\n",
      "loss for batch 11 is 0.6913912052762342\n",
      "loss for batch 11 is 0.6913140725171938\n",
      "loss for batch 11 is 0.6910352940274812\n",
      "loss for batch 11 is 0.6911545253321052\n",
      "loss for batch 11 is 0.6910066781018167\n",
      "loss for batch 11 is 0.690973587499584\n",
      "loss for batch 11 is 0.6909722754755688\n",
      "loss for batch 11 is 0.6910957980711898\n",
      "Epoch 11/300, Average Loss: 0.6911926955303619\n",
      "loss for batch 12 is 0.6908341836187605\n",
      "loss for batch 12 is 0.6906363067419056\n",
      "loss for batch 12 is 0.6906298632599345\n",
      "loss for batch 12 is 0.6905602750030432\n",
      "loss for batch 12 is 0.6902590852577253\n",
      "loss for batch 12 is 0.6904001374891587\n",
      "loss for batch 12 is 0.6902515343913543\n",
      "loss for batch 12 is 0.690214628702687\n",
      "loss for batch 12 is 0.6902115646870594\n",
      "loss for batch 12 is 0.6903409468525867\n",
      "Epoch 12/300, Average Loss: 0.6904338526004216\n",
      "loss for batch 13 is 0.6900819336266962\n",
      "loss for batch 13 is 0.6898795929556105\n",
      "loss for batch 13 is 0.6898711259365615\n",
      "loss for batch 13 is 0.6898080113235088\n",
      "loss for batch 13 is 0.6894852646734863\n",
      "loss for batch 13 is 0.6896472473238238\n",
      "loss for batch 13 is 0.6894981942210417\n",
      "loss for batch 13 is 0.6894575622035698\n",
      "loss for batch 13 is 0.6894520140736228\n",
      "loss for batch 13 is 0.6895867682384115\n",
      "Epoch 13/300, Average Loss: 0.6896767714576334\n",
      "loss for batch 14 is 0.6893301907563995\n",
      "loss for batch 14 is 0.6891222550893923\n",
      "loss for batch 14 is 0.6891131756728176\n",
      "loss for batch 14 is 0.6890538492039786\n",
      "loss for batch 14 is 0.68871062202595\n",
      "loss for batch 14 is 0.6888919715040412\n",
      "loss for batch 14 is 0.6887426564635717\n",
      "loss for batch 14 is 0.6886980411962467\n",
      "loss for batch 14 is 0.6886891083988121\n",
      "loss for batch 14 is 0.6888292298175278\n",
      "Epoch 14/300, Average Loss: 0.6889181100128737\n",
      "loss for batch 15 is 0.6885753427393271\n",
      "loss for batch 15 is 0.6883610941766798\n",
      "loss for batch 15 is 0.6883527319490795\n",
      "loss for batch 15 is 0.6882955357532791\n",
      "loss for batch 15 is 0.6879328255181032\n",
      "loss for batch 15 is 0.6881326238093799\n",
      "loss for batch 15 is 0.6879832460006355\n",
      "loss for batch 15 is 0.687934597941412\n",
      "loss for batch 15 is 0.6879215476668195\n",
      "loss for batch 15 is 0.6880669662248389\n",
      "Epoch 15/300, Average Loss: 0.6881556511779554\n",
      "loss for batch 16 is 0.6878158278352806\n",
      "loss for batch 16 is 0.6875946269806466\n",
      "loss for batch 16 is 0.6875881220804934\n",
      "loss for batch 16 is 0.6875314302033685\n",
      "loss for batch 16 is 0.6871499808131577\n",
      "loss for batch 16 is 0.6873674127027425\n",
      "loss for batch 16 is 0.6872182038459844\n",
      "loss for batch 16 is 0.6871655388348438\n",
      "loss for batch 16 is 0.687147837590859\n",
      "loss for batch 16 is 0.6872987252599196\n",
      "Epoch 16/300, Average Loss: 0.6873877706147297\n",
      "loss for batch 17 is 0.6870506276213056\n",
      "loss for batch 17 is 0.6868220111505435\n",
      "loss for batch 17 is 0.6868186589500328\n",
      "loss for batch 17 is 0.6867611769386098\n",
      "loss for batch 17 is 0.6863620135245317\n",
      "loss for batch 17 is 0.6865964190107853\n",
      "loss for batch 17 is 0.6864478486488336\n",
      "loss for batch 17 is 0.6863912981202734\n",
      "loss for batch 17 is 0.6863684548178715\n",
      "loss for batch 17 is 0.6865247391171859\n",
      "Epoch 17/300, Average Loss: 0.6866143247899973\n",
      "loss for batch 18 is 0.6862799370153052\n",
      "loss for batch 18 is 0.6860428752315696\n",
      "loss for batch 18 is 0.6860437281332068\n",
      "loss for batch 18 is 0.6859836326793409\n",
      "loss for batch 18 is 0.6855673530482491\n",
      "loss for batch 18 is 0.6858173668458497\n",
      "loss for batch 18 is 0.6856692541482763\n",
      "loss for batch 18 is 0.6856084224941301\n",
      "loss for batch 18 is 0.6855790982638742\n",
      "loss for batch 18 is 0.6857402821031184\n",
      "Epoch 18/300, Average Loss: 0.685833194996292\n",
      "loss for batch 19 is 0.6854988431960093\n",
      "loss for batch 19 is 0.6852519468276459\n",
      "loss for batch 19 is 0.6852590297459507\n",
      "loss for batch 19 is 0.6851938446480735\n",
      "loss for batch 19 is 0.684761985873356\n",
      "loss for batch 19 is 0.6850267135260311\n",
      "loss for batch 19 is 0.6848796773862065\n",
      "loss for batch 19 is 0.6848150336207766\n",
      "loss for batch 19 is 0.6847785688953212\n",
      "loss for batch 19 is 0.6849452690184942\n",
      "Epoch 19/300, Average Loss: 0.6850410912737865\n",
      "loss for batch 20 is 0.6847081351051982\n",
      "loss for batch 20 is 0.6844512620714411\n",
      "loss for batch 20 is 0.6844662313652828\n",
      "loss for batch 20 is 0.6843945684728082\n",
      "loss for batch 20 is 0.6839479924872792\n",
      "loss for batch 20 is 0.6842271857199302\n",
      "loss for batch 20 is 0.6840816585601491\n",
      "loss for batch 20 is 0.6840133145982916\n",
      "loss for batch 20 is 0.6839689567895846\n",
      "loss for batch 20 is 0.6841413502543752\n",
      "Epoch 20/300, Average Loss: 0.684240065542434\n",
      "loss for batch 21 is 0.6839086342589017\n",
      "loss for batch 21 is 0.683641226881285\n",
      "loss for batch 21 is 0.6836656859484352\n",
      "loss for batch 21 is 0.6835857446121613\n",
      "loss for batch 21 is 0.6831253690390553\n",
      "loss for batch 21 is 0.6834179772713118\n",
      "loss for batch 21 is 0.6832741362284398\n",
      "loss for batch 21 is 0.6832019953584512\n",
      "loss for batch 21 is 0.6831488992164075\n",
      "loss for batch 21 is 0.6833271333743663\n",
      "Epoch 21/300, Average Loss: 0.6834296802188815\n",
      "loss for batch 22 is 0.6830990815971274\n",
      "loss for batch 22 is 0.6828204034914416\n",
      "loss for batch 22 is 0.682855130897518\n",
      "loss for batch 22 is 0.6827650518210666\n",
      "loss for batch 22 is 0.6822912287107851\n",
      "loss for batch 22 is 0.6825965615431456\n",
      "loss for batch 22 is 0.6824546022237864\n",
      "loss for batch 22 is 0.6823786675985559\n",
      "loss for batch 22 is 0.6823161869029513\n",
      "loss for batch 22 is 0.6825005721197854\n",
      "Epoch 22/300, Average Loss: 0.6826077486906162\n",
      "loss for batch 23 is 0.6822775669637455\n",
      "loss for batch 23 is 0.6819868793541253\n",
      "loss for batch 23 is 0.6820346976880304\n",
      "loss for batch 23 is 0.6819328393484082\n",
      "loss for batch 23 is 0.6814476434594602\n",
      "loss for batch 23 is 0.681765304776506\n",
      "loss for batch 23 is 0.6816265339741568\n",
      "loss for batch 23 is 0.6815467479291851\n",
      "loss for batch 23 is 0.6814741248626212\n",
      "loss for batch 23 is 0.6816646974564655\n",
      "Epoch 23/300, Average Loss: 0.6817757035812704\n",
      "loss for batch 24 is 0.6814469284870688\n",
      "loss for batch 24 is 0.6811425391275383\n",
      "loss for batch 24 is 0.6812050916454034\n",
      "loss for batch 24 is 0.6810883783858701\n",
      "loss for batch 24 is 0.6805928232427088\n",
      "loss for batch 24 is 0.6809209322654644\n",
      "loss for batch 24 is 0.6807855204002945\n",
      "loss for batch 24 is 0.6807019941086301\n",
      "loss for batch 24 is 0.6806181542589275\n",
      "loss for batch 24 is 0.680815127909793\n",
      "Epoch 24/300, Average Loss: 0.6809317489831699\n",
      "loss for batch 25 is 0.6806030233701438\n",
      "loss for batch 25 is 0.6802839813439306\n",
      "loss for batch 25 is 0.6803637034982901\n",
      "loss for batch 25 is 0.6802292556802301\n",
      "loss for batch 25 is 0.6797253641384665\n",
      "loss for batch 25 is 0.6800624862065603\n",
      "loss for batch 25 is 0.6799314214690083\n",
      "loss for batch 25 is 0.6798442782808523\n",
      "loss for batch 25 is 0.679748049219977\n",
      "loss for batch 25 is 0.6799522002652102\n",
      "Epoch 25/300, Average Loss: 0.680074376347267\n",
      "loss for batch 26 is 0.6797461612841278\n",
      "loss for batch 26 is 0.6794116175475045\n",
      "loss for batch 26 is 0.6795117593207864\n",
      "loss for batch 26 is 0.679355948097402\n",
      "loss for batch 26 is 0.6788461438251501\n",
      "loss for batch 26 is 0.6791908383484347\n",
      "loss for batch 26 is 0.6790651241282483\n",
      "loss for batch 26 is 0.6789744636742993\n",
      "loss for batch 26 is 0.6788648544259058\n",
      "loss for batch 26 is 0.6790766393464861\n",
      "Epoch 26/300, Average Loss: 0.6792043549998346\n",
      "loss for batch 27 is 0.6788768983082573\n",
      "loss for batch 27 is 0.6785261164176302\n",
      "loss for batch 27 is 0.6786484622750387\n",
      "loss for batch 27 is 0.6784693669458144\n",
      "loss for batch 27 is 0.6779546881005782\n",
      "loss for batch 27 is 0.6783063529425364\n",
      "loss for batch 27 is 0.678186446498271\n",
      "loss for batch 27 is 0.6780922778933501\n",
      "loss for batch 27 is 0.677968699824776\n",
      "loss for batch 27 is 0.6781883272937305\n",
      "Epoch 27/300, Average Loss: 0.6783217636499982\n",
      "loss for batch 28 is 0.6779951453710077\n",
      "loss for batch 28 is 0.6776274438857364\n",
      "loss for batch 28 is 0.6777730375838343\n",
      "loss for batch 28 is 0.6775687856337985\n",
      "loss for batch 28 is 0.6770496381862622\n",
      "loss for batch 28 is 0.6774072872578899\n",
      "loss for batch 28 is 0.6772931099383365\n",
      "loss for batch 28 is 0.6771951414626733\n",
      "loss for batch 28 is 0.6770560349821767\n",
      "loss for batch 28 is 0.6772832412276469\n",
      "Epoch 28/300, Average Loss: 0.6774248865529362\n",
      "loss for batch 29 is 0.6770969227795027\n",
      "loss for batch 29 is 0.6767107674772398\n",
      "loss for batch 29 is 0.6768820290372195\n",
      "loss for batch 29 is 0.6766503302121258\n",
      "loss for batch 29 is 0.6761282532351199\n",
      "loss for batch 29 is 0.6764912456083978\n",
      "loss for batch 29 is 0.6763834959396938\n",
      "loss for batch 29 is 0.6762820475479797\n",
      "loss for batch 29 is 0.6761266817831401\n",
      "loss for batch 29 is 0.6763619668374128\n",
      "Epoch 29/300, Average Loss: 0.6765113740457832\n",
      "loss for batch 30 is 0.6761831350384366\n",
      "loss for batch 30 is 0.6757777147671282\n",
      "loss for batch 30 is 0.675976366685889\n",
      "loss for batch 30 is 0.6757148918742809\n",
      "loss for batch 30 is 0.6751907068782858\n",
      "loss for batch 30 is 0.675557883383514\n",
      "loss for batch 30 is 0.6754565653182937\n",
      "loss for batch 30 is 0.6753513220968591\n",
      "loss for batch 30 is 0.6751780806352475\n",
      "loss for batch 30 is 0.6754213421581674\n",
      "Epoch 30/300, Average Loss: 0.6755808008836103\n",
      "loss for batch 31 is 0.6752507598962586\n",
      "loss for batch 31 is 0.6748249149321698\n",
      "loss for batch 31 is 0.6750533578701571\n",
      "loss for batch 31 is 0.6747592268195127\n",
      "loss for batch 31 is 0.6742337992356999\n",
      "loss for batch 31 is 0.674603763325027\n",
      "loss for batch 31 is 0.6745089817288416\n",
      "loss for batch 31 is 0.6743995247013755\n",
      "loss for batch 31 is 0.6742066558280257\n",
      "loss for batch 31 is 0.6744578008058785\n",
      "Epoch 31/300, Average Loss: 0.6746298785142947\n",
      "loss for batch 32 is 0.6742955396108187\n",
      "loss for batch 32 is 0.6738474478289732\n",
      "loss for batch 32 is 0.6741074549970346\n",
      "loss for batch 32 is 0.6737783825823944\n",
      "loss for batch 32 is 0.6732533367454617\n",
      "loss for batch 32 is 0.6736261418866225\n",
      "loss for batch 32 is 0.673539342141932\n",
      "loss for batch 32 is 0.6734263190572995\n",
      "loss for batch 32 is 0.67321358984678\n",
      "loss for batch 32 is 0.6734738540909243\n",
      "Epoch 32/300, Average Loss: 0.6736561408788242\n",
      "loss for batch 33 is 0.6733212307727764\n",
      "loss for batch 33 is 0.672850911462583\n",
      "loss for batch 33 is 0.6731450826289529\n",
      "loss for batch 33 is 0.6727794619279761\n",
      "loss for batch 33 is 0.6722559472714361\n",
      "loss for batch 33 is 0.6726312301181234\n",
      "loss for batch 33 is 0.6725528623484194\n",
      "loss for batch 33 is 0.6724361588492747\n",
      "loss for batch 33 is 0.6722024631070602\n",
      "loss for batch 33 is 0.6724717749224692\n",
      "Epoch 33/300, Average Loss: 0.6726647123409071\n",
      "loss for batch 34 is 0.6723289868703503\n",
      "loss for batch 34 is 0.6718350103005744\n",
      "loss for batch 34 is 0.6721649496258798\n",
      "loss for batch 34 is 0.67176003515309\n",
      "loss for batch 34 is 0.6712387750646936\n",
      "loss for batch 34 is 0.6716150771160996\n",
      "loss for batch 34 is 0.6715453912743231\n",
      "loss for batch 34 is 0.6714250296957299\n",
      "loss for batch 34 is 0.6711693270242945\n",
      "loss for batch 34 is 0.6714483575522985\n",
      "Epoch 34/300, Average Loss: 0.6716530939677334\n",
      "loss for batch 35 is 0.6713166078444538\n",
      "loss for batch 35 is 0.6707983454135432\n",
      "loss for batch 35 is 0.6711670078997919\n",
      "loss for batch 35 is 0.6707210391473574\n",
      "loss for batch 35 is 0.670204225015991\n",
      "loss for batch 35 is 0.6705813128804184\n",
      "loss for batch 35 is 0.6705216296163958\n",
      "loss for batch 35 is 0.6703983066076309\n",
      "loss for batch 35 is 0.6701200259295262\n",
      "loss for batch 35 is 0.6704092832287887\n",
      "Epoch 35/300, Average Loss: 0.6706237783583898\n",
      "loss for batch 36 is 0.6702889899982212\n",
      "loss for batch 36 is 0.6697452136909546\n",
      "loss for batch 36 is 0.6701551712061173\n",
      "loss for batch 36 is 0.669665214060268\n",
      "loss for batch 36 is 0.669154437696173\n",
      "loss for batch 36 is 0.6695309920313731\n",
      "loss for batch 36 is 0.6694818492411553\n",
      "loss for batch 36 is 0.6693555476513198\n",
      "loss for batch 36 is 0.6690536488467063\n",
      "loss for batch 36 is 0.6693534094873856\n",
      "Epoch 36/300, Average Loss: 0.6695784473909674\n",
      "loss for batch 37 is 0.669245325541512\n",
      "loss for batch 37 is 0.6686749724926276\n",
      "loss for batch 37 is 0.6691278282295037\n",
      "loss for batch 37 is 0.6685919260123766\n",
      "loss for batch 37 is 0.6680878905437946\n",
      "loss for batch 37 is 0.6684632077933622\n",
      "loss for batch 37 is 0.6684247505153821\n",
      "loss for batch 37 is 0.6682952654560813\n",
      "loss for batch 37 is 0.6679687254909351\n",
      "loss for batch 37 is 0.6682789778450652\n",
      "Epoch 37/300, Average Loss: 0.6685158869920641\n",
      "loss for batch 38 is 0.6681834399557015\n",
      "loss for batch 38 is 0.6675854599073193\n",
      "loss for batch 38 is 0.6680829051724382\n",
      "loss for batch 38 is 0.6674992524910894\n",
      "loss for batch 38 is 0.6670029024876182\n",
      "loss for batch 38 is 0.6673764008747125\n",
      "loss for batch 38 is 0.6673491818538534\n",
      "loss for batch 38 is 0.6672165781371435\n",
      "loss for batch 38 is 0.6668648284695173\n",
      "loss for batch 38 is 0.6671859425781388\n",
      "Epoch 38/300, Average Loss: 0.6674346891927532\n",
      "loss for batch 39 is 0.6671036638554778\n",
      "loss for batch 39 is 0.6664769524422092\n",
      "loss for batch 39 is 0.6670204692383928\n",
      "loss for batch 39 is 0.6663874524669101\n",
      "loss for batch 39 is 0.6658994996475369\n",
      "loss for batch 39 is 0.6662705515841926\n",
      "loss for batch 39 is 0.6662548963225478\n",
      "loss for batch 39 is 0.666119084777745\n",
      "loss for batch 39 is 0.6657411860155202\n",
      "loss for batch 39 is 0.6660732936577353\n",
      "Epoch 39/300, Average Loss: 0.6663347050008268\n",
      "loss for batch 40 is 0.6660046964866313\n",
      "loss for batch 40 is 0.6653481698721253\n",
      "loss for batch 40 is 0.6659395410316763\n",
      "loss for batch 40 is 0.6652551427058795\n",
      "loss for batch 40 is 0.6647764464668608\n",
      "loss for batch 40 is 0.6651444665522935\n",
      "loss for batch 40 is 0.6651411746653537\n",
      "loss for batch 40 is 0.6650024753353698\n",
      "loss for batch 40 is 0.6645977686698474\n",
      "loss for batch 40 is 0.664941475167293\n",
      "Epoch 40/300, Average Loss: 0.6652151356953331\n",
      "loss for batch 41 is 0.6648875318550657\n",
      "loss for batch 41 is 0.6642003668751051\n",
      "loss for batch 41 is 0.6648419713195963\n",
      "loss for batch 41 is 0.6641042412594446\n",
      "loss for batch 41 is 0.6636363919569002\n",
      "loss for batch 41 is 0.6640007023701113\n",
      "loss for batch 41 is 0.6640099358096511\n",
      "loss for batch 41 is 0.663868031000161\n",
      "loss for batch 41 is 0.6634351032643035\n",
      "loss for batch 41 is 0.6637902017567865\n",
      "Epoch 41/300, Average Loss: 0.6640774477467126\n",
      "loss for batch 42 is 0.663751281598055\n",
      "loss for batch 42 is 0.6630320137604704\n",
      "loss for batch 42 is 0.6637257454505519\n",
      "loss for batch 42 is 0.6629326140261546\n",
      "loss for batch 42 is 0.6624771358058208\n",
      "loss for batch 42 is 0.6628371534879816\n",
      "loss for batch 42 is 0.6628601120633383\n",
      "loss for batch 42 is 0.6627157409785268\n",
      "loss for batch 42 is 0.6622540673367955\n",
      "loss for batch 42 is 0.6626214752091589\n",
      "Epoch 42/300, Average Loss: 0.6629207339716854\n",
      "loss for batch 43 is 0.6625990434161562\n",
      "loss for batch 43 is 0.6618468629662196\n",
      "loss for batch 43 is 0.6625961334256264\n",
      "loss for batch 43 is 0.6617434186646378\n",
      "loss for batch 43 is 0.6613017499033721\n",
      "loss for batch 43 is 0.6616549113981363\n",
      "loss for batch 43 is 0.6616916778520288\n",
      "loss for batch 43 is 0.6615440985359547\n",
      "loss for batch 43 is 0.6610514785854471\n",
      "loss for batch 43 is 0.6614304945368361\n",
      "Epoch 43/300, Average Loss: 0.6617459869284416\n",
      "loss for batch 44 is 0.6614243959827737\n",
      "loss for batch 44 is 0.6606371060241857\n",
      "loss for batch 44 is 0.6614440240077404\n",
      "loss for batch 44 is 0.6605289410897519\n",
      "loss for batch 44 is 0.6601025355231949\n",
      "loss for batch 44 is 0.6604480834294096\n",
      "loss for batch 44 is 0.660499433391651\n",
      "loss for batch 44 is 0.6603488505603922\n",
      "loss for batch 44 is 0.6598241500754907\n",
      "loss for batch 44 is 0.6602147665824667\n",
      "Epoch 44/300, Average Loss: 0.6605472286667057\n",
      "loss for batch 45 is 0.6602254348270586\n",
      "loss for batch 45 is 0.6594010748498956\n",
      "loss for batch 45 is 0.6602663895697723\n",
      "loss for batch 45 is 0.6592852931807094\n",
      "loss for batch 45 is 0.6588731777787425\n",
      "loss for batch 45 is 0.6592090068845088\n",
      "loss for batch 45 is 0.6592735816583135\n",
      "loss for batch 45 is 0.6591178294884454\n",
      "loss for batch 45 is 0.6585579540233404\n",
      "loss for batch 45 is 0.6589592615151103\n",
      "Epoch 45/300, Average Loss: 0.6593169003775897\n",
      "loss for batch 46 is 0.6589860518122966\n",
      "loss for batch 46 is 0.6581221548712892\n",
      "loss for batch 46 is 0.6590477947431754\n",
      "loss for batch 46 is 0.657998938686123\n",
      "loss for batch 46 is 0.657601520199802\n",
      "loss for batch 46 is 0.6579286235402736\n",
      "loss for batch 46 is 0.658007497033146\n",
      "loss for batch 46 is 0.657847185541675\n",
      "loss for batch 46 is 0.6572524182875237\n",
      "loss for batch 46 is 0.6576663525532856\n",
      "Epoch 46/300, Average Loss: 0.6580458537268591\n",
      "loss for batch 47 is 0.6577117151657365\n",
      "loss for batch 47 is 0.6568084575473675\n",
      "loss for batch 47 is 0.6577967236189562\n",
      "loss for batch 47 is 0.6566800964582727\n",
      "loss for batch 47 is 0.6562985169053329\n",
      "loss for batch 47 is 0.6566180212458037\n",
      "loss for batch 47 is 0.6567124656619532\n",
      "loss for batch 47 is 0.6565486032744683\n",
      "loss for batch 47 is 0.6559188497239629\n",
      "loss for batch 47 is 0.6563463401877868\n",
      "Epoch 47/300, Average Loss: 0.6567439789789641\n",
      "loss for batch 48 is 0.656411429317519\n",
      "loss for batch 48 is 0.6554681133911244\n",
      "loss for batch 48 is 0.6565211790872347\n",
      "loss for batch 48 is 0.6553355888464725\n",
      "loss for batch 48 is 0.6549702773303107\n",
      "loss for batch 48 is 0.6552820647384726\n",
      "loss for batch 48 is 0.6553919864437162\n",
      "loss for batch 48 is 0.6552243418978835\n",
      "loss for batch 48 is 0.6545582419183251\n",
      "loss for batch 48 is 0.654998961549315\n",
      "Epoch 48/300, Average Loss: 0.6554162184520373\n",
      "loss for batch 49 is 0.6550837265741815\n",
      "loss for batch 49 is 0.654098723764795\n",
      "loss for batch 49 is 0.6552175801213957\n",
      "loss for batch 49 is 0.65396020465602\n",
      "loss for batch 49 is 0.6536113571008929\n",
      "loss for batch 49 is 0.6539143122958405\n",
      "loss for batch 49 is 0.6540398162384017\n",
      "loss for batch 49 is 0.6538681572798236\n",
      "loss for batch 49 is 0.653164248606843\n",
      "loss for batch 49 is 0.6536181376953353\n",
      "Epoch 49/300, Average Loss: 0.6540576264333529\n",
      "loss for batch 50 is 0.6537236500813346\n",
      "loss for batch 50 is 0.6526961875323858\n",
      "loss for batch 50 is 0.6538835052802918\n",
      "loss for batch 50 is 0.6525522922024092\n",
      "loss for batch 50 is 0.6522213764665908\n",
      "loss for batch 50 is 0.6525150754340137\n",
      "loss for batch 50 is 0.6526569901683759\n",
      "loss for batch 50 is 0.6524816312824316\n",
      "loss for batch 50 is 0.6517389676988478\n",
      "loss for batch 50 is 0.6522065590037422\n",
      "Epoch 50/300, Average Loss: 0.6526676235150425\n",
      "loss for batch 51 is 0.6523339371786128\n",
      "loss for batch 51 is 0.6512627885925262\n",
      "loss for batch 51 is 0.6525208771296981\n",
      "loss for batch 51 is 0.6511137001291428\n",
      "loss for batch 51 is 0.650801678904794\n",
      "loss for batch 51 is 0.6510856887638617\n",
      "loss for batch 51 is 0.6512444809175797\n",
      "loss for batch 51 is 0.6510653874964971\n",
      "loss for batch 51 is 0.6502827354240868\n",
      "loss for batch 51 is 0.6507643968972734\n",
      "Epoch 51/300, Average Loss: 0.6512475671434073\n",
      "loss for batch 52 is 0.6509146135945436\n",
      "loss for batch 52 is 0.6497984163457065\n",
      "loss for batch 52 is 0.6511293690329986\n",
      "loss for batch 52 is 0.6496441382176908\n",
      "loss for batch 52 is 0.6493518003088349\n",
      "loss for batch 52 is 0.6496257170243083\n",
      "loss for batch 52 is 0.649801792411086\n",
      "loss for batch 52 is 0.6496188882502103\n",
      "loss for batch 52 is 0.64879500965667\n",
      "loss for batch 52 is 0.6492911150741052\n",
      "Epoch 52/300, Average Loss: 0.6497970859916153\n",
      "loss for batch 53 is 0.6494651572619259\n",
      "loss for batch 53 is 0.6483025814767721\n",
      "loss for batch 53 is 0.6497085293987522\n",
      "loss for batch 53 is 0.6481431055500024\n",
      "loss for batch 53 is 0.6478712170716543\n",
      "loss for batch 53 is 0.6481346319372981\n",
      "loss for batch 53 is 0.6483284443357863\n",
      "loss for batch 53 is 0.6481416450094785\n",
      "loss for batch 53 is 0.6472752078640935\n",
      "loss for batch 53 is 0.6477859615324402\n",
      "Epoch 53/300, Average Loss: 0.6483156481438204\n",
      "loss for batch 54 is 0.6479847867692772\n",
      "loss for batch 54 is 0.6467744506051756\n",
      "loss for batch 54 is 0.6482573622806299\n",
      "loss for batch 54 is 0.6466096063104949\n",
      "loss for batch 54 is 0.6463588344083803\n",
      "loss for batch 54 is 0.6466113090322724\n",
      "loss for batch 54 is 0.6468232073011122\n",
      "loss for batch 54 is 0.6466323487878927\n",
      "loss for batch 54 is 0.6457219289737638\n",
      "loss for batch 54 is 0.6462477380889745\n",
      "Epoch 54/300, Average Loss: 0.6468021572557974\n",
      "loss for batch 55 is 0.6464724159122713\n",
      "loss for batch 55 is 0.6452128840537467\n",
      "loss for batch 55 is 0.6467749619183005\n",
      "loss for batch 55 is 0.6450429164482834\n",
      "loss for batch 55 is 0.6448140773357408\n",
      "loss for batch 55 is 0.6450555178842997\n",
      "loss for batch 55 is 0.6452860017879423\n",
      "loss for batch 55 is 0.6450911217911183\n",
      "loss for batch 55 is 0.6441356942898324\n",
      "loss for batch 55 is 0.6446771184807982\n",
      "Epoch 55/300, Average Loss: 0.6452562709902334\n",
      "loss for batch 56 is 0.6449287342598753\n",
      "loss for batch 56 is 0.6436188950662407\n",
      "loss for batch 56 is 0.6452619558427012\n",
      "loss for batch 56 is 0.6434439879335984\n",
      "loss for batch 56 is 0.6432373002979086\n",
      "loss for batch 56 is 0.6434676414630414\n",
      "loss for batch 56 is 0.6437167606788343\n",
      "loss for batch 56 is 0.6435175397963001\n",
      "loss for batch 56 is 0.6425157445450127\n",
      "loss for batch 56 is 0.6430730501858668\n",
      "Epoch 56/300, Average Loss: 0.643678161006938\n",
      "loss for batch 57 is 0.6433525221706633\n",
      "loss for batch 57 is 0.6419907946485273\n",
      "loss for batch 57 is 0.6437160401366017\n",
      "loss for batch 57 is 0.6418091964801289\n",
      "loss for batch 57 is 0.6416240166690057\n",
      "loss for batch 57 is 0.6418411932993364\n",
      "loss for batch 57 is 0.6421074916759056\n",
      "loss for batch 57 is 0.641902632045774\n",
      "loss for batch 57 is 0.6408505305544505\n",
      "loss for batch 57 is 0.6414222502905327\n",
      "Epoch 57/300, Average Loss: 0.6420616667970926\n",
      "loss for batch 58 is 0.6417309394137798\n",
      "loss for batch 58 is 0.640312303064017\n",
      "loss for batch 58 is 0.6421249158738701\n",
      "loss for batch 58 is 0.6401236504067515\n",
      "loss for batch 58 is 0.6399643371558529\n",
      "loss for batch 58 is 0.6401658329126397\n",
      "loss for batch 58 is 0.6404507049888016\n",
      "loss for batch 58 is 0.6402407512196352\n",
      "loss for batch 58 is 0.6391347969465137\n",
      "loss for batch 58 is 0.6397216422510447\n",
      "Epoch 58/300, Average Loss: 0.6403969874232907\n",
      "loss for batch 59 is 0.6400611670189036\n",
      "loss for batch 59 is 0.6385837504356674\n",
      "loss for batch 59 is 0.6404883006804406\n",
      "loss for batch 59 is 0.638387845900554\n",
      "loss for batch 59 is 0.6382575944803742\n",
      "loss for batch 59 is 0.6384429500653619\n",
      "loss for batch 59 is 0.6387482669264455\n",
      "loss for batch 59 is 0.6385345356535828\n",
      "loss for batch 59 is 0.6373747122503243\n",
      "loss for batch 59 is 0.6379792548061287\n",
      "Epoch 59/300, Average Loss: 0.6386858378217782\n",
      "loss for batch 60 is 0.6383522159675198\n",
      "loss for batch 60 is 0.6368153315352347\n",
      "loss for batch 60 is 0.638814968350643\n",
      "loss for batch 60 is 0.6366134691569719\n",
      "loss for batch 60 is 0.6365122964169632\n",
      "loss for batch 60 is 0.636680750561211\n",
      "loss for batch 60 is 0.6370051941593767\n",
      "loss for batch 60 is 0.6367861954673067\n",
      "loss for batch 60 is 0.6355703016999108\n",
      "loss for batch 60 is 0.6361910850095783\n",
      "Epoch 60/300, Average Loss: 0.6369341808324716\n",
      "loss for batch 61 is 0.6365976690065055\n",
      "loss for batch 61 is 0.6350003136928235\n",
      "loss for batch 61 is 0.6370943504562386\n",
      "loss for batch 61 is 0.6347939341792176\n",
      "loss for batch 61 is 0.6347201717139923\n",
      "loss for batch 61 is 0.6348746025613291\n",
      "loss for batch 61 is 0.6352187104443041\n",
      "loss for batch 61 is 0.6349956845375058\n",
      "loss for batch 61 is 0.6337251228168406\n",
      "loss for batch 61 is 0.6343640946425836\n",
      "Epoch 61/300, Average Loss: 0.635138465405134\n",
      "loss for batch 62 is 0.6348065738357672\n",
      "loss for batch 62 is 0.6331486310530599\n",
      "loss for batch 62 is 0.6353373094130885\n",
      "loss for batch 62 is 0.6329392170534989\n",
      "loss for batch 62 is 0.6328922256030147\n",
      "loss for batch 62 is 0.6330339230177545\n",
      "loss for batch 62 is 0.6333967313343024\n",
      "loss for batch 62 is 0.6331681814667278\n",
      "loss for batch 62 is 0.6318399563508528\n",
      "loss for batch 62 is 0.6324953708658437\n",
      "Epoch 62/300, Average Loss: 0.6333058119993911\n",
      "loss for batch 63 is 0.6329738287099129\n",
      "loss for batch 63 is 0.6312514083563706\n",
      "loss for batch 63 is 0.6335363980704244\n",
      "loss for batch 63 is 0.6310353031514396\n",
      "loss for batch 63 is 0.6310152381988421\n",
      "loss for batch 63 is 0.6311406697834933\n",
      "loss for batch 63 is 0.6315218505952004\n",
      "loss for batch 63 is 0.6312885094286432\n",
      "loss for batch 63 is 0.6298994943441719\n",
      "loss for batch 63 is 0.6305723460777278\n",
      "Epoch 63/300, Average Loss: 0.6314235046716227\n",
      "loss for batch 64 is 0.6310912971455516\n",
      "loss for batch 64 is 0.629302936718354\n",
      "loss for batch 64 is 0.6316901428619601\n",
      "loss for batch 64 is 0.629084144971404\n",
      "loss for batch 64 is 0.6290941850153221\n",
      "loss for batch 64 is 0.6292045088348984\n",
      "loss for batch 64 is 0.6296054437939079\n",
      "loss for batch 64 is 0.6293679887648868\n",
      "loss for batch 64 is 0.627917292134163\n",
      "loss for batch 64 is 0.6286084589483094\n",
      "Epoch 64/300, Average Loss: 0.6294966399188757\n",
      "loss for batch 65 is 0.6291703883418358\n",
      "loss for batch 65 is 0.6273148115341496\n",
      "loss for batch 65 is 0.6298062442403513\n",
      "loss for batch 65 is 0.6270944529800647\n",
      "loss for batch 65 is 0.6271348425364895\n",
      "loss for batch 65 is 0.6272303153508776\n",
      "loss for batch 65 is 0.6276509067314431\n",
      "loss for batch 65 is 0.627409069091672\n",
      "loss for batch 65 is 0.6258956990318145\n",
      "loss for batch 65 is 0.6266054828102078\n",
      "Epoch 65/300, Average Loss: 0.6275312212648905\n",
      "loss for batch 66 is 0.6272113796353141\n",
      "loss for batch 66 is 0.6252874447600537\n",
      "loss for batch 66 is 0.6278828190450492\n",
      "loss for batch 66 is 0.6250660796252147\n",
      "loss for batch 66 is 0.6251354297894604\n",
      "loss for batch 66 is 0.6252180707318025\n",
      "loss for batch 66 is 0.6256579119882059\n",
      "loss for batch 66 is 0.6254118006067367\n",
      "loss for batch 66 is 0.6238353794965307\n",
      "loss for batch 66 is 0.6245645034708371\n",
      "Epoch 66/300, Average Loss: 0.6255270819149205\n",
      "loss for batch 67 is 0.6252160726768374\n",
      "loss for batch 67 is 0.6232234257441931\n",
      "loss for batch 67 is 0.6259238443490786\n",
      "loss for batch 67 is 0.623002906079839\n",
      "loss for batch 67 is 0.6231010406773745\n",
      "loss for batch 67 is 0.6231718777087806\n",
      "loss for batch 67 is 0.6236310683351056\n",
      "loss for batch 67 is 0.6233808709142126\n",
      "loss for batch 67 is 0.6217403659653539\n",
      "loss for batch 67 is 0.6224890414781504\n",
      "Epoch 67/300, Average Loss: 0.6234880513928925\n",
      "loss for batch 68 is 0.6231875261439945\n",
      "loss for batch 68 is 0.6211245727184126\n",
      "loss for batch 68 is 0.6239309688045145\n",
      "loss for batch 68 is 0.6209046170713265\n",
      "loss for batch 68 is 0.621031236255437\n",
      "loss for batch 68 is 0.6210903518638923\n",
      "loss for batch 68 is 0.621568394338307\n",
      "loss for batch 68 is 0.6213136331379457\n",
      "loss for batch 68 is 0.6196075719995102\n",
      "loss for batch 68 is 0.6203758909534417\n",
      "Epoch 68/300, Average Loss: 0.6214134763286782\n",
      "loss for batch 69 is 0.6211227905244301\n",
      "loss for batch 69 is 0.6189884150103255\n",
      "loss for batch 69 is 0.6219015156234349\n",
      "loss for batch 69 is 0.6187697999026317\n",
      "loss for batch 69 is 0.6189243091284943\n",
      "loss for batch 69 is 0.6189725548246074\n",
      "loss for batch 69 is 0.6194690529563748\n",
      "loss for batch 69 is 0.6192095003592504\n",
      "loss for batch 69 is 0.6174366876688469\n",
      "loss for batch 69 is 0.6182249597735651\n",
      "Epoch 69/300, Average Loss: 0.6193019585771962\n",
      "loss for batch 70 is 0.6190218209160877\n",
      "loss for batch 70 is 0.616814949881046\n",
      "loss for batch 70 is 0.6198353643148025\n",
      "loss for batch 70 is 0.6165984066434403\n",
      "loss for batch 70 is 0.6167801304381191\n",
      "loss for batch 70 is 0.6168185299641334\n",
      "loss for batch 70 is 0.6173330879418738\n",
      "loss for batch 70 is 0.6170685583429122\n",
      "loss for batch 70 is 0.6152279112472309\n",
      "loss for batch 70 is 0.6160363764774953\n",
      "Epoch 70/300, Average Loss: 0.6171535136167141\n",
      "loss for batch 71 is 0.616884728366378\n",
      "loss for batch 71 is 0.6146043888876193\n",
      "loss for batch 71 is 0.617732456825896\n",
      "loss for batch 71 is 0.6143905423408287\n",
      "loss for batch 71 is 0.6145986802529018\n",
      "loss for batch 71 is 0.614628334509534\n",
      "loss for batch 71 is 0.6151605157970087\n",
      "loss for batch 71 is 0.6148907960390833\n",
      "loss for batch 71 is 0.6129812642371689\n",
      "loss for batch 71 is 0.6138102326587211\n",
      "Epoch 71/300, Average Loss: 0.614968193991514\n",
      "loss for batch 72 is 0.6147116367711848\n",
      "loss for batch 72 is 0.6123569476498764\n",
      "loss for batch 72 is 0.6155928045485746\n",
      "loss for batch 72 is 0.612146651192778\n",
      "loss for batch 72 is 0.612380127696765\n",
      "loss for batch 72 is 0.6124022836324339\n",
      "loss for batch 72 is 0.6129515626923425\n",
      "loss for batch 72 is 0.612676415641574\n",
      "loss for batch 72 is 0.6106969868156199\n",
      "loss for batch 72 is 0.6115467731987334\n",
      "Epoch 72/300, Average Loss: 0.6127462189839883\n",
      "loss for batch 73 is 0.6125027963543336\n",
      "loss for batch 73 is 0.6100729317967771\n",
      "loss for batch 73 is 0.6134164889804813\n",
      "loss for batch 73 is 0.6098669826503557\n",
      "loss for batch 73 is 0.6101246276762675\n",
      "loss for batch 73 is 0.610140602567957\n",
      "loss for batch 73 is 0.6107063409506547\n",
      "loss for batch 73 is 0.6104254876570607\n",
      "loss for batch 73 is 0.6083751137586694\n",
      "loss for batch 73 is 0.60924601659373\n",
      "Epoch 73/300, Average Loss: 0.6104877388986287\n",
      "loss for batch 74 is 0.6102582249280322\n",
      "loss for batch 74 is 0.6077525102183605\n",
      "loss for batch 74 is 0.6112036145146058\n",
      "loss for batch 74 is 0.6075519173319417\n",
      "loss for batch 74 is 0.6078323734626975\n",
      "loss for batch 74 is 0.6078438650138517\n",
      "loss for batch 74 is 0.6084252750153623\n",
      "loss for batch 74 is 0.6081383330998786\n",
      "loss for batch 74 is 0.6060159380540429\n",
      "loss for batch 74 is 0.6069081248924518\n",
      "Epoch 74/300, Average Loss: 0.6081930176531224\n",
      "loss for batch 75 is 0.6079780052565524\n",
      "loss for batch 75 is 0.6053958089277772\n",
      "loss for batch 75 is 0.6089543794974128\n",
      "loss for batch 75 is 0.6052016247074915\n",
      "loss for batch 75 is 0.6055035998134571\n",
      "loss for batch 75 is 0.6055120835673046\n",
      "loss for batch 75 is 0.6061086451239333\n",
      "loss for batch 75 is 0.6058154317954338\n",
      "loss for batch 75 is 0.6036201830889644\n",
      "loss for batch 75 is 0.6045338790727911\n",
      "Epoch 75/300, Average Loss: 0.605862364085112\n",
      "loss for batch 76 is 0.6056628977485923\n",
      "loss for batch 76 is 0.603003821661934\n",
      "loss for batch 76 is 0.6066693299487259\n",
      "loss for batch 76 is 0.6028165443951726\n",
      "loss for batch 76 is 0.6031385715180115\n",
      "loss for batch 76 is 0.6031455980091069\n",
      "loss for batch 76 is 0.6037566089140658\n",
      "loss for batch 76 is 0.6034567673698326\n",
      "loss for batch 76 is 0.6011876061089358\n",
      "loss for batch 76 is 0.6021229584995944\n",
      "Epoch 76/300, Average Loss: 0.6034960704173973\n",
      "loss for batch 77 is 0.6033126190722001\n",
      "loss for batch 77 is 0.6005761602770122\n",
      "loss for batch 77 is 0.6043480111496602\n",
      "loss for batch 77 is 0.6003968022607611\n",
      "loss for batch 77 is 0.6007372327381869\n",
      "loss for batch 77 is 0.6007445605805218\n",
      "loss for batch 77 is 0.6013691401091442\n",
      "loss for batch 77 is 0.6010623454721435\n",
      "loss for batch 77 is 0.5987183399155079\n",
      "loss for batch 77 is 0.599675393890067\n",
      "Epoch 77/300, Average Loss: 0.6010940605465205\n",
      "loss for batch 78 is 0.600927028342436\n",
      "loss for batch 78 is 0.5981124771090667\n",
      "loss for batch 78 is 0.6019898897290057\n",
      "loss for batch 78 is 0.5979414277388825\n",
      "loss for batch 78 is 0.5982983890815196\n",
      "loss for batch 78 is 0.5983075246534092\n",
      "loss for batch 78 is 0.5989446270412774\n",
      "loss for batch 78 is 0.5986301936665688\n",
      "loss for batch 78 is 0.5962100029485533\n",
      "loss for batch 78 is 0.5971888273054192\n",
      "Epoch 78/300, Average Loss: 0.5986550387616137\n",
      "loss for batch 79 is 0.5985039740636731\n",
      "loss for batch 79 is 0.5956108451254014\n",
      "loss for batch 79 is 0.5995931553470992\n",
      "loss for batch 79 is 0.5954491879997605\n",
      "loss for batch 79 is 0.5958209928579268\n",
      "loss for batch 79 is 0.5958340394873906\n",
      "loss for batch 79 is 0.5964830180538098\n",
      "loss for batch 79 is 0.5961608009880183\n",
      "loss for batch 79 is 0.5936636729856997\n",
      "loss for batch 79 is 0.5946646193684589\n",
      "Epoch 79/300, Average Loss: 0.5961784306277237\n",
      "loss for batch 80 is 0.5960449256553733\n",
      "loss for batch 80 is 0.5930729215303787\n",
      "loss for batch 80 is 0.5971594297494844\n",
      "loss for batch 80 is 0.5929217165005051\n",
      "loss for batch 80 is 0.5933066366627945\n",
      "loss for batch 80 is 0.5933255191114342\n",
      "loss for batch 80 is 0.5939854886914916\n",
      "loss for batch 80 is 0.5936551893027094\n",
      "loss for batch 80 is 0.591080172903894\n",
      "loss for batch 80 is 0.5921033114353451\n",
      "Epoch 80/300, Average Loss: 0.593665531154341\n",
      "loss for batch 81 is 0.593550293656095\n",
      "loss for batch 81 is 0.5904990934261195\n",
      "loss for batch 81 is 0.5946889238088101\n",
      "loss for batch 81 is 0.590359239960982\n",
      "loss for batch 81 is 0.5907554470598898\n",
      "loss for batch 81 is 0.5907820287510143\n",
      "loss for batch 81 is 0.59145201718944\n",
      "loss for batch 81 is 0.5911132856438966\n",
      "loss for batch 81 is 0.5884593139050821\n",
      "loss for batch 81 is 0.5895048248141873\n",
      "Epoch 81/300, Average Loss: 0.5911164468215516\n",
      "loss for batch 82 is 0.5910200988538307\n",
      "loss for batch 82 is 0.5878896025727842\n",
      "loss for batch 82 is 0.5921817529973725\n",
      "loss for batch 82 is 0.5877620142403647\n",
      "loss for batch 82 is 0.5881675592789745\n",
      "loss for batch 82 is 0.5882037540390019\n",
      "loss for batch 82 is 0.588882856533819\n",
      "loss for batch 82 is 0.5885353754818073\n",
      "loss for batch 82 is 0.5858013645630591\n",
      "loss for batch 82 is 0.5868690763226212\n",
      "Epoch 82/300, Average Loss: 0.5885313454883635\n",
      "loss for batch 83 is 0.5884540866263964\n",
      "loss for batch 83 is 0.5852441483067491\n",
      "loss for batch 83 is 0.5896377706750882\n",
      "loss for batch 83 is 0.5851296110380292\n",
      "loss for batch 83 is 0.5855426697572363\n",
      "loss for batch 83 is 0.58559035884179\n",
      "loss for batch 83 is 0.5862775335827627\n",
      "loss for batch 83 is 0.5859208596385176\n",
      "loss for batch 83 is 0.5831056520655102\n",
      "loss for batch 83 is 0.5841954151512304\n",
      "Epoch 83/300, Average Loss: 0.585909810568331\n",
      "loss for batch 84 is 0.5858517976727906\n",
      "loss for batch 84 is 0.5825622483121439\n",
      "loss for batch 84 is 0.5870561820933766\n",
      "loss for batch 84 is 0.5824615274929149\n",
      "loss for batch 84 is 0.5828806539498446\n",
      "loss for batch 84 is 0.5829415736632836\n",
      "loss for batch 84 is 0.5836362473467857\n",
      "loss for batch 84 is 0.5832698838070839\n",
      "loss for batch 84 is 0.5803724283492077\n",
      "loss for batch 84 is 0.5814842251729607\n",
      "Epoch 84/300, Average Loss: 0.5832516767860392\n",
      "loss for batch 85 is 0.5832138258801913\n",
      "loss for batch 85 is 0.579844996548228\n",
      "loss for batch 85 is 0.5844378022315336\n",
      "loss for batch 85 is 0.5797587355657928\n",
      "loss for batch 85 is 0.5801819333052519\n",
      "loss for batch 85 is 0.5802578804182984\n",
      "loss for batch 85 is 0.5809590754348405\n",
      "loss for batch 85 is 0.5805826427392865\n",
      "loss for batch 85 is 0.5776016554135165\n",
      "loss for batch 85 is 0.5787353301641303\n",
      "Epoch 85/300, Average Loss: 0.580557387770107\n",
      "loss for batch 86 is 0.580539747429077\n",
      "loss for batch 86 is 0.5770917516728897\n",
      "loss for batch 86 is 0.5817823391477713\n",
      "loss for batch 86 is 0.5770208640696376\n",
      "loss for batch 86 is 0.577446146214335\n",
      "loss for batch 86 is 0.5775390144762017\n",
      "loss for batch 86 is 0.5782453811144737\n",
      "loss for batch 86 is 0.5778583840884377\n",
      "loss for batch 86 is 0.5747922824582153\n",
      "loss for batch 86 is 0.5759473818511783\n",
      "Epoch 86/300, Average Loss: 0.5778263292522218\n",
      "loss for batch 87 is 0.5778279705889956\n",
      "loss for batch 87 is 0.5743001696133679\n",
      "loss for batch 87 is 0.5790875755298085\n",
      "loss for batch 87 is 0.5742443227091726\n",
      "loss for batch 87 is 0.5746698724126998\n",
      "loss for batch 87 is 0.5747808209815969\n",
      "loss for batch 87 is 0.5754911971761034\n",
      "loss for batch 87 is 0.575093462608043\n",
      "loss for batch 87 is 0.5719403810598578\n",
      "loss for batch 87 is 0.5731169238424092\n",
      "Epoch 87/300, Average Loss: 0.5750552696522055\n",
      "loss for batch 88 is 0.5750759740154017\n",
      "loss for batch 88 is 0.5714683862337075\n",
      "loss for batch 88 is 0.5763530247210208\n",
      "loss for batch 88 is 0.571429506763059\n",
      "loss for batch 88 is 0.5718542803046437\n",
      "loss for batch 88 is 0.5719852975776355\n",
      "loss for batch 88 is 0.5726990812476531\n",
      "loss for batch 88 is 0.572290719788378\n",
      "loss for batch 88 is 0.5690494164375894\n",
      "loss for batch 88 is 0.5702471207518746\n",
      "Epoch 88/300, Average Loss: 0.5722452807840963\n",
      "loss for batch 89 is 0.5722866419707232\n",
      "loss for batch 89 is 0.5685997619488974\n",
      "loss for batch 89 is 0.5735806572118347\n",
      "loss for batch 89 is 0.5685789130526516\n",
      "loss for batch 89 is 0.5690007635823843\n",
      "loss for batch 89 is 0.5691538160556627\n",
      "loss for batch 89 is 0.5698697980972139\n",
      "loss for batch 89 is 0.5694505814301379\n",
      "loss for batch 89 is 0.5661196258949364\n",
      "loss for batch 89 is 0.5673380571235245\n",
      "Epoch 89/300, Average Loss: 0.5693978616367966\n",
      "loss for batch 90 is 0.5694600857614988\n",
      "loss for batch 90 is 0.5656942201364772\n",
      "loss for batch 90 is 0.5707700921293828\n",
      "loss for batch 90 is 0.5656926472738556\n",
      "loss for batch 90 is 0.5661094586459534\n",
      "loss for batch 90 is 0.5662869745530598\n",
      "loss for batch 90 is 0.5670038686120319\n",
      "loss for batch 90 is 0.5665737246956676\n",
      "loss for batch 90 is 0.5631520261989225\n",
      "loss for batch 90 is 0.5643906569588846\n",
      "Epoch 90/300, Average Loss: 0.5665133754965733\n",
      "loss for batch 91 is 0.566597326161792\n",
      "loss for batch 91 is 0.5627532048201683\n",
      "loss for batch 91 is 0.5679218883362777\n",
      "loss for batch 91 is 0.5627723813167598\n",
      "loss for batch 91 is 0.5631811405442535\n",
      "loss for batch 91 is 0.5633859963043434\n",
      "loss for batch 91 is 0.5641021498761791\n",
      "loss for batch 91 is 0.5636609139338183\n",
      "loss for batch 91 is 0.5601474917021273\n",
      "loss for batch 91 is 0.5614055475440628\n",
      "Epoch 91/300, Average Loss: 0.5635928040539783\n",
      "loss for batch 92 is 0.5636989823128501\n",
      "loss for batch 92 is 0.5597776051847986\n",
      "loss for batch 92 is 0.5650363645662693\n",
      "loss for batch 92 is 0.5598190638879987\n",
      "loss for batch 92 is 0.5602163077724018\n",
      "loss for batch 92 is 0.5604516604064884\n",
      "loss for batch 92 is 0.5611652335430556\n",
      "loss for batch 92 is 0.5607127294280648\n",
      "loss for batch 92 is 0.5571066588063173\n",
      "loss for batch 92 is 0.5583831781942632\n",
      "Epoch 92/300, Average Loss: 0.5606367784102507\n",
      "loss for batch 93 is 0.5607654991104334\n",
      "loss for batch 93 is 0.5567678443613233\n",
      "loss for batch 93 is 0.5621127682437462\n",
      "loss for batch 93 is 0.5568313365145469\n",
      "loss for batch 93 is 0.5572128092677322\n",
      "loss for batch 93 is 0.5574806484827228\n",
      "loss for batch 93 is 0.5581887114312549\n",
      "loss for batch 93 is 0.557723932389794\n",
      "loss for batch 93 is 0.5540221524134221\n",
      "loss for batch 93 is 0.5553145064860552\n",
      "Epoch 93/300, Average Loss: 0.5576420208701032\n",
      "loss for batch 94 is 0.5577879775222138\n",
      "loss for batch 94 is 0.5537136754763682\n",
      "loss for batch 94 is 0.5591446610762095\n",
      "loss for batch 94 is 0.5538018858246981\n",
      "loss for batch 94 is 0.5541660283360359\n",
      "loss for batch 94 is 0.5544691966608405\n",
      "loss for batch 94 is 0.5551705976271779\n",
      "loss for batch 94 is 0.554694602802245\n",
      "loss for batch 94 is 0.5508962541813517\n",
      "loss for batch 94 is 0.5522042947052092\n",
      "Epoch 94/300, Average Loss: 0.554604917421235\n",
      "loss for batch 95 is 0.5547732485129523\n",
      "loss for batch 95 is 0.5506248898843252\n",
      "loss for batch 95 is 0.556139322279566\n",
      "loss for batch 95 is 0.5507409050695882\n",
      "loss for batch 95 is 0.5510844046406637\n",
      "loss for batch 95 is 0.5514271052106041\n",
      "loss for batch 95 is 0.5521203614666054\n",
      "loss for batch 95 is 0.5516334358944298\n",
      "loss for batch 95 is 0.5477379686912618\n",
      "loss for batch 95 is 0.5490606935432785\n",
      "Epoch 95/300, Average Loss: 0.5515342335193275\n",
      "loss for batch 96 is 0.5517277128049709\n",
      "loss for batch 96 is 0.5475075060583882\n",
      "loss for batch 96 is 0.5531011792118007\n",
      "loss for batch 96 is 0.5476536085680426\n",
      "loss for batch 96 is 0.5479716279293585\n",
      "loss for batch 96 is 0.5483584574823637\n",
      "loss for batch 96 is 0.5490411214826557\n",
      "loss for batch 96 is 0.548543223194417\n",
      "loss for batch 96 is 0.5445503864713883\n",
      "loss for batch 96 is 0.5458865060524822\n",
      "Epoch 96/300, Average Loss: 0.5484341329255867\n",
      "loss for batch 97 is 0.5486537324331059\n",
      "loss for batch 97 is 0.5443648340297008\n",
      "loss for batch 97 is 0.5500319194319232\n",
      "loss for batch 97 is 0.544542892839097\n",
      "loss for batch 97 is 0.5448295820767236\n",
      "loss for batch 97 is 0.5452653631760002\n",
      "loss for batch 97 is 0.5459346823018494\n",
      "loss for batch 97 is 0.5454252618610953\n",
      "loss for batch 97 is 0.5413345347075603\n",
      "loss for batch 97 is 0.5426820059886021\n",
      "Epoch 97/300, Average Loss: 0.5453064808845658\n",
      "loss for batch 98 is 0.5455512632397646\n",
      "loss for batch 98 is 0.5411968542467537\n",
      "loss for batch 98 is 0.5469312136731475\n",
      "loss for batch 98 is 0.5414083597461338\n",
      "loss for batch 98 is 0.5416578689215715\n",
      "loss for batch 98 is 0.54214742877604\n",
      "loss for batch 98 is 0.5428007046815688\n",
      "loss for batch 98 is 0.5422793597748865\n",
      "loss for batch 98 is 0.53809027149518\n",
      "loss for batch 98 is 0.5394471699802802\n",
      "Epoch 98/300, Average Loss: 0.5421510494535327\n",
      "loss for batch 99 is 0.5424204749776376\n",
      "loss for batch 99 is 0.5380041362674223\n",
      "loss for batch 99 is 0.5437995447424677\n",
      "loss for batch 99 is 0.5382508917471391\n",
      "loss for batch 99 is 0.5384572446012348\n",
      "loss for batch 99 is 0.5390057318461043\n",
      "loss for batch 99 is 0.5396402612114404\n",
      "loss for batch 99 is 0.5391066492927806\n",
      "loss for batch 99 is 0.5348188272752961\n",
      "loss for batch 99 is 0.536183013276759\n",
      "Epoch 99/300, Average Loss: 0.538968677523828\n",
      "loss for batch 100 is 0.5392622021099168\n",
      "loss for batch 100 is 0.5347876315583936\n",
      "loss for batch 100 is 0.540637440537544\n",
      "loss for batch 100 is 0.5350709570128191\n",
      "loss for batch 100 is 0.5352279814745281\n",
      "loss for batch 100 is 0.5358403212746602\n",
      "loss for batch 100 is 0.5364530700048957\n",
      "loss for batch 100 is 0.5359067368342286\n",
      "loss for batch 100 is 0.5315196125718876\n",
      "loss for batch 100 is 0.532889312231101\n",
      "Epoch 100/300, Average Loss: 0.5357595265609976\n",
      "loss for batch 101 is 0.5360767225303902\n",
      "loss for batch 101 is 0.531548727821333\n",
      "loss for batch 101 is 0.5374465753640583\n",
      "loss for batch 101 is 0.5318717455161689\n",
      "loss for batch 101 is 0.5319732303363779\n",
      "loss for batch 101 is 0.532656362158856\n",
      "loss for batch 101 is 0.5332445763894464\n",
      "loss for batch 101 is 0.5326850318644196\n",
      "loss for batch 101 is 0.5281976974681513\n",
      "loss for batch 101 is 0.529569651627225\n",
      "Epoch 101/300, Average Loss: 0.5325270321076427\n",
      "loss for batch 102 is 0.5328654255816837\n",
      "loss for batch 102 is 0.5282863785586852\n",
      "loss for batch 102 is 0.5342258890265741\n",
      "loss for batch 102 is 0.5286463241460129\n",
      "loss for batch 102 is 0.5286846357885941\n",
      "loss for batch 102 is 0.5294388943988976\n",
      "loss for batch 102 is 0.529997222262505\n",
      "loss for batch 102 is 0.5294239611051\n",
      "loss for batch 102 is 0.5248319444098727\n",
      "loss for batch 102 is 0.5262048813610377\n",
      "Epoch 102/300, Average Loss: 0.5292605556638963\n",
      "loss for batch 103 is 0.5296126588502249\n",
      "loss for batch 103 is 0.5249830597604797\n",
      "loss for batch 103 is 0.5309664716170358\n",
      "loss for batch 103 is 0.5253842656737479\n",
      "loss for batch 103 is 0.5253597811661556\n",
      "loss for batch 103 is 0.526189170884721\n",
      "loss for batch 103 is 0.5267176942122959\n",
      "loss for batch 103 is 0.526132967068441\n",
      "loss for batch 103 is 0.5214364102761345\n",
      "loss for batch 103 is 0.5228101299135577\n",
      "Epoch 103/300, Average Loss: 0.5259592609422794\n",
      "loss for batch 104 is 0.5263333082375425\n",
      "loss for batch 104 is 0.521657866818322\n",
      "loss for batch 104 is 0.5276806001141731\n",
      "loss for batch 104 is 0.522104631614375\n",
      "loss for batch 104 is 0.5220122902264509\n",
      "loss for batch 104 is 0.5229233765402876\n",
      "loss for batch 104 is 0.5234197021501452\n",
      "loss for batch 104 is 0.5228235988822573\n",
      "loss for batch 104 is 0.5180228199137781\n",
      "loss for batch 104 is 0.5193955590688935\n",
      "Epoch 104/300, Average Loss: 0.5226373753566225\n",
      "loss for batch 105 is 0.5230361750057078\n",
      "loss for batch 105 is 0.5183219929848621\n",
      "loss for batch 105 is 0.524376999576242\n",
      "loss for batch 105 is 0.5188217664532514\n",
      "loss for batch 105 is 0.5186554738211666\n",
      "loss for batch 105 is 0.5196592356511934\n",
      "loss for batch 105 is 0.5201225524966516\n",
      "loss for batch 105 is 0.5195161801700652\n",
      "loss for batch 105 is 0.5146159924213741\n",
      "loss for batch 105 is 0.5159882373815896\n",
      "Epoch 105/300, Average Loss: 0.5193114605962105\n",
      "loss for batch 106 is 0.519747020573004\n",
      "loss for batch 106 is 0.5150045650647099\n",
      "loss for batch 106 is 0.5210746325164937\n",
      "loss for batch 106 is 0.5155542802359357\n",
      "loss for batch 106 is 0.5152973937264562\n",
      "loss for batch 106 is 0.5164007603405574\n",
      "loss for batch 106 is 0.5168249381170225\n",
      "loss for batch 106 is 0.5162046689143904\n",
      "loss for batch 106 is 0.5112070000422608\n",
      "loss for batch 106 is 0.5125736714898856\n",
      "Epoch 106/300, Average Loss: 0.5159888931020716\n",
      "loss for batch 107 is 0.5164465452776039\n",
      "loss for batch 107 is 0.5116813550205572\n",
      "loss for batch 107 is 0.5177537466399904\n",
      "loss for batch 107 is 0.5122808377388519\n",
      "loss for batch 107 is 0.5119256398821312\n",
      "loss for batch 107 is 0.5131346838361341\n",
      "loss for batch 107 is 0.5135161342020419\n",
      "loss for batch 107 is 0.5128806814224476\n",
      "loss for batch 107 is 0.5077855040284034\n",
      "loss for batch 107 is 0.509143393003911\n",
      "Epoch 107/300, Average Loss: 0.5126548521052073\n",
      "loss for batch 108 is 0.5131319708894174\n",
      "loss for batch 108 is 0.5083490389511792\n",
      "loss for batch 108 is 0.5144158721185258\n",
      "loss for batch 108 is 0.5090002814104135\n",
      "loss for batch 108 is 0.5085399458563845\n",
      "loss for batch 108 is 0.5098610798573032\n",
      "loss for batch 108 is 0.5101972943958921\n",
      "loss for batch 108 is 0.5095463563308714\n",
      "loss for batch 108 is 0.5043541096173573\n",
      "loss for batch 108 is 0.5057015684655473\n",
      "Epoch 108/300, Average Loss: 0.5093097517892892\n",
      "loss for batch 109 is 0.509808382884338\n",
      "loss for batch 109 is 0.5050165811533712\n",
      "loss for batch 109 is 0.5110670567052763\n",
      "loss for batch 109 is 0.5057248613375849\n",
      "loss for batch 109 is 0.505147794913222\n",
      "loss for batch 109 is 0.5065926772565335\n",
      "loss for batch 109 is 0.5068787194740938\n",
      "loss for batch 109 is 0.5062119740552217\n",
      "loss for batch 109 is 0.5009246042224624\n",
      "loss for batch 109 is 0.5022557944713633\n",
      "Epoch 109/300, Average Loss: 0.5059628446473468\n",
      "loss for batch 110 is 0.5064824139342124\n",
      "loss for batch 110 is 0.5016854960006359\n",
      "loss for batch 110 is 0.5077084332072999\n",
      "loss for batch 110 is 0.5024492478826708\n",
      "loss for batch 110 is 0.5017474272033309\n",
      "loss for batch 110 is 0.5033184880901361\n",
      "loss for batch 110 is 0.5035502217090632\n",
      "loss for batch 110 is 0.5028663607217694\n",
      "loss for batch 110 is 0.4974819350966902\n",
      "loss for batch 110 is 0.4987943842065805\n",
      "Epoch 110/300, Average Loss: 0.502608440805239\n",
      "loss for batch 111 is 0.5031430792039949\n",
      "loss for batch 111 is 0.4983450433282267\n",
      "loss for batch 111 is 0.5043364241009359\n",
      "loss for batch 111 is 0.4991666343194743\n",
      "loss for batch 111 is 0.4983362903870862\n",
      "loss for batch 111 is 0.5000380884950447\n",
      "loss for batch 111 is 0.5002137127964928\n",
      "loss for batch 111 is 0.49951278465525545\n",
      "loss for batch 111 is 0.49403123263683174\n",
      "loss for batch 111 is 0.4953231101990192\n",
      "Epoch 111/300, Average Loss: 0.4992446400122363\n",
      "loss for batch 112 is 0.4997954398261577\n",
      "loss for batch 112 is 0.49500239133317\n",
      "loss for batch 112 is 0.5009553931025691\n",
      "loss for batch 112 is 0.49588368893304996\n",
      "loss for batch 112 is 0.4949189883750639\n",
      "loss for batch 112 is 0.49675695078595544\n",
      "loss for batch 112 is 0.49687384524286055\n",
      "loss for batch 112 is 0.4961552900433098\n",
      "loss for batch 112 is 0.49057652595876344\n",
      "loss for batch 112 is 0.49184548778099013\n",
      "Epoch 112/300, Average Loss: 0.49587640013818907\n",
      "loss for batch 113 is 0.4964422723355807\n",
      "loss for batch 113 is 0.4916601267265356\n",
      "loss for batch 113 is 0.49756692844617345\n",
      "loss for batch 113 is 0.49260342227849524\n",
      "loss for batch 113 is 0.4914969796689671\n",
      "loss for batch 113 is 0.493478624967446\n",
      "loss for batch 113 is 0.49353329993693723\n",
      "loss for batch 113 is 0.4927966234119696\n",
      "loss for batch 113 is 0.48712176293104037\n",
      "loss for batch 113 is 0.48836403532193\n",
      "Epoch 113/300, Average Loss: 0.4925064076025075\n",
      "loss for batch 114 is 0.49308824570753856\n",
      "loss for batch 114 is 0.48832401240585566\n",
      "loss for batch 114 is 0.49417374063939395\n",
      "loss for batch 114 is 0.48933037688309267\n",
      "loss for batch 114 is 0.48807358564223585\n",
      "loss for batch 114 is 0.49020321546448287\n",
      "loss for batch 114 is 0.4901910930365602\n",
      "loss for batch 114 is 0.4894355439589741\n",
      "loss for batch 114 is 0.4836628347131227\n",
      "loss for batch 114 is 0.4848763063970935\n",
      "Epoch 114/300, Average Loss: 0.489135895484835\n",
      "loss for batch 115 is 0.4897293321974149\n",
      "loss for batch 115 is 0.4849862314826085\n",
      "loss for batch 115 is 0.49077770848405106\n",
      "loss for batch 115 is 0.48605666955373544\n",
      "loss for batch 115 is 0.4846499500181857\n",
      "loss for batch 115 is 0.48693065137213426\n",
      "loss for batch 115 is 0.4868533807666074\n",
      "loss for batch 115 is 0.48607899561509277\n",
      "loss for batch 115 is 0.48020947126315466\n",
      "loss for batch 115 is 0.48139390873563054\n",
      "Epoch 115/300, Average Loss: 0.48576662994886155\n",
      "loss for batch 116 is 0.4863789701591697\n",
      "loss for batch 116 is 0.48166612389759095\n",
      "loss for batch 116 is 0.4873891003982873\n",
      "loss for batch 116 is 0.48280460460408814\n",
      "loss for batch 116 is 0.4812395840387729\n",
      "loss for batch 116 is 0.48368043779687797\n",
      "loss for batch 116 is 0.48353547699235444\n",
      "loss for batch 116 is 0.4827384287371571\n",
      "loss for batch 116 is 0.47676974645715436\n",
      "loss for batch 116 is 0.47792089679537614\n",
      "Epoch 116/300, Average Loss: 0.4824123369876829\n",
      "loss for batch 117 is 0.48303426274810607\n",
      "loss for batch 117 is 0.478365977278615\n",
      "loss for batch 117 is 0.48400754491367914\n",
      "loss for batch 117 is 0.4795690518072187\n",
      "loss for batch 117 is 0.47783433899269967\n",
      "loss for batch 117 is 0.4804443050482314\n",
      "loss for batch 117 is 0.48022697079089766\n",
      "loss for batch 117 is 0.4794047210425435\n",
      "loss for batch 117 is 0.47333426827218417\n",
      "loss for batch 117 is 0.47444877932909085\n",
      "Epoch 117/300, Average Loss: 0.4790670220223266\n",
      "loss for batch 118 is 0.4796918811799934\n",
      "loss for batch 118 is 0.4750713045510629\n",
      "loss for batch 118 is 0.48062615178625984\n",
      "loss for batch 118 is 0.4763365339798104\n",
      "loss for batch 118 is 0.4744325747225569\n",
      "loss for batch 118 is 0.47720749712447413\n",
      "loss for batch 118 is 0.4769166846443064\n",
      "loss for batch 118 is 0.47606936365081476\n",
      "loss for batch 118 is 0.4698929202817534\n",
      "loss for batch 118 is 0.47096979181310916\n",
      "Epoch 118/300, Average Loss: 0.47572147037341417\n",
      "loss for batch 119 is 0.4763454272499439\n",
      "loss for batch 119 is 0.4717656833770934\n",
      "loss for batch 119 is 0.47724853103577136\n",
      "loss for batch 119 is 0.4730882733383648\n",
      "loss for batch 119 is 0.47103140210333366\n",
      "loss for batch 119 is 0.4739549713548573\n",
      "loss for batch 119 is 0.47359184908644836\n",
      "loss for batch 119 is 0.4727206908459928\n",
      "loss for batch 119 is 0.466428338744064\n",
      "loss for batch 119 is 0.46746795661712104\n",
      "Epoch 119/300, Average Loss: 0.472364312375299\n",
      "loss for batch 120 is 0.472976861722703\n",
      "loss for batch 120 is 0.46843265029359443\n",
      "loss for batch 120 is 0.473859913121721\n",
      "loss for batch 120 is 0.46981050889135795\n",
      "loss for batch 120 is 0.46761465836907246\n",
      "loss for batch 120 is 0.47067920633335386\n",
      "loss for batch 120 is 0.4702475375967829\n",
      "loss for batch 120 is 0.4693579947849652\n",
      "loss for batch 120 is 0.462946565704187\n",
      "loss for batch 120 is 0.46395236333365064\n",
      "Epoch 120/300, Average Loss: 0.4689878260151389\n",
      "loss for batch 121 is 0.4695970521599975\n",
      "loss for batch 121 is 0.46508193757033367\n",
      "loss for batch 121 is 0.4704701533407969\n",
      "loss for batch 121 is 0.46651472550708734\n",
      "loss for batch 121 is 0.46419513774380355\n",
      "loss for batch 121 is 0.46738827688074835\n",
      "loss for batch 121 is 0.46688964706551705\n",
      "loss for batch 121 is 0.4659839847841833\n",
      "loss for batch 121 is 0.4594484123340906\n",
      "loss for batch 121 is 0.4604238485449725\n",
      "Epoch 121/300, Average Loss: 0.4655993175931531\n",
      "loss for batch 122 is 0.46620919330771665\n",
      "loss for batch 122 is 0.4617256747156828\n",
      "loss for batch 122 is 0.46708731619116606\n",
      "loss for batch 122 is 0.4632212235148123\n",
      "loss for batch 122 is 0.46079016156653396\n",
      "loss for batch 122 is 0.46411075605320445\n",
      "loss for batch 122 is 0.46355016592078546\n",
      "loss for batch 122 is 0.4626324465021013\n",
      "loss for batch 122 is 0.45597126451122005\n",
      "loss for batch 122 is 0.4569187337008906\n",
      "Epoch 122/300, Average Loss: 0.4622216935984113\n",
      "loss for batch 123 is 0.46284519645938527\n",
      "loss for batch 123 is 0.45839821735111647\n",
      "loss for batch 123 is 0.46373747557444683\n",
      "loss for batch 123 is 0.45995573978512494\n",
      "loss for batch 123 is 0.4574168104134271\n",
      "loss for batch 123 is 0.4608641254114245\n",
      "loss for batch 123 is 0.46024090285067387\n",
      "loss for batch 123 is 0.4593114266165377\n",
      "loss for batch 123 is 0.4525196224745705\n",
      "loss for batch 123 is 0.45343973270455046\n",
      "Epoch 123/300, Average Loss: 0.4588729249641258\n",
      "loss for batch 124 is 0.4595061000661095\n",
      "loss for batch 124 is 0.45510191299121106\n",
      "loss for batch 124 is 0.4604193267714232\n",
      "loss for batch 124 is 0.45671948403232093\n",
      "loss for batch 124 is 0.4540743459661164\n",
      "loss for batch 124 is 0.4576470301777537\n",
      "loss for batch 124 is 0.45696075933219743\n",
      "loss for batch 124 is 0.45601834697140065\n",
      "loss for batch 124 is 0.4490931722953406\n",
      "loss for batch 124 is 0.44998545237161497\n",
      "Epoch 124/300, Average Loss: 0.4555525930975488\n",
      "loss for batch 125 is 0.45618948406279\n",
      "loss for batch 125 is 0.45183274047381095\n",
      "loss for batch 125 is 0.4571267734634317\n",
      "loss for batch 125 is 0.45350808625525535\n",
      "loss for batch 125 is 0.4507585264754047\n",
      "loss for batch 125 is 0.45445590210416165\n",
      "loss for batch 125 is 0.45370767333327894\n",
      "loss for batch 125 is 0.4527536789134967\n",
      "loss for batch 125 is 0.4456941923217477\n",
      "loss for batch 125 is 0.446560220742275\n",
      "Epoch 125/300, Average Loss: 0.4522587278145653\n",
      "loss for batch 126 is 0.45290197220219924\n",
      "loss for batch 126 is 0.44859679619804166\n",
      "loss for batch 126 is 0.45387153084272813\n",
      "loss for batch 126 is 0.45032970413335205\n",
      "loss for batch 126 is 0.44748317427781875\n",
      "loss for batch 126 is 0.4513013309853276\n",
      "loss for batch 126 is 0.45049247920806174\n",
      "loss for batch 126 is 0.4495278041697765\n",
      "loss for batch 126 is 0.44233026180096524\n",
      "loss for batch 126 is 0.4431723818876018\n",
      "Epoch 126/300, Average Loss: 0.4490007435705873\n",
      "loss for batch 127 is 0.44965376775140486\n",
      "loss for batch 127 is 0.4454016950672424\n",
      "loss for batch 127 is 0.450664081585511\n",
      "loss for batch 127 is 0.44719239648803305\n",
      "loss for batch 127 is 0.4442588278604797\n",
      "loss for batch 127 is 0.44818845022708304\n",
      "loss for batch 127 is 0.4473195234868399\n",
      "loss for batch 127 is 0.446349439295095\n",
      "loss for batch 127 is 0.43901105990759626\n",
      "loss for batch 127 is 0.43982625944260906\n",
      "Epoch 127/300, Average Loss: 0.4457865501111894\n",
      "loss for batch 128 is 0.4464464285582397\n",
      "loss for batch 128 is 0.4422334311143105\n",
      "loss for batch 128 is 0.44750416372329094\n",
      "loss for batch 128 is 0.4440748696016077\n",
      "loss for batch 128 is 0.4410785149066472\n",
      "loss for batch 128 is 0.4450989742606562\n",
      "loss for batch 128 is 0.4441742210289644\n",
      "loss for batch 128 is 0.44320259054359096\n",
      "loss for batch 128 is 0.4357132226176058\n",
      "loss for batch 128 is 0.4365058839170658\n",
      "Epoch 128/300, Average Loss: 0.4426032300271979\n",
      "loss for batch 129 is 0.44327037957184495\n",
      "loss for batch 129 is 0.4390948107284043\n",
      "loss for batch 129 is 0.4443913771909818\n",
      "loss for batch 129 is 0.4409908572929675\n",
      "loss for batch 129 is 0.43794819975402005\n",
      "loss for batch 129 is 0.4420504375043725\n",
      "loss for batch 129 is 0.44107382574933546\n",
      "loss for batch 129 is 0.44010471735565243\n",
      "loss for batch 129 is 0.43246361796539257\n",
      "loss for batch 129 is 0.4332352791448664\n",
      "Epoch 129/300, Average Loss: 0.43946235022578384\n",
      "loss for batch 130 is 0.4401460665569083\n",
      "loss for batch 130 is 0.43600430057754974\n",
      "loss for batch 130 is 0.44133530961309814\n",
      "loss for batch 130 is 0.43794863946524365\n",
      "loss for batch 130 is 0.4348727280567975\n",
      "loss for batch 130 is 0.43904546267145117\n",
      "loss for batch 130 is 0.43801777368486566\n",
      "loss for batch 130 is 0.43705316658123755\n",
      "loss for batch 130 is 0.4292477942992207\n",
      "loss for batch 130 is 0.4299996542116246\n",
      "Epoch 130/300, Average Loss: 0.43636708957179965\n",
      "loss for batch 131 is 0.4370594120992314\n",
      "loss for batch 131 is 0.4329546836634285\n",
      "loss for batch 131 is 0.43833217557688997\n",
      "loss for batch 131 is 0.43495058542852255\n",
      "loss for batch 131 is 0.4318510424295584\n",
      "loss for batch 131 is 0.4360908395179673\n",
      "loss for batch 131 is 0.43501418695320837\n",
      "loss for batch 131 is 0.4340569313656557\n",
      "loss for batch 131 is 0.4260835814389241\n",
      "loss for batch 131 is 0.42681630119121255\n",
      "Epoch 131/300, Average Loss: 0.4333209739664599\n",
      "loss for batch 132 is 0.4340267040343366\n",
      "loss for batch 132 is 0.4299616153830315\n",
      "loss for batch 132 is 0.43538900509952216\n",
      "loss for batch 132 is 0.4320063830846233\n",
      "loss for batch 132 is 0.42888965834234394\n",
      "loss for batch 132 is 0.4331912854988017\n",
      "loss for batch 132 is 0.43206583301144913\n",
      "loss for batch 132 is 0.4311178885068499\n",
      "loss for batch 132 is 0.4229698922151317\n",
      "loss for batch 132 is 0.4236841818734403\n",
      "Epoch 132/300, Average Loss: 0.430330244704953\n",
      "loss for batch 133 is 0.43104774773732935\n",
      "loss for batch 133 is 0.42702271329687047\n",
      "loss for batch 133 is 0.4325085237246796\n",
      "loss for batch 133 is 0.42911325373563103\n",
      "loss for batch 133 is 0.42598908752966874\n",
      "loss for batch 133 is 0.43034281131030583\n",
      "loss for batch 133 is 0.42916674385725867\n",
      "loss for batch 133 is 0.4282320539905433\n",
      "loss for batch 133 is 0.4198973747049186\n",
      "loss for batch 133 is 0.42059190485980563\n",
      "Epoch 133/300, Average Loss: 0.42739122147470116\n",
      "loss for batch 134 is 0.42810958776794006\n",
      "loss for batch 134 is 0.42411609666109723\n",
      "loss for batch 134 is 0.4296825473338364\n",
      "loss for batch 134 is 0.42625220865606916\n",
      "loss for batch 134 is 0.42314209427653043\n",
      "loss for batch 134 is 0.4275331943152475\n",
      "loss for batch 134 is 0.4263120174800271\n",
      "loss for batch 134 is 0.4253979493639082\n",
      "loss for batch 134 is 0.41687297347400637\n",
      "loss for batch 134 is 0.4175509468715557\n",
      "Epoch 134/300, Average Loss: 0.4244969616200217\n",
      "loss for batch 135 is 0.42523017375692634\n",
      "loss for batch 135 is 0.42126299594625743\n",
      "loss for batch 135 is 0.42692355575209295\n",
      "loss for batch 135 is 0.42343948416469246\n",
      "loss for batch 135 is 0.4203622622251973\n",
      "loss for batch 135 is 0.4247722248268127\n",
      "loss for batch 135 is 0.42350820406487627\n",
      "loss for batch 135 is 0.4226202298957634\n",
      "loss for batch 135 is 0.41389506584208285\n",
      "loss for batch 135 is 0.4145577408921667\n",
      "Epoch 135/300, Average Loss: 0.42165719373668686\n",
      "loss for batch 136 is 0.42240596604043223\n",
      "loss for batch 136 is 0.41845730053786\n",
      "loss for batch 136 is 0.42422855506682\n",
      "loss for batch 136 is 0.4206762041689396\n",
      "loss for batch 136 is 0.41765171824996206\n",
      "loss for batch 136 is 0.42206980847816866\n",
      "loss for batch 136 is 0.42076350607372254\n",
      "loss for batch 136 is 0.4199057403255971\n",
      "loss for batch 136 is 0.41097084897144676\n",
      "loss for batch 136 is 0.41161739439686473\n",
      "Epoch 136/300, Average Loss: 0.4188747042309814\n",
      "loss for batch 137 is 0.41964085717606997\n",
      "loss for batch 137 is 0.4157023177837524\n",
      "loss for batch 137 is 0.4216022598194492\n",
      "loss for batch 137 is 0.4179612197611368\n",
      "loss for batch 137 is 0.4150088376624226\n",
      "loss for batch 137 is 0.4194191311308148\n",
      "loss for batch 137 is 0.4180735898048126\n",
      "loss for batch 137 is 0.4172528553522976\n",
      "loss for batch 137 is 0.4080975851667275\n",
      "loss for batch 137 is 0.40872975868903166\n",
      "Epoch 137/300, Average Loss: 0.41614884123465146\n",
      "loss for batch 138 is 0.41693597493248835\n",
      "loss for batch 138 is 0.413004272096571\n",
      "loss for batch 138 is 0.41904679761233626\n",
      "loss for batch 138 is 0.41530350997520127\n",
      "loss for batch 138 is 0.41243733294487694\n",
      "loss for batch 138 is 0.41683074663120834\n",
      "loss for batch 138 is 0.4154465665109536\n",
      "loss for batch 138 is 0.41466838218337054\n",
      "loss for batch 138 is 0.4052821488200855\n",
      "loss for batch 138 is 0.40590026903441556\n",
      "Epoch 138/300, Average Loss: 0.41348560007415075\n",
      "loss for batch 139 is 0.4142966254070032\n",
      "loss for batch 139 is 0.41036910437140195\n",
      "loss for batch 139 is 0.41656524000020584\n",
      "loss for batch 139 is 0.41270809970713224\n",
      "loss for batch 139 is 0.4099403329503601\n",
      "loss for batch 139 is 0.4143085050207428\n",
      "loss for batch 139 is 0.4128853652086702\n",
      "loss for batch 139 is 0.412155193380354\n",
      "loss for batch 139 is 0.40252636307057893\n",
      "loss for batch 139 is 0.4031302286309272\n",
      "Epoch 139/300, Average Loss: 0.4108885057747377\n",
      "loss for batch 140 is 0.4117246843029725\n",
      "loss for batch 140 is 0.4077980077881375\n",
      "loss for batch 140 is 0.4141592402401626\n",
      "loss for batch 140 is 0.4101757564458245\n",
      "loss for batch 140 is 0.40751912889828035\n",
      "loss for batch 140 is 0.411853091377288\n",
      "loss for batch 140 is 0.4103904801419204\n",
      "loss for batch 140 is 0.4097142722903764\n",
      "loss for batch 140 is 0.39982984787346676\n",
      "loss for batch 140 is 0.40041936715229787\n",
      "Epoch 140/300, Average Loss: 0.4083583876510727\n",
      "loss for batch 141 is 0.4092209075192628\n",
      "loss for batch 141 is 0.40529086793602226\n",
      "loss for batch 141 is 0.41183024928510703\n",
      "loss for batch 141 is 0.4077063224762868\n",
      "loss for batch 141 is 0.4051750720481599\n",
      "loss for batch 141 is 0.40946490499263716\n",
      "loss for batch 141 is 0.40796232368161583\n",
      "loss for batch 141 is 0.4073468193818976\n",
      "loss for batch 141 is 0.39719226459376633\n",
      "loss for batch 141 is 0.39776756871816343\n",
      "Epoch 141/300, Average Loss: 0.4058957300632919\n",
      "loss for batch 142 is 0.4067863294564615\n",
      "loss for batch 142 is 0.402847706230081\n",
      "loss for batch 142 is 0.40958008030615023\n",
      "loss for batch 142 is 0.40529980477791394\n",
      "loss for batch 142 is 0.40290985780979693\n",
      "loss for batch 142 is 0.40714460535606684\n",
      "loss for batch 142 is 0.40560155511747886\n",
      "loss for batch 142 is 0.40505435475267487\n",
      "loss for batch 142 is 0.39461355813816756\n",
      "loss for batch 142 is 0.39517505497912214\n",
      "Epoch 142/300, Average Loss: 0.4035012906923914\n",
      "loss for batch 143 is 0.4044223423019539\n",
      "loss for batch 143 is 0.40046893376655923\n",
      "loss for batch 143 is 0.40741081956672714\n",
      "loss for batch 143 is 0.4029566382477497\n",
      "loss for batch 143 is 0.4007255306874701\n",
      "loss for batch 143 is 0.40489331514179666\n",
      "loss for batch 143 is 0.40330923952781317\n",
      "loss for batch 143 is 0.4028388509349009\n",
      "loss for batch 143 is 0.392094090189613\n",
      "loss for batch 143 is 0.39264243402002574\n",
      "Epoch 143/300, Average Loss: 0.401176219438461\n",
      "loss for batch 144 is 0.40213043314557756\n",
      "loss for batch 144 is 0.3981549234757245\n",
      "loss for batch 144 is 0.40532433358007464\n",
      "loss for batch 144 is 0.40067757433747864\n",
      "loss for batch 144 is 0.39862329693162796\n",
      "loss for batch 144 is 0.40271149666570466\n",
      "loss for batch 144 is 0.401084928192394\n",
      "loss for batch 144 is 0.4007011416037244\n",
      "loss for batch 144 is 0.38963446945062497\n",
      "loss for batch 144 is 0.3901706887444904\n",
      "Epoch 144/300, Average Loss: 0.39892132861274215\n",
      "loss for batch 145 is 0.39991038024877146\n",
      "loss for batch 145 is 0.3959059277280269\n",
      "loss for batch 145 is 0.4033206976083272\n",
      "loss for batch 145 is 0.39846323066905814\n",
      "loss for batch 145 is 0.396602089627988\n",
      "loss for batch 145 is 0.4006015870914027\n",
      "loss for batch 145 is 0.3989301588179327\n",
      "loss for batch 145 is 0.3986436537614253\n",
      "loss for batch 145 is 0.3872341575210279\n",
      "loss for batch 145 is 0.38776070417650144\n",
      "Epoch 145/300, Average Loss: 0.39673725872504617\n",
      "loss for batch 146 is 0.39776575883603\n",
      "loss for batch 146 is 0.3937285134720682\n",
      "loss for batch 146 is 0.401405479444283\n",
      "loss for batch 146 is 0.3963218744913745\n",
      "loss for batch 146 is 0.3946700213866746\n",
      "loss for batch 146 is 0.3985717771970525\n",
      "loss for batch 146 is 0.3968550375050037\n",
      "loss for batch 146 is 0.39667521173093073\n",
      "loss for batch 146 is 0.3849023693378655\n",
      "loss for batch 146 is 0.38541970449130647\n",
      "Epoch 146/300, Average Loss: 0.3946315747892589\n",
      "loss for batch 147 is 0.39570497678797645\n",
      "loss for batch 147 is 0.391625968531687\n",
      "loss for batch 147 is 0.3995841632698382\n",
      "loss for batch 147 is 0.3942526623344601\n",
      "loss for batch 147 is 0.39283158660896095\n",
      "loss for batch 147 is 0.3966194577641272\n",
      "loss for batch 147 is 0.39485575329606726\n",
      "loss for batch 147 is 0.3947947062521395\n",
      "loss for batch 147 is 0.38263427084638635\n",
      "loss for batch 147 is 0.383144940455459\n",
      "Epoch 147/300, Average Loss: 0.3926048486147102\n",
      "loss for batch 148 is 0.3937265562070967\n",
      "loss for batch 148 is 0.3895898000204694\n",
      "loss for batch 148 is 0.39785781053103253\n",
      "loss for batch 148 is 0.39224333966371894\n",
      "loss for batch 148 is 0.39108926927940385\n",
      "loss for batch 148 is 0.3947369116260968\n",
      "loss for batch 148 is 0.3929258333906998\n",
      "loss for batch 148 is 0.3929999350066788\n",
      "loss for batch 148 is 0.3804239913384801\n",
      "loss for batch 148 is 0.38093327189767257\n",
      "Epoch 148/300, Average Loss: 0.39065267189613495\n",
      "loss for batch 149 is 0.39183045002689276\n",
      "loss for batch 149 is 0.3876168594315435\n",
      "loss for batch 149 is 0.3962277868403998\n",
      "loss for batch 149 is 0.39029728348776055\n",
      "loss for batch 149 is 0.3894422938005686\n",
      "loss for batch 149 is 0.3929329791860064\n",
      "loss for batch 149 is 0.3910747213818651\n",
      "loss for batch 149 is 0.3913013707380936\n",
      "loss for batch 149 is 0.37829137432647575\n",
      "loss for batch 149 is 0.37880366527953013\n",
      "Epoch 149/300, Average Loss: 0.3887818784499136\n",
      "loss for batch 150 is 0.3900321784072106\n",
      "loss for batch 150 is 0.38573625761307034\n",
      "loss for batch 150 is 0.3947011420703145\n",
      "loss for batch 150 is 0.3884497399292775\n",
      "loss for batch 150 is 0.3879037450307867\n",
      "loss for batch 150 is 0.3912357807268238\n",
      "loss for batch 150 is 0.38932223860728127\n",
      "loss for batch 150 is 0.38971560011473555\n",
      "loss for batch 150 is 0.3762466196454422\n",
      "loss for batch 150 is 0.37676485938246496\n",
      "Epoch 150/300, Average Loss: 0.38701081615274074\n",
      "loss for batch 151 is 0.38834145194917563\n",
      "loss for batch 151 is 0.38395103256209634\n",
      "loss for batch 151 is 0.39329084508779755\n",
      "loss for batch 151 is 0.3866986189681449\n",
      "loss for batch 151 is 0.38648615884046533\n",
      "loss for batch 151 is 0.3896413919768192\n",
      "loss for batch 151 is 0.3876678897906961\n",
      "loss for batch 151 is 0.3882453998870582\n",
      "loss for batch 151 is 0.3742947042112636\n",
      "loss for batch 151 is 0.37482307842241996\n",
      "Epoch 151/300, Average Loss: 0.38534405716959363\n",
      "loss for batch 152 is 0.38676830609838253\n",
      "loss for batch 152 is 0.3822714614274646\n",
      "loss for batch 152 is 0.391997291405697\n",
      "loss for batch 152 is 0.38506258414748706\n",
      "loss for batch 152 is 0.38519228292437163\n",
      "loss for batch 152 is 0.38817477497728514\n",
      "loss for batch 152 is 0.38613280271548317\n",
      "loss for batch 152 is 0.38690931397816286\n",
      "loss for batch 152 is 0.37246722784273323\n",
      "loss for batch 152 is 0.37300159623504653\n",
      "Epoch 152/300, Average Loss: 0.3837977641752114\n",
      "loss for batch 153 is 0.3853319647973075\n",
      "loss for batch 153 is 0.38072764442827656\n",
      "loss for batch 153 is 0.3908179864647653\n",
      "loss for batch 153 is 0.38356369018616077\n",
      "loss for batch 153 is 0.3840203834047902\n",
      "loss for batch 153 is 0.386839882418693\n",
      "loss for batch 153 is 0.384713185060043\n",
      "loss for batch 153 is 0.3856975589950768\n",
      "loss for batch 153 is 0.3707464126381339\n",
      "loss for batch 153 is 0.3712835284171911\n",
      "Epoch 153/300, Average Loss: 0.3823742236810438\n",
      "loss for batch 154 is 0.38401137369673427\n",
      "loss for batch 154 is 0.3792929216013084\n",
      "loss for batch 154 is 0.3897441721606612\n",
      "loss for batch 154 is 0.3821682296923039\n",
      "loss for batch 154 is 0.38295673042511225\n",
      "loss for batch 154 is 0.38560933866119174\n",
      "loss for batch 154 is 0.383389277296232\n",
      "loss for batch 154 is 0.38459365619780933\n",
      "loss for batch 154 is 0.3691116397894233\n",
      "loss for batch 154 is 0.3696551918855572\n",
      "Epoch 154/300, Average Loss: 0.3810532531406333\n",
      "loss for batch 155 is 0.38279209349981436\n",
      "loss for batch 155 is 0.3779518467593317\n",
      "loss for batch 155 is 0.3887714228479746\n",
      "loss for batch 155 is 0.38085999352492356\n",
      "loss for batch 155 is 0.3819943625422656\n",
      "loss for batch 155 is 0.3844685467313901\n",
      "loss for batch 155 is 0.3821481364116974\n",
      "loss for batch 155 is 0.3835838288372101\n",
      "loss for batch 155 is 0.36753886232631205\n",
      "loss for batch 155 is 0.3680949352250828\n",
      "Epoch 155/300, Average Loss: 0.37982040287060026\n",
      "loss for batch 156 is 0.38164592458268665\n",
      "loss for batch 156 is 0.37665917792738285\n",
      "loss for batch 156 is 0.3878864812819319\n",
      "loss for batch 156 is 0.3795735925437696\n",
      "loss for batch 156 is 0.381110209855894\n",
      "loss for batch 156 is 0.38336696914255275\n",
      "loss for batch 156 is 0.38095578598988666\n",
      "loss for batch 156 is 0.38264278970666366\n",
      "loss for batch 156 is 0.3660007880941921\n",
      "loss for batch 156 is 0.36659137420660115\n",
      "Epoch 156/300, Average Loss: 0.37864330933315615\n",
      "loss for batch 157 is 0.3805687610386683\n",
      "loss for batch 157 is 0.37542458185636235\n",
      "loss for batch 157 is 0.38709631113328213\n",
      "loss for batch 157 is 0.3783491803987045\n",
      "loss for batch 157 is 0.3803236255508392\n",
      "loss for batch 157 is 0.3823534975871721\n",
      "loss for batch 157 is 0.3798541313037438\n",
      "loss for batch 157 is 0.3818087840746746\n",
      "loss for batch 157 is 0.36455458901300497\n",
      "loss for batch 157 is 0.36519012869954504\n",
      "Epoch 157/300, Average Loss: 0.3775523590655997\n",
      "loss for batch 158 is 0.37960378555255775\n",
      "loss for batch 158 is 0.3743023116665858\n",
      "loss for batch 158 is 0.38640993428254233\n",
      "loss for batch 158 is 0.37724048804842153\n",
      "loss for batch 158 is 0.3796484950647132\n",
      "loss for batch 158 is 0.3814620098536573\n",
      "loss for batch 158 is 0.37886577304644736\n",
      "loss for batch 158 is 0.3810968078394615\n",
      "loss for batch 158 is 0.36322307928015674\n",
      "loss for batch 158 is 0.3639056257497371\n",
      "Epoch 158/300, Average Loss: 0.37657583103842807\n",
      "loss for batch 159 is 0.37876299586819345\n",
      "loss for batch 159 is 0.37330814011728863\n",
      "loss for batch 159 is 0.3858274034098071\n",
      "loss for batch 159 is 0.3762669920423858\n",
      "loss for batch 159 is 0.37908670100983766\n",
      "loss for batch 159 is 0.38070882663789773\n",
      "loss for batch 159 is 0.3780017130173418\n",
      "loss for batch 159 is 0.3805152156720426\n",
      "loss for batch 159 is 0.3620236659375892\n",
      "loss for batch 159 is 0.3627505855955617\n",
      "Epoch 159/300, Average Loss: 0.3757252239307946\n",
      "loss for batch 160 is 0.3780593990728728\n",
      "loss for batch 160 is 0.37246500559460716\n",
      "loss for batch 160 is 0.38534304140998965\n",
      "loss for batch 160 is 0.37545039561528565\n",
      "loss for batch 160 is 0.3786348251833351\n",
      "loss for batch 160 is 0.38010229026309494\n",
      "loss for batch 160 is 0.37726237937197304\n",
      "loss for batch 160 is 0.38005881813133663\n",
      "loss for batch 160 is 0.3609501963895722\n",
      "loss for batch 160 is 0.36171363608766516\n",
      "Epoch 160/300, Average Loss: 0.3750039987119732\n",
      "loss for batch 161 is 0.3774761098157052\n",
      "loss for batch 161 is 0.3717473593570478\n",
      "loss for batch 161 is 0.38494477409213296\n",
      "loss for batch 161 is 0.37475300250323823\n",
      "loss for batch 161 is 0.3782755819447403\n",
      "loss for batch 161 is 0.3796064464597422\n",
      "loss for batch 161 is 0.37661964037433954\n",
      "loss for batch 161 is 0.3797018791486761\n",
      "loss for batch 161 is 0.3599697917031488\n",
      "loss for batch 161 is 0.360769973688548\n",
      "Epoch 161/300, Average Loss: 0.3743864559087319\n",
      "loss for batch 162 is 0.3769875212525103\n",
      "loss for batch 162 is 0.37112823466248235\n",
      "loss for batch 162 is 0.38462023001914475\n",
      "loss for batch 162 is 0.37414782088930915\n",
      "loss for batch 162 is 0.3779934083679972\n",
      "loss for batch 162 is 0.37920082412753836\n",
      "loss for batch 162 is 0.37605758383033333\n",
      "loss for batch 162 is 0.3794288097201133\n",
      "loss for batch 162 is 0.35906511681627673\n",
      "loss for batch 162 is 0.3599059946919555\n",
      "Epoch 162/300, Average Loss: 0.3738535544377661\n",
      "loss for batch 163 is 0.3765769310992539\n",
      "loss for batch 163 is 0.3705869507539045\n",
      "loss for batch 163 is 0.3843609126026579\n",
      "loss for batch 163 is 0.37360970070137545\n",
      "loss for batch 163 is 0.3777780505222283\n",
      "loss for batch 163 is 0.3788644845466798\n",
      "loss for batch 163 is 0.37556022607960504\n",
      "loss for batch 163 is 0.3792250793902203\n",
      "loss for batch 163 is 0.35821992308546485\n",
      "loss for batch 163 is 0.35910908989010293\n",
      "Epoch 163/300, Average Loss: 0.3733891348671493\n",
      "loss for batch 164 is 0.3762299553944616\n",
      "loss for batch 164 is 0.3701036008331872\n",
      "loss for batch 164 is 0.38415840279109786\n",
      "loss for batch 164 is 0.3731160738991414\n",
      "loss for batch 164 is 0.37761843751740337\n",
      "loss for batch 164 is 0.3785759478012552\n",
      "loss for batch 164 is 0.3751102546534431\n",
      "loss for batch 164 is 0.37907569293197013\n",
      "loss for batch 164 is 0.3574186193367456\n",
      "loss for batch 164 is 0.35836986401935145\n",
      "Epoch 164/300, Average Loss: 0.37297768491780564\n",
      "loss for batch 165 is 0.37593684637586994\n",
      "loss for batch 165 is 0.369670280335095\n",
      "loss for batch 165 is 0.384010261126971\n",
      "loss for batch 165 is 0.3726684294083371\n",
      "loss for batch 165 is 0.3775142890899841\n",
      "loss for batch 165 is 0.3783451577824183\n",
      "loss for batch 165 is 0.37471857676712705\n",
      "loss for batch 165 is 0.3789903792014548\n",
      "loss for batch 165 is 0.35668339517128267\n",
      "loss for batch 165 is 0.35770328161288706\n",
      "Epoch 165/300, Average Loss: 0.3726240896871427\n",
      "loss for batch 166 is 0.3757122769252063\n",
      "loss for batch 166 is 0.369307733991858\n",
      "loss for batch 166 is 0.3839113679144861\n",
      "loss for batch 166 is 0.3722908768863909\n",
      "loss for batch 166 is 0.37746518571598203\n",
      "loss for batch 166 is 0.37818319592106975\n",
      "loss for batch 166 is 0.37438893436840687\n",
      "loss for batch 166 is 0.3789669858648415\n",
      "loss for batch 166 is 0.3560151617824078\n",
      "loss for batch 166 is 0.3571060819785121\n",
      "Epoch 166/300, Average Loss: 0.37233478013491617\n",
      "loss for batch 167 is 0.37555021893294727\n",
      "loss for batch 167 is 0.36900923907435706\n",
      "loss for batch 167 is 0.38385236018370245\n",
      "loss for batch 167 is 0.371967864315244\n",
      "loss for batch 167 is 0.37745829893516347\n",
      "loss for batch 167 is 0.37807000565567817\n",
      "loss for batch 167 is 0.37410316065837534\n",
      "loss for batch 167 is 0.37898587618740975\n",
      "loss for batch 167 is 0.35539063301747875\n",
      "loss for batch 167 is 0.35655769927215214\n",
      "Epoch 167/300, Average Loss: 0.37209453562325084\n",
      "loss for batch 168 is 0.3754264918573235\n",
      "loss for batch 168 is 0.36874781972840753\n",
      "loss for batch 168 is 0.38382025838476097\n",
      "loss for batch 168 is 0.3716724475367571\n",
      "loss for batch 168 is 0.3774792848868088\n",
      "loss for batch 168 is 0.37798709666046226\n",
      "loss for batch 168 is 0.3738500513869061\n",
      "loss for batch 168 is 0.37903795434430054\n",
      "loss for batch 168 is 0.3548069858901163\n",
      "loss for batch 168 is 0.35605740870839236\n",
      "Epoch 168/300, Average Loss: 0.37188857993842356\n",
      "loss for batch 169 is 0.37534260727231566\n",
      "loss for batch 169 is 0.3685318727873217\n",
      "loss for batch 169 is 0.38381266506317235\n",
      "loss for batch 169 is 0.3714212325649478\n",
      "loss for batch 169 is 0.37752880691427515\n",
      "loss for batch 169 is 0.37794584329830033\n",
      "loss for batch 169 is 0.37363473002721376\n",
      "loss for batch 169 is 0.3791238436117076\n",
      "loss for batch 169 is 0.35426523844215624\n",
      "loss for batch 169 is 0.3556014962353204\n",
      "Epoch 169/300, Average Loss: 0.3717208336216731\n",
      "loss for batch 170 is 0.37529078389717996\n",
      "loss for batch 170 is 0.36835106905560705\n",
      "loss for batch 170 is 0.38382139211359395\n",
      "loss for batch 170 is 0.3711960161757642\n",
      "loss for batch 170 is 0.3775958501895722\n",
      "loss for batch 170 is 0.3779287532170996\n",
      "loss for batch 170 is 0.3734445610509819\n",
      "loss for batch 170 is 0.37923194067758226\n",
      "loss for batch 170 is 0.35375611944231655\n",
      "loss for batch 170 is 0.35518349846310104\n",
      "Epoch 170/300, Average Loss: 0.3715799984282799\n",
      "loss for batch 171 is 0.3752644565064713\n",
      "loss for batch 171 is 0.36820151750756547\n",
      "loss for batch 171 is 0.38384256021014973\n",
      "loss for batch 171 is 0.3709972830779785\n",
      "loss for batch 171 is 0.37767704762527127\n",
      "loss for batch 171 is 0.37793527760692197\n",
      "loss for batch 171 is 0.3732780073869969\n",
      "loss for batch 171 is 0.3793586888855752\n",
      "loss for batch 171 is 0.3532783854182535\n",
      "loss for batch 171 is 0.35479992153693063\n",
      "Epoch 171/300, Average Loss: 0.3714633145762114\n",
      "loss for batch 172 is 0.37525804081109043\n",
      "loss for batch 172 is 0.36807764711968105\n",
      "loss for batch 172 is 0.3838683766650645\n",
      "loss for batch 172 is 0.37081770753074617\n",
      "loss for batch 172 is 0.3777640384463508\n",
      "loss for batch 172 is 0.3779560609558268\n",
      "loss for batch 172 is 0.3731269115149435\n",
      "loss for batch 172 is 0.3794949114592532\n",
      "loss for batch 172 is 0.3528258840787003\n",
      "loss for batch 172 is 0.35444417234121284\n",
      "Epoch 172/300, Average Loss: 0.37136337509228695\n",
      "loss for batch 173 is 0.37526466180428686\n",
      "loss for batch 173 is 0.3679736825386759\n",
      "loss for batch 173 is 0.3838929718629527\n",
      "loss for batch 173 is 0.37065442696434525\n",
      "loss for batch 173 is 0.37785193502344794\n",
      "loss for batch 173 is 0.3779885468581343\n",
      "loss for batch 173 is 0.37298864675648785\n",
      "loss for batch 173 is 0.37963694980534163\n",
      "loss for batch 173 is 0.352396565635651\n",
      "loss for batch 173 is 0.3541126922821417\n",
      "Epoch 173/300, Average Loss: 0.3712761079531465\n",
      "loss for batch 174 is 0.3752799543298656\n",
      "loss for batch 174 is 0.3678846115918882\n",
      "loss for batch 174 is 0.3839113668976488\n",
      "loss for batch 174 is 0.3704993020949314\n",
      "loss for batch 174 is 0.37793538307325564\n",
      "loss for batch 174 is 0.37802111376538067\n",
      "loss for batch 174 is 0.3728532233076003\n",
      "loss for batch 174 is 0.37977409622556096\n",
      "loss for batch 174 is 0.35198195508371427\n",
      "loss for batch 174 is 0.3537971871515056\n",
      "Epoch 174/300, Average Loss: 0.37119381935213513\n",
      "loss for batch 175 is 0.3752945710469432\n",
      "loss for batch 175 is 0.36779558237182164\n",
      "loss for batch 175 is 0.3839168218857237\n",
      "loss for batch 175 is 0.37034073534794987\n",
      "loss for batch 175 is 0.3780088884125505\n",
      "loss for batch 175 is 0.3780459369561495\n",
      "loss for batch 175 is 0.37271618802054984\n",
      "loss for batch 175 is 0.37990273876604225\n",
      "loss for batch 175 is 0.3515811053430222\n",
      "loss for batch 175 is 0.35349662421105704\n",
      "Epoch 175/300, Average Loss: 0.371109919236181\n",
      "loss for batch 176 is 0.37530803684103886\n",
      "loss for batch 176 is 0.36771215458415385\n",
      "loss for batch 176 is 0.3839088254677031\n",
      "loss for batch 176 is 0.37018676574366427\n",
      "loss for batch 176 is 0.378071191034281\n",
      "loss for batch 176 is 0.37806960547982965\n",
      "loss for batch 176 is 0.3725814547097683\n",
      "loss for batch 176 is 0.3800248857367535\n",
      "loss for batch 176 is 0.3511957126431169\n",
      "loss for batch 176 is 0.3532111262116502\n",
      "Epoch 176/300, Average Loss: 0.37102697584519595\n",
      "loss for batch 177 is 0.37531967494331486\n",
      "loss for batch 177 is 0.3676367483954841\n",
      "loss for batch 177 is 0.3838861726217565\n",
      "loss for batch 177 is 0.37003862081229916\n",
      "loss for batch 177 is 0.37811964248436747\n",
      "loss for batch 177 is 0.3780916030498743\n",
      "loss for batch 177 is 0.37244770661447585\n",
      "loss for batch 177 is 0.38013792001328867\n",
      "loss for batch 177 is 0.35082284347127013\n",
      "loss for batch 177 is 0.3529373138218492\n",
      "Epoch 177/300, Average Loss: 0.370943824622798\n",
      "loss for batch 178 is 0.37532527054021475\n",
      "loss for batch 178 is 0.3675659841410694\n",
      "loss for batch 178 is 0.3838464373558294\n",
      "loss for batch 178 is 0.3698927706955451\n",
      "loss for batch 178 is 0.3781508474190734\n",
      "loss for batch 178 is 0.3781082436034644\n",
      "loss for batch 178 is 0.3723119715891086\n",
      "loss for batch 178 is 0.38023852276820763\n",
      "loss for batch 178 is 0.35045954484575365\n",
      "loss for batch 178 is 0.3526724262789955\n",
      "Epoch 178/300, Average Loss: 0.3708572019237261\n",
      "loss for batch 179 is 0.37532195954838715\n",
      "loss for batch 179 is 0.3674975080244516\n",
      "loss for batch 179 is 0.3837882955864794\n",
      "loss for batch 179 is 0.36974726782910877\n",
      "loss for batch 179 is 0.3781631949124442\n",
      "loss for batch 179 is 0.37811809257963397\n",
      "loss for batch 179 is 0.3721731789900029\n",
      "loss for batch 179 is 0.38032542954421844\n",
      "loss for batch 179 is 0.35010472539901427\n",
      "loss for batch 179 is 0.35241510232063766\n",
      "Epoch 179/300, Average Loss: 0.3707654754734378\n",
      "loss for batch 180 is 0.37530848372210424\n",
      "loss for batch 180 is 0.3674302192200266\n",
      "loss for batch 180 is 0.3837113783112859\n",
      "loss for batch 180 is 0.36960123873010814\n",
      "loss for batch 180 is 0.37815591164615237\n",
      "loss for batch 180 is 0.37812006239675056\n",
      "loss for batch 180 is 0.3720304154233463\n",
      "loss for batch 180 is 0.38039751966293334\n",
      "loss for batch 180 is 0.3497571404537315\n",
      "loss for batch 180 is 0.35216395062320555\n",
      "Epoch 180/300, Average Loss: 0.37066763201896447\n",
      "loss for batch 181 is 0.37528356053899214\n",
      "loss for batch 181 is 0.3673626480391905\n",
      "loss for batch 181 is 0.38361556435511923\n",
      "loss for batch 181 is 0.36945324833588605\n",
      "loss for batch 181 is 0.37812848890002887\n",
      "loss for batch 181 is 0.37811273443000754\n",
      "loss for batch 181 is 0.3718826772869349\n",
      "loss for batch 181 is 0.38045382481260803\n",
      "loss for batch 181 is 0.3494155873102645\n",
      "loss for batch 181 is 0.35191810006049157\n",
      "Epoch 181/300, Average Loss: 0.37056264340695233\n",
      "loss for batch 182 is 0.3752461666373466\n",
      "loss for batch 182 is 0.3672925048993684\n",
      "loss for batch 182 is 0.38350237918055957\n",
      "loss for batch 182 is 0.3693011034259409\n",
      "loss for batch 182 is 0.37808142263381606\n",
      "loss for batch 182 is 0.37809456651833145\n",
      "loss for batch 182 is 0.37172924688150794\n",
      "loss for batch 182 is 0.38049400509256875\n",
      "loss for batch 182 is 0.3490793886434923\n",
      "loss for batch 182 is 0.35167708622777466\n",
      "Epoch 182/300, Average Loss: 0.37044978701407066\n",
      "loss for batch 183 is 0.3751961514906691\n",
      "loss for batch 183 is 0.3672191534861825\n",
      "loss for batch 183 is 0.3833718348772327\n",
      "loss for batch 183 is 0.3691433447297293\n",
      "loss for batch 183 is 0.37801382836729686\n",
      "loss for batch 183 is 0.3780634952736883\n",
      "loss for batch 183 is 0.37156805879422783\n",
      "loss for batch 183 is 0.3805154158739624\n",
      "loss for batch 183 is 0.3487442235791857\n",
      "loss for batch 183 is 0.35143640721967295\n",
      "Epoch 183/300, Average Loss: 0.37032719136918474\n",
      "loss for batch 184 is 0.37512824778918147\n",
      "loss for batch 184 is 0.36713594031748087\n",
      "loss for batch 184 is 0.3832213287953178\n",
      "loss for batch 184 is 0.3689725787487931\n",
      "loss for batch 184 is 0.3779222287298241\n",
      "loss for batch 184 is 0.37801328384396876\n",
      "loss for batch 184 is 0.37139477989518055\n",
      "loss for batch 184 is 0.3805143239882341\n",
      "loss for batch 184 is 0.34840559534925264\n",
      "loss for batch 184 is 0.3511930449251309\n",
      "Epoch 184/300, Average Loss: 0.37019013523823646\n",
      "loss for batch 185 is 0.3750412912486056\n",
      "loss for batch 185 is 0.3670428349664879\n",
      "loss for batch 185 is 0.38305097511945607\n",
      "loss for batch 185 is 0.36879064326219957\n",
      "loss for batch 185 is 0.3778085642253097\n",
      "loss for batch 185 is 0.3779479672735831\n",
      "loss for batch 185 is 0.37121315148516243\n",
      "loss for batch 185 is 0.3804953174452028\n",
      "loss for batch 185 is 0.3480681344999684\n",
      "loss for batch 185 is 0.3509511063775204\n",
      "Epoch 185/300, Average Loss: 0.37004099859034956\n",
      "loss for batch 186 is 0.37494095721171816\n",
      "loss for batch 186 is 0.3669456246133619\n",
      "loss for batch 186 is 0.3828658409404624\n",
      "loss for batch 186 is 0.3686041430897211\n",
      "loss for batch 186 is 0.3776780761199383\n",
      "loss for batch 186 is 0.37787370292409844\n",
      "loss for batch 186 is 0.3710279963789967\n",
      "loss for batch 186 is 0.3804635660640126\n",
      "loss for batch 186 is 0.347736115953544\n",
      "loss for batch 186 is 0.350714037701191\n",
      "Epoch 186/300, Average Loss: 0.3698850060997044\n",
      "loss for batch 187 is 0.37483145942941776\n",
      "loss for batch 187 is 0.36684760094676827\n",
      "loss for batch 187 is 0.38266950999615573\n",
      "loss for batch 187 is 0.3684164692498353\n",
      "loss for batch 187 is 0.37753406049161375\n",
      "loss for batch 187 is 0.3777936348349877\n",
      "loss for batch 187 is 0.3708416786299353\n",
      "loss for batch 187 is 0.3804218038312069\n",
      "loss for batch 187 is 0.3474110393685751\n",
      "loss for batch 187 is 0.3504832632485995\n",
      "Epoch 187/300, Average Loss: 0.36972505200270955\n",
      "loss for batch 188 is 0.3747153504446112\n",
      "loss for batch 188 is 0.3667507055014693\n",
      "loss for batch 188 is 0.38246466501676385\n",
      "loss for batch 188 is 0.36822880149885834\n",
      "loss for batch 188 is 0.37737847342821035\n",
      "loss for batch 188 is 0.3777080298592826\n",
      "loss for batch 188 is 0.3706542129275222\n",
      "loss for batch 188 is 0.380370356313122\n",
      "loss for batch 188 is 0.34709127792740985\n",
      "loss for batch 188 is 0.3502570941639589\n",
      "Epoch 188/300, Average Loss: 0.36956189670812084\n",
      "loss for batch 189 is 0.374591042362693\n",
      "loss for batch 189 is 0.36665183788412087\n",
      "loss for batch 189 is 0.38225237463945627\n",
      "loss for batch 189 is 0.36803899378140015\n",
      "loss for batch 189 is 0.37721235596129693\n",
      "loss for batch 189 is 0.3776163522988887\n",
      "loss for batch 189 is 0.37046623010212937\n",
      "loss for batch 189 is 0.3803108591333719\n",
      "loss for batch 189 is 0.3467774728884588\n",
      "loss for batch 189 is 0.35003719151829077\n",
      "Epoch 189/300, Average Loss: 0.3693954710570107\n",
      "loss for batch 190 is 0.37446239418027283\n",
      "loss for batch 190 is 0.36655554031710197\n",
      "loss for batch 190 is 0.38203695354318534\n",
      "loss for batch 190 is 0.3678529936258253\n",
      "loss for batch 190 is 0.37704100887627384\n",
      "loss for batch 190 is 0.3775265829758903\n",
      "loss for batch 190 is 0.3702840848372198\n",
      "loss for batch 190 is 0.3802510407007609\n",
      "loss for batch 190 is 0.34647732118154007\n",
      "loss for batch 190 is 0.3498299516019377\n",
      "Epoch 190/300, Average Loss: 0.36923178718400085\n",
      "loss for batch 191 is 0.3743377035266785\n",
      "loss for batch 191 is 0.36647019550255383\n",
      "loss for batch 191 is 0.3818233816885543\n",
      "loss for batch 191 is 0.36767939607774447\n",
      "loss for batch 191 is 0.37686958029724094\n",
      "loss for batch 191 is 0.37744484047301535\n",
      "loss for batch 191 is 0.37011162703662814\n",
      "loss for batch 191 is 0.3801941314862258\n",
      "loss for batch 191 is 0.34619184407245074\n",
      "loss for batch 191 is 0.34963556009423924\n",
      "Epoch 191/300, Average Loss: 0.3690758260255331\n",
      "loss for batch 192 is 0.3742171238699917\n",
      "loss for batch 192 is 0.3663943969045261\n",
      "loss for batch 192 is 0.3816122963311768\n",
      "loss for batch 192 is 0.36751493507469885\n",
      "loss for batch 192 is 0.3766976531917214\n",
      "loss for batch 192 is 0.3773680374706355\n",
      "loss for batch 192 is 0.369946416692275\n",
      "loss for batch 192 is 0.3801382968121741\n",
      "loss for batch 192 is 0.34591760518369047\n",
      "loss for batch 192 is 0.34945153568997805\n",
      "Epoch 192/300, Average Loss: 0.36892582972208676\n",
      "loss for batch 193 is 0.37409876356253763\n",
      "loss for batch 193 is 0.36632543806393997\n",
      "loss for batch 193 is 0.38140425850710485\n",
      "loss for batch 193 is 0.36735734513809004\n",
      "loss for batch 193 is 0.3765257396701606\n",
      "loss for batch 193 is 0.3772952532622947\n",
      "loss for batch 193 is 0.3697884867130037\n",
      "loss for batch 193 is 0.3800846295567625\n",
      "loss for batch 193 is 0.34565486292285064\n",
      "loss for batch 193 is 0.3492785157340226\n",
      "Epoch 193/300, Average Loss: 0.3687813293130767\n",
      "loss for batch 194 is 0.37398426258243656\n",
      "loss for batch 194 is 0.3662644953839385\n",
      "loss for batch 194 is 0.3812013824519934\n",
      "loss for batch 194 is 0.36720813095254246\n",
      "loss for batch 194 is 0.3763560953497852\n",
      "loss for batch 194 is 0.3772284958269945\n",
      "loss for batch 194 is 0.369639407874057\n",
      "loss for batch 194 is 0.38003542618521835\n",
      "loss for batch 194 is 0.34540493449112153\n",
      "loss for batch 194 is 0.34911783805487195\n",
      "Epoch 194/300, Average Loss: 0.36864404691529595\n",
      "loss for batch 195 is 0.37387606345071644\n",
      "loss for batch 195 is 0.3662136948662759\n",
      "loss for batch 195 is 0.38100611278506913\n",
      "loss for batch 195 is 0.3670698324054876\n",
      "loss for batch 195 is 0.37619141368081255\n",
      "loss for batch 195 is 0.3771706654051099\n",
      "loss for batch 195 is 0.36950131208817927\n",
      "loss for batch 195 is 0.37999319233122514\n",
      "loss for batch 195 is 0.34516912630350155\n",
      "loss for batch 195 is 0.3489705865941943\n",
      "Epoch 195/300, Average Loss: 0.36851619999105717\n",
      "loss for batch 196 is 0.373776008006524\n",
      "loss for batch 196 is 0.3661742280831408\n",
      "loss for batch 196 is 0.3808202417403879\n",
      "loss for batch 196 is 0.3669434194178598\n",
      "loss for batch 196 is 0.3760333631564503\n",
      "loss for batch 196 is 0.3771227988157657\n",
      "loss for batch 196 is 0.3693750380218403\n",
      "loss for batch 196 is 0.3799593001346833\n",
      "loss for batch 196 is 0.3449478033066631\n",
      "loss for batch 196 is 0.34883728449626406\n",
      "Epoch 196/300, Average Loss: 0.3683989485179579\n",
      "loss for batch 197 is 0.3736855060678734\n",
      "loss for batch 197 is 0.3661471699668196\n",
      "loss for batch 197 is 0.3806454747577899\n",
      "loss for batch 197 is 0.36683037988753986\n",
      "loss for batch 197 is 0.3758838646877048\n",
      "loss for batch 197 is 0.3770868343745723\n",
      "loss for batch 197 is 0.36926213448677836\n",
      "loss for batch 197 is 0.3799358217145078\n",
      "loss for batch 197 is 0.344742260713207\n",
      "loss for batch 197 is 0.3487189338526799\n",
      "Epoch 197/300, Average Loss: 0.3682938380509473\n",
      "loss for batch 198 is 0.37360611029957275\n",
      "loss for batch 198 is 0.3661334009191778\n",
      "loss for batch 198 is 0.38048309409475556\n",
      "loss for batch 198 is 0.36673113292951265\n",
      "loss for batch 198 is 0.37574397512759233\n",
      "loss for batch 198 is 0.37706303541539\n",
      "loss for batch 198 is 0.36916268682140313\n",
      "loss for batch 198 is 0.37992318329450075\n",
      "loss for batch 198 is 0.3445519577600215\n",
      "loss for batch 198 is 0.34861519521231205\n",
      "Epoch 198/300, Average Loss: 0.36820137718742385\n",
      "loss for batch 199 is 0.3735379972397204\n",
      "loss for batch 199 is 0.3661325822508507\n",
      "loss for batch 199 is 0.3803338795446834\n",
      "loss for batch 199 is 0.3666456928646163\n",
      "loss for batch 199 is 0.37561462228624276\n",
      "loss for batch 199 is 0.3770519803816843\n",
      "loss for batch 199 is 0.36907730232644026\n",
      "loss for batch 199 is 0.3799225433842327\n",
      "loss for batch 199 is 0.3443775221419674\n",
      "loss for batch 199 is 0.3485267383385954\n",
      "Epoch 199/300, Average Loss: 0.36812208607590335\n",
      "loss for batch 200 is 0.3734826124028208\n",
      "loss for batch 200 is 0.366146012891683\n",
      "loss for batch 200 is 0.3801990662637878\n",
      "loss for batch 200 is 0.36657524275855147\n",
      "loss for batch 200 is 0.37549704839582587\n",
      "loss for batch 200 is 0.3770547302866363\n",
      "loss for batch 200 is 0.36900651214166974\n",
      "loss for batch 200 is 0.37993465381548847\n",
      "loss for batch 200 is 0.3442189752020815\n",
      "loss for batch 200 is 0.3484535227554697\n",
      "Epoch 200/300, Average Loss: 0.36805683769140146\n",
      "loss for batch 201 is 0.3734402601404964\n",
      "loss for batch 201 is 0.36617347681991735\n",
      "loss for batch 201 is 0.38007912760697693\n",
      "loss for batch 201 is 0.3665196402939669\n",
      "loss for batch 201 is 0.37539177009375524\n",
      "loss for batch 201 is 0.3770713818982013\n",
      "loss for batch 201 is 0.3689504440983669\n",
      "loss for batch 201 is 0.3799598416560434\n",
      "loss for batch 201 is 0.34407602701372136\n",
      "loss for batch 201 is 0.3483952435148558\n",
      "Epoch 201/300, Average Loss: 0.3680057213136302\n",
      "loss for batch 202 is 0.37341090596180804\n",
      "loss for batch 202 is 0.36621454485684746\n",
      "loss for batch 202 is 0.37997427892712726\n",
      "loss for batch 202 is 0.3664784822613802\n",
      "loss for batch 202 is 0.37529903151791666\n",
      "loss for batch 202 is 0.37710173402014296\n",
      "loss for batch 202 is 0.3689090143525834\n",
      "loss for batch 202 is 0.37999847886812765\n",
      "loss for batch 202 is 0.34394883207154636\n",
      "loss for batch 202 is 0.348352235544611\n",
      "Epoch 202/300, Average Loss: 0.36796875383820915\n",
      "loss for batch 203 is 0.3733957290354442\n",
      "loss for batch 203 is 0.3662704782214468\n",
      "loss for batch 203 is 0.3798855021969253\n",
      "loss for batch 203 is 0.3664536429432511\n",
      "loss for batch 203 is 0.3752202496690659\n",
      "loss for batch 203 is 0.3771479032788855\n",
      "loss for batch 203 is 0.3688835598889713\n",
      "loss for batch 203 is 0.38005198259673173\n",
      "loss for batch 203 is 0.343838345767934\n",
      "loss for batch 203 is 0.34832501482135414\n",
      "Epoch 203/300, Average Loss: 0.36794724084200103\n",
      "loss for batch 204 is 0.37339525011647856\n",
      "loss for batch 204 is 0.36634122640463745\n",
      "loss for batch 204 is 0.3798128657787621\n",
      "loss for batch 204 is 0.36644475092940776\n",
      "loss for batch 204 is 0.37515537328715165\n",
      "loss for batch 204 is 0.3772093168390506\n",
      "loss for batch 204 is 0.36887359362026434\n",
      "loss for batch 204 is 0.3801200541412325\n",
      "loss for batch 204 is 0.3437440065541608\n",
      "loss for batch 204 is 0.34831300882179367\n",
      "Epoch 204/300, Average Loss: 0.36794094464929394\n",
      "loss for batch 205 is 0.37340932508088287\n",
      "loss for batch 205 is 0.36642668175868665\n",
      "loss for batch 205 is 0.37975637728564643\n",
      "loss for batch 205 is 0.3664517959737238\n",
      "loss for batch 205 is 0.3751045259761279\n",
      "loss for batch 205 is 0.3772860210806032\n",
      "loss for batch 205 is 0.3688790468554128\n",
      "loss for batch 205 is 0.38020274348753963\n",
      "loss for batch 205 is 0.343665669113793\n",
      "loss for batch 205 is 0.3483160732327261\n",
      "Epoch 205/300, Average Loss: 0.3679498259845142\n",
      "loss for batch 206 is 0.37343783272017106\n",
      "loss for batch 206 is 0.3665262944824264\n",
      "loss for batch 206 is 0.37971554511696387\n",
      "loss for batch 206 is 0.3664734361695278\n",
      "loss for batch 206 is 0.37506686845293835\n",
      "loss for batch 206 is 0.37737619139850864\n",
      "loss for batch 206 is 0.36889827227062705\n",
      "loss for batch 206 is 0.3802983370139971\n",
      "loss for batch 206 is 0.34360127385386696\n",
      "loss for batch 206 is 0.34833233301910216\n",
      "Epoch 206/300, Average Loss: 0.367972638449813\n",
      "loss for batch 207 is 0.37347873819730926\n",
      "loss for batch 207 is 0.3666376085171369\n",
      "loss for batch 207 is 0.37968916139101755\n",
      "loss for batch 207 is 0.36650738794920484\n",
      "loss for batch 207 is 0.3750412895733007\n",
      "loss for batch 207 is 0.3774780520503821\n",
      "loss for batch 207 is 0.36892996387757265\n",
      "loss for batch 207 is 0.3804057017635002\n",
      "loss for batch 207 is 0.34354966190530095\n",
      "loss for batch 207 is 0.34836082665424145\n",
      "Epoch 207/300, Average Loss: 0.3680078391878967\n",
      "loss for batch 208 is 0.373531210662972\n",
      "loss for batch 208 is 0.3667598701829759\n",
      "loss for batch 208 is 0.37967686803546713\n",
      "loss for batch 208 is 0.3665536035956318\n",
      "loss for batch 208 is 0.375027888753764\n",
      "loss for batch 208 is 0.37759209823617085\n",
      "loss for batch 208 is 0.3689745545262997\n",
      "loss for batch 208 is 0.3805254651723122\n",
      "loss for batch 208 is 0.34351154853187543\n",
      "loss for batch 208 is 0.3484021253265274\n",
      "Epoch 208/300, Average Loss: 0.36805552330239966\n",
      "loss for batch 209 is 0.37359629196983457\n",
      "loss for batch 209 is 0.36689383299648354\n",
      "loss for batch 209 is 0.3796786376572274\n",
      "loss for batch 209 is 0.36661216264008545\n",
      "loss for batch 209 is 0.37502639973172397\n",
      "loss for batch 209 is 0.3777176939489406\n",
      "loss for batch 209 is 0.3690312452756871\n",
      "loss for batch 209 is 0.38065664388072584\n",
      "loss for batch 209 is 0.34348574275790617\n",
      "loss for batch 209 is 0.348455016024701\n",
      "Epoch 209/300, Average Loss: 0.36811536668833156\n",
      "loss for batch 210 is 0.37367251289102377\n",
      "loss for batch 210 is 0.3670378919241731\n",
      "loss for batch 210 is 0.3796934457129897\n",
      "loss for batch 210 is 0.36668157944602325\n",
      "loss for batch 210 is 0.3750359693613938\n",
      "loss for batch 210 is 0.3778537115981042\n",
      "loss for batch 210 is 0.36909921108855176\n",
      "loss for batch 210 is 0.3807985174326406\n",
      "loss for batch 210 is 0.3434716377505039\n",
      "loss for batch 210 is 0.3485191852835279\n",
      "Epoch 210/300, Average Loss: 0.3681863662488932\n",
      "loss for batch 211 is 0.37375960148121296\n",
      "loss for batch 211 is 0.36719173516280496\n",
      "loss for batch 211 is 0.3797214055818864\n",
      "loss for batch 211 is 0.36676164993730825\n",
      "loss for batch 211 is 0.37505665683420975\n",
      "loss for batch 211 is 0.3779999614622395\n",
      "loss for batch 211 is 0.3691781929541351\n",
      "loss for batch 211 is 0.38095092590143503\n",
      "loss for batch 211 is 0.3434689474694793\n",
      "loss for batch 211 is 0.34859432690337616\n",
      "Epoch 211/300, Average Loss: 0.3682683403688088\n",
      "loss for batch 212 is 0.37385728357260906\n",
      "loss for batch 212 is 0.3673549832884474\n",
      "loss for batch 212 is 0.3797622724738138\n",
      "loss for batch 212 is 0.36685217719668817\n",
      "loss for batch 212 is 0.37508863865946623\n",
      "loss for batch 212 is 0.378156332238676\n",
      "loss for batch 212 is 0.3692684222145912\n",
      "loss for batch 212 is 0.38111448364753703\n",
      "loss for batch 212 is 0.3434781558240151\n",
      "loss for batch 212 is 0.3486809038856882\n",
      "Epoch 212/300, Average Loss: 0.36836136530015323\n",
      "loss for batch 213 is 0.37396599601757813\n",
      "loss for batch 213 is 0.3675276770016541\n",
      "loss for batch 213 is 0.37981631538506483\n",
      "loss for batch 213 is 0.3669527965724214\n",
      "loss for batch 213 is 0.3751315050505177\n",
      "loss for batch 213 is 0.37832217710188215\n",
      "loss for batch 213 is 0.36936889616580204\n",
      "loss for batch 213 is 0.38128786185385743\n",
      "loss for batch 213 is 0.3434981501042882\n",
      "loss for batch 213 is 0.3487775492920801\n",
      "Epoch 213/300, Average Loss: 0.3684648924545146\n",
      "loss for batch 214 is 0.3740843191202982\n",
      "loss for batch 214 is 0.3677088089672899\n",
      "loss for batch 214 is 0.3798812803094897\n",
      "loss for batch 214 is 0.3670628624018245\n",
      "loss for batch 214 is 0.3751837038336764\n",
      "loss for batch 214 is 0.3784969864565939\n",
      "loss for batch 214 is 0.3694790374541825\n",
      "loss for batch 214 is 0.38146994341009094\n",
      "loss for batch 214 is 0.34352771631207374\n",
      "loss for batch 214 is 0.3488826339407235\n",
      "Epoch 214/300, Average Loss: 0.3685777292206243\n",
      "loss for batch 215 is 0.3742111905005701\n",
      "loss for batch 215 is 0.3678978542476626\n",
      "loss for batch 215 is 0.379955348446308\n",
      "loss for batch 215 is 0.3671804941940231\n",
      "loss for batch 215 is 0.37524217517823566\n",
      "loss for batch 215 is 0.3786726503954702\n",
      "loss for batch 215 is 0.36958871928512144\n",
      "loss for batch 215 is 0.38164522577249627\n",
      "loss for batch 215 is 0.34354530080874845\n",
      "loss for batch 215 is 0.34897226343591126\n",
      "Epoch 215/300, Average Loss: 0.36869112222645467\n",
      "loss for batch 216 is 0.3743079528193633\n",
      "loss for batch 216 is 0.36803694951451016\n",
      "loss for batch 216 is 0.3800026169662723\n",
      "loss for batch 216 is 0.36721368343195443\n",
      "loss for batch 216 is 0.37525014350837266\n",
      "loss for batch 216 is 0.378745045361246\n",
      "loss for batch 216 is 0.36961693803337037\n",
      "loss for batch 216 is 0.3817307688224406\n",
      "loss for batch 216 is 0.3434736322501457\n",
      "loss for batch 216 is 0.3489866235507415\n",
      "Epoch 216/300, Average Loss: 0.36873643542584167\n",
      "loss for batch 217 is 0.37431086169176797\n",
      "loss for batch 217 is 0.36806494287163294\n",
      "loss for batch 217 is 0.37999522248744777\n",
      "loss for batch 217 is 0.36713343594881737\n",
      "loss for batch 217 is 0.37519896711312023\n",
      "loss for batch 217 is 0.37872811686775776\n",
      "loss for batch 217 is 0.3695895601010798\n",
      "loss for batch 217 is 0.3817687859002448\n",
      "loss for batch 217 is 0.34337261564407234\n",
      "loss for batch 217 is 0.3489861453046814\n",
      "Epoch 217/300, Average Loss: 0.3687148653930622\n",
      "loss for batch 218 is 0.3743050667415651\n",
      "loss for batch 218 is 0.3680958581402487\n",
      "loss for batch 218 is 0.3800047922413415\n",
      "loss for batch 218 is 0.36709816309364335\n",
      "loss for batch 218 is 0.3751936775964793\n",
      "loss for batch 218 is 0.3787917243936305\n",
      "loss for batch 218 is 0.3696415909092012\n",
      "loss for batch 218 is 0.3818966006547046\n",
      "loss for batch 218 is 0.3433717709831595\n",
      "loss for batch 218 is 0.34909080185272184\n",
      "Epoch 218/300, Average Loss: 0.3687490046606695\n",
      "loss for batch 219 is 0.3744260593428203\n",
      "loss for batch 219 is 0.36825705162120037\n",
      "loss for batch 219 is 0.3801073528648497\n",
      "loss for batch 219 is 0.3671979847609945\n",
      "loss for batch 219 is 0.37527968250642324\n",
      "loss for batch 219 is 0.3789693611280871\n",
      "loss for batch 219 is 0.36977917648573455\n",
      "loss for batch 219 is 0.3821004510064301\n",
      "loss for batch 219 is 0.3434327657009474\n",
      "loss for batch 219 is 0.34923839811303453\n",
      "Epoch 219/300, Average Loss: 0.3688788283530522\n",
      "loss for batch 220 is 0.3745834970993834\n",
      "loss for batch 220 is 0.36844966560422465\n",
      "loss for batch 220 is 0.3802172026194067\n",
      "loss for batch 220 is 0.36731103838794604\n",
      "loss for batch 220 is 0.37535813420501274\n",
      "loss for batch 220 is 0.37913990987270907\n",
      "loss for batch 220 is 0.36990243646287513\n",
      "loss for batch 220 is 0.3822838946172676\n",
      "loss for batch 220 is 0.34347544054222845\n",
      "loss for batch 220 is 0.3493560107655619\n",
      "Epoch 220/300, Average Loss: 0.3690077230176615\n",
      "loss for batch 221 is 0.37470613699311317\n",
      "loss for batch 221 is 0.3686188841262387\n",
      "loss for batch 221 is 0.3802988904303138\n",
      "loss for batch 221 is 0.36740115640712456\n",
      "loss for batch 221 is 0.3754096921816572\n",
      "loss for batch 221 is 0.37928585562791817\n",
      "loss for batch 221 is 0.3700041745779724\n",
      "loss for batch 221 is 0.38244508547201495\n",
      "loss for batch 221 is 0.3435024440483574\n",
      "loss for batch 221 is 0.3494531355834476\n",
      "Epoch 221/300, Average Loss: 0.36911254554481576\n",
      "loss for batch 222 is 0.3748077971193237\n",
      "loss for batch 222 is 0.36877576472316953\n",
      "loss for batch 222 is 0.38036979647879077\n",
      "loss for batch 222 is 0.3674865655322325\n",
      "loss for batch 222 is 0.3754551391522313\n",
      "loss for batch 222 is 0.3794309079161846\n",
      "loss for batch 222 is 0.37010764344049424\n",
      "loss for batch 222 is 0.38260838611279935\n",
      "loss for batch 222 is 0.34353837148487104\n",
      "loss for batch 222 is 0.34956000969868745\n",
      "Epoch 222/300, Average Loss: 0.36921403816587844\n",
      "loss for batch 223 is 0.3749184529638149\n",
      "loss for batch 223 is 0.36894445074792087\n",
      "loss for batch 223 is 0.38045258205779475\n",
      "loss for batch 223 is 0.3675872489180969\n",
      "loss for batch 223 is 0.3755094309996482\n",
      "loss for batch 223 is 0.379588834512208\n",
      "loss for batch 223 is 0.37022211025084123\n",
      "loss for batch 223 is 0.3827841164686627\n",
      "loss for batch 223 is 0.3435935284588126\n",
      "loss for batch 223 is 0.3496873690572738\n",
      "Epoch 223/300, Average Loss: 0.36932881244350735\n",
      "loss for batch 224 is 0.3750435910571726\n",
      "loss for batch 224 is 0.3691259717674392\n",
      "loss for batch 224 is 0.3805513027913478\n",
      "loss for batch 224 is 0.3677060003281676\n",
      "loss for batch 224 is 0.3755755098423218\n",
      "loss for batch 224 is 0.3797622259594191\n",
      "loss for batch 224 is 0.37034852148826986\n",
      "loss for batch 224 is 0.38297072370531143\n",
      "loss for batch 224 is 0.3436655160178007\n",
      "loss for batch 224 is 0.34983271924835474\n",
      "Epoch 224/300, Average Loss: 0.3694582082205605\n",
      "loss for batch 225 is 0.37517811230183945\n",
      "loss for batch 225 is 0.3693117606060216\n",
      "loss for batch 225 is 0.38066082298800136\n",
      "loss for batch 225 is 0.3678320444185045\n",
      "loss for batch 225 is 0.37564585001242834\n",
      "loss for batch 225 is 0.37993880470346464\n",
      "loss for batch 225 is 0.3704777141091642\n",
      "loss for batch 225 is 0.3831613733719896\n",
      "loss for batch 225 is 0.34374984789455093\n",
      "loss for batch 225 is 0.3499916010557492\n",
      "Epoch 225/300, Average Loss: 0.36959479314617144\n",
      "loss for batch 226 is 0.3753178530473586\n",
      "loss for batch 226 is 0.3694989394022759\n",
      "loss for batch 226 is 0.38077959901662833\n",
      "loss for batch 226 is 0.36796421286651787\n",
      "loss for batch 226 is 0.3757202819048358\n",
      "loss for batch 226 is 0.38012103296107236\n",
      "loss for batch 226 is 0.3706118202053315\n",
      "loss for batch 226 is 0.3833605811906198\n",
      "loss for batch 226 is 0.34384814490481896\n",
      "loss for batch 226 is 0.35016614939539586\n",
      "Epoch 226/300, Average Loss: 0.3697388614894855\n",
      "loss for batch 227 is 0.375472655292466\n",
      "loss for batch 227 is 0.36970394346566926\n",
      "loss for batch 227 is 0.38091679720720395\n",
      "loss for batch 227 is 0.3681212476737206\n",
      "loss for batch 227 is 0.37581205787323146\n",
      "loss for batch 227 is 0.3803264656859947\n",
      "loss for batch 227 is 0.3707649694917417\n",
      "loss for batch 227 is 0.38357428337745686\n",
      "loss for batch 227 is 0.3439571673721785\n",
      "loss for batch 227 is 0.3503450368490737\n",
      "Epoch 227/300, Average Loss: 0.3698994624288736\n",
      "loss for batch 228 is 0.3756309371861202\n",
      "loss for batch 228 is 0.36991106660857614\n",
      "loss for batch 228 is 0.38104422079111294\n",
      "loss for batch 228 is 0.36827196894694086\n",
      "loss for batch 228 is 0.37589422845400866\n",
      "loss for batch 228 is 0.3805200574894135\n",
      "loss for batch 228 is 0.370909313621441\n",
      "loss for batch 228 is 0.3837752829018337\n",
      "loss for batch 228 is 0.34406099062566836\n",
      "loss for batch 228 is 0.3505161828378295\n",
      "Epoch 228/300, Average Loss: 0.3700534249462945\n",
      "loss for batch 229 is 0.37578437255462915\n",
      "loss for batch 229 is 0.37011427936831914\n",
      "loss for batch 229 is 0.38116326834730935\n",
      "loss for batch 229 is 0.3684227378731765\n",
      "loss for batch 229 is 0.3759738272531743\n",
      "loss for batch 229 is 0.38071353183518\n",
      "loss for batch 229 is 0.3710542963511776\n",
      "loss for batch 229 is 0.3839728296350124\n",
      "loss for batch 229 is 0.3441657589735971\n",
      "loss for batch 229 is 0.3506848558880701\n",
      "Epoch 229/300, Average Loss: 0.3702049758079645\n",
      "loss for batch 230 is 0.37593562408192993\n",
      "loss for batch 230 is 0.37031603520748885\n",
      "loss for batch 230 is 0.38127681730411317\n",
      "loss for batch 230 is 0.36857190648655186\n",
      "loss for batch 230 is 0.3760495475778719\n",
      "loss for batch 230 is 0.38090386465426185\n",
      "loss for batch 230 is 0.3711983011999696\n",
      "loss for batch 230 is 0.3841662129308981\n",
      "loss for batch 230 is 0.34427106286101344\n",
      "loss for batch 230 is 0.3508523797328318\n",
      "Epoch 230/300, Average Loss: 0.370354175203693\n",
      "loss for batch 231 is 0.37608584247759447\n",
      "loss for batch 231 is 0.3705195101240541\n",
      "loss for batch 231 is 0.38138897787554943\n",
      "loss for batch 231 is 0.36872441566882125\n",
      "loss for batch 231 is 0.37612566131044517\n",
      "loss for batch 231 is 0.38109751785381785\n",
      "loss for batch 231 is 0.3713457463824287\n",
      "loss for batch 231 is 0.3843593984333674\n",
      "loss for batch 231 is 0.34437867182239135\n",
      "loss for batch 231 is 0.35102027142923714\n",
      "Epoch 231/300, Average Loss: 0.3705046013377707\n",
      "loss for batch 232 is 0.3762330179435326\n",
      "loss for batch 232 is 0.37071874414361217\n",
      "loss for batch 232 is 0.38149786334809516\n",
      "loss for batch 232 is 0.3688735181603919\n",
      "loss for batch 232 is 0.37619487986817157\n",
      "loss for batch 232 is 0.38128393532319516\n",
      "loss for batch 232 is 0.3714865066233104\n",
      "loss for batch 232 is 0.3845448526562389\n",
      "loss for batch 232 is 0.3444877542643694\n",
      "loss for batch 232 is 0.3511903753571571\n",
      "Epoch 232/300, Average Loss: 0.37065114476880745\n",
      "loss for batch 233 is 0.3763755714851969\n",
      "loss for batch 233 is 0.3709134228272708\n",
      "loss for batch 233 is 0.3816069502154468\n",
      "loss for batch 233 is 0.36902161556168983\n",
      "loss for batch 233 is 0.3762626027929734\n",
      "loss for batch 233 is 0.38146861886393046\n",
      "loss for batch 233 is 0.3716289509705149\n",
      "loss for batch 233 is 0.38472989566278804\n",
      "loss for batch 233 is 0.34460232536816837\n",
      "loss for batch 233 is 0.3513651948547694\n",
      "Epoch 233/300, Average Loss: 0.3707975148602749\n",
      "loss for batch 234 is 0.3765200944209139\n",
      "loss for batch 234 is 0.371111294993897\n",
      "loss for batch 234 is 0.38171894197106887\n",
      "loss for batch 234 is 0.3691735504387008\n",
      "loss for batch 234 is 0.3763318011231594\n",
      "loss for batch 234 is 0.38165516744878397\n",
      "loss for batch 234 is 0.37177473897073415\n",
      "loss for batch 234 is 0.3849137995502042\n",
      "loss for batch 234 is 0.3447189254083907\n",
      "loss for batch 234 is 0.3515392524743058\n",
      "Epoch 234/300, Average Loss: 0.37094575668001584\n",
      "loss for batch 235 is 0.37666252542634227\n",
      "loss for batch 235 is 0.3713062185664657\n",
      "loss for batch 235 is 0.38182505264054545\n",
      "loss for batch 235 is 0.36932022429166134\n",
      "loss for batch 235 is 0.37639456768375884\n",
      "loss for batch 235 is 0.3818331557040613\n",
      "loss for batch 235 is 0.37191428921676484\n",
      "loss for batch 235 is 0.385085861370929\n",
      "loss for batch 235 is 0.3448310710817919\n",
      "loss for batch 235 is 0.351706440811259\n",
      "Epoch 235/300, Average Loss: 0.371087940679358\n",
      "loss for batch 236 is 0.37679715260189195\n",
      "loss for batch 236 is 0.3714943103469949\n",
      "loss for batch 236 is 0.38192177701154634\n",
      "loss for batch 236 is 0.3694644318298904\n",
      "loss for batch 236 is 0.37645316279651686\n",
      "loss for batch 236 is 0.38201115441583283\n",
      "loss for batch 236 is 0.37205641019342955\n",
      "loss for batch 236 is 0.3852576115551371\n",
      "loss for batch 236 is 0.3449481323735688\n",
      "loss for batch 236 is 0.35187509354505947\n",
      "Epoch 236/300, Average Loss: 0.3712279236669868\n",
      "loss for batch 237 is 0.3769370758598583\n",
      "loss for batch 237 is 0.37169162398066646\n",
      "loss for batch 237 is 0.3820183781658253\n",
      "loss for batch 237 is 0.3696206682272217\n",
      "loss for batch 237 is 0.3765153842577688\n",
      "loss for batch 237 is 0.38220016870316154\n",
      "loss for batch 237 is 0.3722068821322714\n",
      "loss for batch 237 is 0.38543283963557684\n",
      "loss for batch 237 is 0.3450694375972365\n",
      "loss for batch 237 is 0.35204271232229073\n",
      "Epoch 237/300, Average Loss: 0.37137351708818783\n",
      "loss for batch 238 is 0.377074852387505\n",
      "loss for batch 238 is 0.37188463006409295\n",
      "loss for batch 238 is 0.3821061357660031\n",
      "loss for batch 238 is 0.36976135972356183\n",
      "loss for batch 238 is 0.376559329243966\n",
      "loss for batch 238 is 0.3823575222405599\n",
      "loss for batch 238 is 0.37232702508898596\n",
      "loss for batch 238 is 0.38556588671022657\n",
      "loss for batch 238 is 0.34515204439063746\n",
      "loss for batch 238 is 0.352172535854019\n",
      "Epoch 238/300, Average Loss: 0.37149613214695576\n",
      "loss for batch 239 is 0.3771550282773238\n",
      "loss for batch 239 is 0.37200577528752415\n",
      "loss for batch 239 is 0.3821547178924816\n",
      "loss for batch 239 is 0.36981760246850864\n",
      "loss for batch 239 is 0.3765500683249891\n",
      "loss for batch 239 is 0.38243490488160925\n",
      "loss for batch 239 is 0.3723950387631467\n",
      "loss for batch 239 is 0.3856416911254664\n",
      "loss for batch 239 is 0.3451980017708717\n",
      "loss for batch 239 is 0.3522770053853412\n",
      "Epoch 239/300, Average Loss: 0.37156298341772626\n",
      "loss for batch 240 is 0.37720502248501236\n",
      "loss for batch 240 is 0.3721054978332604\n",
      "loss for batch 240 is 0.38219694897438106\n",
      "loss for batch 240 is 0.3698737816796904\n",
      "loss for batch 240 is 0.3765446284567145\n",
      "loss for batch 240 is 0.3825298618176992\n",
      "loss for batch 240 is 0.3724833359188786\n",
      "loss for batch 240 is 0.38573611234499455\n",
      "loss for batch 240 is 0.3452651319300991\n",
      "loss for batch 240 is 0.35239675570389145\n",
      "Epoch 240/300, Average Loss: 0.3716337077144622\n",
      "loss for batch 241 is 0.3772718473742669\n",
      "loss for batch 241 is 0.3722247254704402\n",
      "loss for batch 241 is 0.3822365865004771\n",
      "loss for batch 241 is 0.3699390116662743\n",
      "loss for batch 241 is 0.3765349836645904\n",
      "loss for batch 241 is 0.382617291679196\n",
      "loss for batch 241 is 0.37255996964123217\n",
      "loss for batch 241 is 0.3858072761605208\n",
      "loss for batch 241 is 0.3453221290030318\n",
      "loss for batch 241 is 0.3525019398806823\n",
      "Epoch 241/300, Average Loss: 0.3717015761040712\n",
      "loss for batch 242 is 0.3773161086360378\n",
      "loss for batch 242 is 0.37231725716372266\n",
      "loss for batch 242 is 0.3822503096580727\n",
      "loss for batch 242 is 0.3699817748971127\n",
      "loss for batch 242 is 0.37650311932682096\n",
      "loss for batch 242 is 0.38268045647277305\n",
      "loss for batch 242 is 0.37261974180072394\n",
      "loss for batch 242 is 0.38585649037960795\n",
      "loss for batch 242 is 0.34537394713922787\n",
      "loss for batch 242 is 0.35259999457388946\n",
      "Epoch 242/300, Average Loss: 0.3717499200047989\n",
      "loss for batch 243 is 0.3773474357816931\n",
      "loss for batch 243 is 0.3723981525481209\n",
      "loss for batch 243 is 0.38225114008317473\n",
      "loss for batch 243 is 0.37001605210184724\n",
      "loss for batch 243 is 0.37646047918871134\n",
      "loss for batch 243 is 0.38273301875892785\n",
      "loss for batch 243 is 0.3726734334009546\n",
      "loss for batch 243 is 0.38589199933054186\n",
      "loss for batch 243 is 0.3454232390683302\n",
      "loss for batch 243 is 0.35269281743841785\n",
      "Epoch 243/300, Average Loss: 0.371788776770072\n",
      "loss for batch 244 is 0.3773690371776032\n",
      "loss for batch 244 is 0.372469007516741\n",
      "loss for batch 244 is 0.3822410037895794\n",
      "loss for batch 244 is 0.37004036741498286\n",
      "loss for batch 244 is 0.3764065062746758\n",
      "loss for batch 244 is 0.3827726878792287\n",
      "loss for batch 244 is 0.37271842215659107\n",
      "loss for batch 244 is 0.38591249339878736\n",
      "loss for batch 244 is 0.34546844589042514\n",
      "loss for batch 244 is 0.35277912770513037\n",
      "Epoch 244/300, Average Loss: 0.3718177099203745\n",
      "loss for batch 245 is 0.3773803004313682\n",
      "loss for batch 245 is 0.3725315441286149\n",
      "loss for batch 245 is 0.38222085011257806\n",
      "loss for batch 245 is 0.3700584827790227\n",
      "loss for batch 245 is 0.37634289049794223\n",
      "loss for batch 245 is 0.3828043516984445\n",
      "loss for batch 245 is 0.37275892785462394\n",
      "loss for batch 245 is 0.38592022667855524\n",
      "loss for batch 245 is 0.34551000546825356\n",
      "loss for batch 245 is 0.35285891334620445\n",
      "Epoch 245/300, Average Loss: 0.3718386492995608\n",
      "loss for batch 246 is 0.3773820085405786\n",
      "loss for batch 246 is 0.3725863674452592\n",
      "loss for batch 246 is 0.3821909749224339\n",
      "loss for batch 246 is 0.37007112702341893\n",
      "loss for batch 246 is 0.37627133639894295\n",
      "loss for batch 246 is 0.38283170364663294\n",
      "loss for batch 246 is 0.3727992536931448\n",
      "loss for batch 246 is 0.3859220365722076\n",
      "loss for batch 246 is 0.3455530550694097\n",
      "loss for batch 246 is 0.3529385458030292\n",
      "Epoch 246/300, Average Loss: 0.3718546409115057\n",
      "loss for batch 247 is 0.37738245953103244\n",
      "loss for batch 247 is 0.3726438219449711\n",
      "loss for batch 247 is 0.3821618651943722\n",
      "loss for batch 247 is 0.37008668443131354\n",
      "loss for batch 247 is 0.376196904508283\n",
      "loss for batch 247 is 0.3828581204482222\n",
      "loss for batch 247 is 0.3728394738327112\n",
      "loss for batch 247 is 0.3859138809134939\n",
      "loss for batch 247 is 0.34559342783015407\n",
      "loss for batch 247 is 0.3530116983739018\n",
      "Epoch 247/300, Average Loss: 0.3718688337008455\n",
      "loss for batch 248 is 0.3773699605210612\n",
      "loss for batch 248 is 0.37268825038291087\n",
      "loss for batch 248 is 0.38211907451743565\n",
      "loss for batch 248 is 0.3700895975260571\n",
      "loss for batch 248 is 0.37610787225794357\n",
      "loss for batch 248 is 0.38287017065295414\n",
      "loss for batch 248 is 0.37287182001525\n",
      "loss for batch 248 is 0.38589126164375925\n",
      "loss for batch 248 is 0.34563213508719787\n",
      "loss for batch 248 is 0.3530813547861504\n",
      "Epoch 248/300, Average Loss: 0.37187214973907196\n",
      "loss for batch 249 is 0.37735164760120826\n",
      "loss for batch 249 is 0.372731454025457\n",
      "loss for batch 249 is 0.38207181379482547\n",
      "loss for batch 249 is 0.3700939600479682\n",
      "loss for batch 249 is 0.37601620724484314\n",
      "loss for batch 249 is 0.38288213607364013\n",
      "loss for batch 249 is 0.372904902193923\n",
      "loss for batch 249 is 0.3858605562408204\n",
      "loss for batch 249 is 0.3456676268853801\n",
      "loss for batch 249 is 0.3531446569391752\n",
      "Epoch 249/300, Average Loss: 0.37187249610472406\n",
      "loss for batch 250 is 0.37732280310063226\n",
      "loss for batch 250 is 0.37276426584642197\n",
      "loss for batch 250 is 0.3820138339469398\n",
      "loss for batch 250 is 0.3700891006002971\n",
      "loss for batch 250 is 0.375910724170173\n",
      "loss for batch 250 is 0.3828813161647774\n",
      "loss for batch 250 is 0.37292891041749243\n",
      "loss for batch 250 is 0.3858133131299741\n",
      "loss for batch 250 is 0.3457033302678578\n",
      "loss for batch 250 is 0.35320657949779816\n",
      "Epoch 250/300, Average Loss: 0.3718634177142365\n",
      "loss for batch 251 is 0.37728253108757165\n",
      "loss for batch 251 is 0.3727873353035321\n",
      "loss for batch 251 is 0.3819490889324179\n",
      "loss for batch 251 is 0.3700790064581509\n",
      "loss for batch 251 is 0.3757947012610564\n",
      "loss for batch 251 is 0.3828713998094968\n",
      "loss for batch 251 is 0.3729476268111699\n",
      "loss for batch 251 is 0.38575188876143857\n",
      "loss for batch 251 is 0.3457383880812275\n",
      "loss for batch 251 is 0.3532648256410515\n",
      "Epoch 251/300, Average Loss: 0.3718466792147113\n",
      "loss for batch 252 is 0.3772316845540095\n",
      "loss for batch 252 is 0.37280125555635557\n",
      "loss for batch 252 is 0.38187583687428556\n",
      "loss for batch 252 is 0.3700619829206535\n",
      "loss for batch 252 is 0.37566743692735044\n",
      "loss for batch 252 is 0.38285055421855924\n",
      "loss for batch 252 is 0.3729592938162435\n",
      "loss for batch 252 is 0.3856735021419384\n",
      "loss for batch 252 is 0.34576966241386126\n",
      "loss for batch 252 is 0.353316356057866\n",
      "Epoch 252/300, Average Loss: 0.3718207565481123\n",
      "loss for batch 253 is 0.3771676886321045\n",
      "loss for batch 253 is 0.3728029733043152\n",
      "loss for batch 253 is 0.38179205120631715\n",
      "loss for batch 253 is 0.3700348859664325\n",
      "loss for batch 253 is 0.37552771843034893\n",
      "loss for batch 253 is 0.38281722447515587\n",
      "loss for batch 253 is 0.37296420934230007\n",
      "loss for batch 253 is 0.38557830185272757\n",
      "loss for batch 253 is 0.3457972600964436\n",
      "loss for batch 253 is 0.353361435990394\n",
      "Epoch 253/300, Average Loss: 0.371784374929654\n",
      "loss for batch 254 is 0.3770910675062835\n",
      "loss for batch 254 is 0.3727933986980603\n",
      "loss for batch 254 is 0.3816999719450364\n",
      "loss for batch 254 is 0.36999983110608126\n",
      "loss for batch 254 is 0.37537712658215777\n",
      "loss for batch 254 is 0.3827720729051106\n",
      "loss for batch 254 is 0.3729623093274628\n",
      "loss for batch 254 is 0.3854655912062917\n",
      "loss for batch 254 is 0.3458208432157419\n",
      "loss for batch 254 is 0.35339942920304557\n",
      "Epoch 254/300, Average Loss: 0.37173816416952715\n",
      "loss for batch 255 is 0.3770013962423067\n",
      "loss for batch 255 is 0.3727714350594427\n",
      "loss for batch 255 is 0.38159808553585733\n",
      "loss for batch 255 is 0.36995587512368794\n",
      "loss for batch 255 is 0.37521583638284906\n",
      "loss for batch 255 is 0.3827169862960973\n",
      "loss for batch 255 is 0.37295654771947134\n",
      "loss for batch 255 is 0.38534013854378685\n",
      "loss for batch 255 is 0.3458440311270839\n",
      "loss for batch 255 is 0.35343413644723926\n",
      "Epoch 255/300, Average Loss: 0.37168344684778226\n",
      "loss for batch 256 is 0.3769041795317234\n",
      "loss for batch 256 is 0.3727440163779449\n",
      "loss for batch 256 is 0.3814922547119946\n",
      "loss for batch 256 is 0.3699101450234874\n",
      "loss for batch 256 is 0.37504842348780065\n",
      "loss for batch 256 is 0.3826568614784488\n",
      "loss for batch 256 is 0.3729491652088218\n",
      "loss for batch 256 is 0.3852029632797522\n",
      "loss for batch 256 is 0.34586745138810765\n",
      "loss for batch 256 is 0.35346545259825185\n",
      "Epoch 256/300, Average Loss: 0.3716240913086333\n",
      "loss for batch 257 is 0.3768001758275534\n",
      "loss for batch 257 is 0.37271171089005584\n",
      "loss for batch 257 is 0.3813813013881216\n",
      "loss for batch 257 is 0.3698623013051789\n",
      "loss for batch 257 is 0.3748754326634578\n",
      "loss for batch 257 is 0.38259243996176817\n",
      "loss for batch 257 is 0.3729416225397049\n",
      "loss for batch 257 is 0.3850556516297236\n",
      "loss for batch 257 is 0.3458915176250541\n",
      "loss for batch 257 is 0.35349378173611246\n",
      "Epoch 257/300, Average Loss: 0.3715605935566731\n",
      "loss for batch 258 is 0.3766883828356601\n",
      "loss for batch 258 is 0.37267186060572366\n",
      "loss for batch 258 is 0.3812658049834305\n",
      "loss for batch 258 is 0.36980995082768825\n",
      "loss for batch 258 is 0.37469466169565413\n",
      "loss for batch 258 is 0.38251926755468774\n",
      "loss for batch 258 is 0.37292961932111596\n",
      "loss for batch 258 is 0.38489374855648323\n",
      "loss for batch 258 is 0.34591433269660904\n",
      "loss for batch 258 is 0.35351736418462987\n",
      "Epoch 258/300, Average Loss: 0.37149049932616823\n",
      "loss for batch 259 is 0.3765664786289236\n",
      "loss for batch 259 is 0.37262131248909325\n",
      "loss for batch 259 is 0.38114657971236404\n",
      "loss for batch 259 is 0.3697539092424218\n",
      "loss for batch 259 is 0.3745069740613747\n",
      "loss for batch 259 is 0.38244199969040266\n",
      "loss for batch 259 is 0.37291654258715706\n",
      "loss for batch 259 is 0.3847234258316043\n",
      "loss for batch 259 is 0.3459399302670295\n",
      "loss for batch 259 is 0.35354072585716684\n",
      "Epoch 259/300, Average Loss: 0.37141578783675383\n",
      "loss for batch 260 is 0.3764415959685151\n",
      "loss for batch 260 is 0.372568663119529\n",
      "loss for batch 260 is 0.38102873316242875\n",
      "loss for batch 260 is 0.36969995748027024\n",
      "loss for batch 260 is 0.37431756551644196\n",
      "loss for batch 260 is 0.38236303449673475\n",
      "loss for batch 260 is 0.37290408641851097\n",
      "loss for batch 260 is 0.38454568511274817\n",
      "loss for batch 260 is 0.34596754755723663\n",
      "loss for batch 260 is 0.35356257194315066\n",
      "Epoch 260/300, Average Loss: 0.37133994407755666\n",
      "loss for batch 261 is 0.3763116205881722\n",
      "loss for batch 261 is 0.3725104218203692\n",
      "loss for batch 261 is 0.3809084676752149\n",
      "loss for batch 261 is 0.3696435796974977\n",
      "loss for batch 261 is 0.37412310671579796\n",
      "loss for batch 261 is 0.38227841075566055\n",
      "loss for batch 261 is 0.3728896725842701\n",
      "loss for batch 261 is 0.38435673345975996\n",
      "loss for batch 261 is 0.345994982943588\n",
      "loss for batch 261 is 0.3535809670453872\n",
      "Epoch 261/300, Average Loss: 0.3712597963285718\n",
      "loss for batch 262 is 0.37617268451431957\n",
      "loss for batch 262 is 0.3724417674295827\n",
      "loss for batch 262 is 0.3807863559621785\n",
      "loss for batch 262 is 0.36958101276730204\n",
      "loss for batch 262 is 0.3739218360378727\n",
      "loss for batch 262 is 0.38218356729314107\n",
      "loss for batch 262 is 0.3728712592079864\n",
      "loss for batch 262 is 0.384151799074209\n",
      "loss for batch 262 is 0.34602165788061995\n",
      "loss for batch 262 is 0.3535953270133875\n",
      "Epoch 262/300, Average Loss: 0.3711727267180599\n",
      "loss for batch 263 is 0.37602379636709543\n",
      "loss for batch 263 is 0.37236230910652696\n",
      "loss for batch 263 is 0.380664668772098\n",
      "loss for batch 263 is 0.3695177812568152\n",
      "loss for batch 263 is 0.3737235506778304\n",
      "loss for batch 263 is 0.38209810156478335\n",
      "loss for batch 263 is 0.37287040747216404\n",
      "loss for batch 263 is 0.3839686321466004\n",
      "loss for batch 263 is 0.34607217516861066\n",
      "loss for batch 263 is 0.35363525432419335\n",
      "Epoch 263/300, Average Loss: 0.3710936676856718\n",
      "loss for batch 264 is 0.3759243924179829\n",
      "loss for batch 264 is 0.37234803612917783\n",
      "loss for batch 264 is 0.3805915295893983\n",
      "loss for batch 264 is 0.3695313300081757\n",
      "loss for batch 264 is 0.3735911611073062\n",
      "loss for batch 264 is 0.3821040357137232\n",
      "loss for batch 264 is 0.3729322413716961\n",
      "loss for batch 264 is 0.3838765961921875\n",
      "loss for batch 264 is 0.34615542302493824\n",
      "loss for batch 264 is 0.35370214297722014\n",
      "Epoch 264/300, Average Loss: 0.3710756888531806\n",
      "loss for batch 265 is 0.37588533080052805\n",
      "loss for batch 265 is 0.3723898443602316\n",
      "loss for batch 265 is 0.3805452834252102\n",
      "loss for batch 265 is 0.3695839529619331\n",
      "loss for batch 265 is 0.3734854109905663\n",
      "loss for batch 265 is 0.382136547265022\n",
      "loss for batch 265 is 0.3730088336281499\n",
      "loss for batch 265 is 0.3837917037847042\n",
      "loss for batch 265 is 0.3462473746566037\n",
      "loss for batch 265 is 0.3537738400327038\n",
      "Epoch 265/300, Average Loss: 0.37108481219056527\n",
      "loss for batch 266 is 0.37583898104444907\n",
      "loss for batch 266 is 0.3724214793368199\n",
      "loss for batch 266 is 0.3804984021165184\n",
      "loss for batch 266 is 0.36962981040179954\n",
      "loss for batch 266 is 0.3733718729042023\n",
      "loss for batch 266 is 0.3821583975549351\n",
      "loss for batch 266 is 0.3730812226883067\n",
      "loss for batch 266 is 0.3836998885566665\n",
      "loss for batch 266 is 0.34634470349544005\n",
      "loss for batch 266 is 0.3538492167554131\n",
      "Epoch 266/300, Average Loss: 0.37108939748545505\n",
      "loss for batch 267 is 0.37579916462491336\n",
      "loss for batch 267 is 0.37246652464227625\n",
      "loss for batch 267 is 0.3804596957709502\n",
      "loss for batch 267 is 0.3696965935248471\n",
      "loss for batch 267 is 0.3732754710464661\n",
      "loss for batch 267 is 0.38220673178627096\n",
      "loss for batch 267 is 0.37317038859193946\n",
      "loss for batch 267 is 0.38363902686363344\n",
      "loss for batch 267 is 0.3464496023176652\n",
      "loss for batch 267 is 0.35393089140309086\n",
      "Epoch 267/300, Average Loss: 0.3711094090572053\n",
      "loss for batch 268 is 0.3757848374382629\n",
      "loss for batch 268 is 0.37253559575661055\n",
      "loss for batch 268 is 0.3804312675344885\n",
      "loss for batch 268 is 0.3697830416060678\n",
      "loss for batch 268 is 0.37319719832242854\n",
      "loss for batch 268 is 0.38227748785390375\n",
      "loss for batch 268 is 0.373272989381998\n",
      "loss for batch 268 is 0.38359705307269343\n",
      "loss for batch 268 is 0.34656034384611556\n",
      "loss for batch 268 is 0.3540162280241204\n",
      "Epoch 268/300, Average Loss: 0.37114560428366894\n",
      "loss for batch 269 is 0.37577698927489855\n",
      "loss for batch 269 is 0.3726069096505118\n",
      "loss for batch 269 is 0.3804036208278409\n",
      "loss for batch 269 is 0.36986586711892155\n",
      "loss for batch 269 is 0.37311074387563065\n",
      "loss for batch 269 is 0.3823318263358324\n",
      "loss for batch 269 is 0.3733626759782371\n",
      "loss for batch 269 is 0.38353098405996805\n",
      "loss for batch 269 is 0.3466652362493795\n",
      "loss for batch 269 is 0.3540934458053769\n",
      "Epoch 269/300, Average Loss: 0.3711748299176597\n",
      "loss for batch 270 is 0.37575181331321184\n",
      "loss for batch 270 is 0.3726618245727218\n",
      "loss for batch 270 is 0.3803731718579769\n",
      "loss for batch 270 is 0.369945188010179\n",
      "loss for batch 270 is 0.37302304396458574\n",
      "loss for batch 270 is 0.3823868860728381\n",
      "loss for batch 270 is 0.3734556287601245\n",
      "loss for batch 270 is 0.38346949813086884\n",
      "loss for batch 270 is 0.3467769136575205\n",
      "loss for batch 270 is 0.3541767680240169\n",
      "Epoch 270/300, Average Loss: 0.37120207363640445\n",
      "loss for batch 271 is 0.3757352013813842\n",
      "loss for batch 271 is 0.3727245564135993\n",
      "loss for batch 271 is 0.38035281764948103\n",
      "loss for batch 271 is 0.37003276515450034\n",
      "loss for batch 271 is 0.37294065473216914\n",
      "loss for batch 271 is 0.38244547661658324\n",
      "loss for batch 271 is 0.3735509043130542\n",
      "loss for batch 271 is 0.3834063682800127\n",
      "loss for batch 271 is 0.34689110811304524\n",
      "loss for batch 271 is 0.35425988403683367\n",
      "Epoch 271/300, Average Loss: 0.3712339736690663\n",
      "loss for batch 272 is 0.3757143737868191\n",
      "loss for batch 272 is 0.3727805166653933\n",
      "loss for batch 272 is 0.3803334205985517\n",
      "loss for batch 272 is 0.37011829261066725\n",
      "loss for batch 272 is 0.3728569531686586\n",
      "loss for batch 272 is 0.38250071402665337\n",
      "loss for batch 272 is 0.3736457470007346\n",
      "loss for batch 272 is 0.3833397884030509\n",
      "loss for batch 272 is 0.3470084020980029\n",
      "loss for batch 272 is 0.35434478087726595\n",
      "Epoch 272/300, Average Loss: 0.3712642989235798\n",
      "loss for batch 273 is 0.37569350859642\n",
      "loss for batch 273 is 0.37283517947883754\n",
      "loss for batch 273 is 0.38032052291794105\n",
      "loss for batch 273 is 0.37020657867339885\n",
      "loss for batch 273 is 0.37277568746415174\n",
      "loss for batch 273 is 0.3825552489942963\n",
      "loss for batch 273 is 0.37374113961101185\n",
      "loss for batch 273 is 0.38326948530556626\n",
      "loss for batch 273 is 0.3471274810402586\n",
      "loss for batch 273 is 0.35442940924252525\n",
      "Epoch 273/300, Average Loss: 0.37129542413244077\n",
      "loss for batch 274 is 0.3756698447828466\n",
      "loss for batch 274 is 0.37288445087902955\n",
      "loss for batch 274 is 0.38031030905121876\n",
      "loss for batch 274 is 0.37029409157749604\n",
      "loss for batch 274 is 0.37269573379969423\n",
      "loss for batch 274 is 0.3826092191974493\n",
      "loss for batch 274 is 0.3738387671360645\n",
      "loss for batch 274 is 0.38319963359935055\n",
      "loss for batch 274 is 0.3472505738000643\n",
      "loss for batch 274 is 0.35451695310972586\n",
      "Epoch 274/300, Average Loss: 0.37132695769329394\n",
      "loss for batch 275 is 0.3756494533109684\n",
      "loss for batch 275 is 0.3729358109396295\n",
      "loss for batch 275 is 0.3803091697391079\n",
      "loss for batch 275 is 0.37038647446213546\n",
      "loss for batch 275 is 0.3726213182798801\n",
      "loss for batch 275 is 0.3826645469442395\n",
      "loss for batch 275 is 0.37393820988420284\n",
      "loss for batch 275 is 0.38312822319865397\n",
      "loss for batch 275 is 0.34737568463148044\n",
      "loss for batch 275 is 0.3546045050689445\n",
      "Epoch 275/300, Average Loss: 0.37136133964592427\n",
      "loss for batch 276 is 0.37562849521139596\n",
      "loss for batch 276 is 0.37298417449874793\n",
      "loss for batch 276 is 0.3803116341226803\n",
      "loss for batch 276 is 0.37047880248283144\n",
      "loss for batch 276 is 0.37254983771344835\n",
      "loss for batch 276 is 0.38271999991205075\n",
      "loss for batch 276 is 0.3740405072945096\n",
      "loss for batch 276 is 0.3830590021914264\n",
      "loss for batch 276 is 0.3475043008952547\n",
      "loss for batch 276 is 0.35469459440086004\n",
      "Epoch 276/300, Average Loss: 0.37139713487232057\n",
      "loss for batch 277 is 0.37561240213736347\n",
      "loss for batch 277 is 0.3730356312756284\n",
      "loss for batch 277 is 0.38032524735727885\n",
      "loss for batch 277 is 0.3705777150419958\n",
      "loss for batch 277 is 0.3724874367712252\n",
      "loss for batch 277 is 0.382781335037371\n",
      "loss for batch 277 is 0.3741481841874689\n",
      "loss for batch 277 is 0.3829936161165462\n",
      "loss for batch 277 is 0.34763565135181607\n",
      "loss for batch 277 is 0.35478565310531673\n",
      "Epoch 277/300, Average Loss: 0.37143828723820105\n",
      "loss for batch 278 is 0.37559761665039915\n",
      "loss for batch 278 is 0.3730834576049539\n",
      "loss for batch 278 is 0.38034374803216114\n",
      "loss for batch 278 is 0.370675725056354\n",
      "loss for batch 278 is 0.37242632350569177\n",
      "loss for batch 278 is 0.3828388972797763\n",
      "loss for batch 278 is 0.37425453294816396\n",
      "loss for batch 278 is 0.38292528536298676\n",
      "loss for batch 278 is 0.34777182268651274\n",
      "loss for batch 278 is 0.35488063625164207\n",
      "Epoch 278/300, Average Loss: 0.3714798045378642\n",
      "loss for batch 279 is 0.37558451735428533\n",
      "loss for batch 279 is 0.37313211179811806\n",
      "loss for batch 279 is 0.38037675547892696\n",
      "loss for batch 279 is 0.37078463332430844\n",
      "loss for batch 279 is 0.37237835798320557\n",
      "loss for batch 279 is 0.3829108552467234\n",
      "loss for batch 279 is 0.3743729455463727\n",
      "loss for batch 279 is 0.38288273086302893\n",
      "loss for batch 279 is 0.3479190092035571\n",
      "loss for batch 279 is 0.3549895066198292\n",
      "Epoch 279/300, Average Loss: 0.37153314234183554\n",
      "loss for batch 280 is 0.3756043170637971\n",
      "loss for batch 280 is 0.37321253017653444\n",
      "loss for batch 280 is 0.38043602482833505\n",
      "loss for batch 280 is 0.37092627593336536\n",
      "loss for batch 280 is 0.37237278333528884\n",
      "loss for batch 280 is 0.3830297360926935\n",
      "loss for batch 280 is 0.3745193042982768\n",
      "loss for batch 280 is 0.38289732016454125\n",
      "loss for batch 280 is 0.3480780059834786\n",
      "loss for batch 280 is 0.3551127618514758\n",
      "Epoch 280/300, Average Loss: 0.3716189059727787\n",
      "loss for batch 281 is 0.37566233577569075\n",
      "loss for batch 281 is 0.3733240835112222\n",
      "loss for batch 281 is 0.3805120705085721\n",
      "loss for batch 281 is 0.3710826207842734\n",
      "loss for batch 281 is 0.37238520905563893\n",
      "loss for batch 281 is 0.383163477868072\n",
      "loss for batch 281 is 0.37466949291476837\n",
      "loss for batch 281 is 0.38292600338204047\n",
      "loss for batch 281 is 0.34823077299932537\n",
      "loss for batch 281 is 0.3552281791299498\n",
      "Epoch 281/300, Average Loss: 0.37171842459295534\n",
      "loss for batch 282 is 0.3757204865385013\n",
      "loss for batch 282 is 0.3734340019653677\n",
      "loss for batch 282 is 0.3805863706008561\n",
      "loss for batch 282 is 0.371232785925752\n",
      "loss for batch 282 is 0.37239379599522493\n",
      "loss for batch 282 is 0.3832866860357057\n",
      "loss for batch 282 is 0.3748102326681973\n",
      "loss for batch 282 is 0.3829388210569068\n",
      "loss for batch 282 is 0.3483757125923625\n",
      "loss for batch 282 is 0.35533287170504835\n",
      "Epoch 282/300, Average Loss: 0.37181117650839224\n",
      "loss for batch 283 is 0.375758943388679\n",
      "loss for batch 283 is 0.37352142809467576\n",
      "loss for batch 283 is 0.38065365063037715\n",
      "loss for batch 283 is 0.37136954070339473\n",
      "loss for batch 283 is 0.3723948400733546\n",
      "loss for batch 283 is 0.3833992727682035\n",
      "loss for batch 283 is 0.3749487311022829\n",
      "loss for batch 283 is 0.38295100069743737\n",
      "loss for batch 283 is 0.34852325795245004\n",
      "loss for batch 283 is 0.35544158894531863\n",
      "Epoch 283/300, Average Loss: 0.3718962254356174\n",
      "loss for batch 284 is 0.3758125036358106\n",
      "loss for batch 284 is 0.3736257934937069\n",
      "loss for batch 284 is 0.38073954605070975\n",
      "loss for batch 284 is 0.37152631225172733\n",
      "loss for batch 284 is 0.3724219599212257\n",
      "loss for batch 284 is 0.3835404258888202\n",
      "loss for batch 284 is 0.37510728759046597\n",
      "loss for batch 284 is 0.38299394878465354\n",
      "loss for batch 284 is 0.34867974679664904\n",
      "loss for batch 284 is 0.35555960335309195\n",
      "Epoch 284/300, Average Loss: 0.3720007127766861\n",
      "loss for batch 285 is 0.3758838481414066\n",
      "loss for batch 285 is 0.37374035726039945\n",
      "loss for batch 285 is 0.3808347917458831\n",
      "loss for batch 285 is 0.3716857032162638\n",
      "loss for batch 285 is 0.37245266166920904\n",
      "loss for batch 285 is 0.38367650950242366\n",
      "loss for batch 285 is 0.37526231917628383\n",
      "loss for batch 285 is 0.38302782282758874\n",
      "loss for batch 285 is 0.3488355186850872\n",
      "loss for batch 285 is 0.35567523342044643\n",
      "Epoch 285/300, Average Loss: 0.37210747656449916\n",
      "loss for batch 286 is 0.3759504567462016\n",
      "loss for batch 286 is 0.3738478073238201\n",
      "loss for batch 286 is 0.3809340608542475\n",
      "loss for batch 286 is 0.3718455836298569\n",
      "loss for batch 286 is 0.3724881790062922\n",
      "loss for batch 286 is 0.3838167187176075\n",
      "loss for batch 286 is 0.3754243194574984\n",
      "loss for batch 286 is 0.38307226499585206\n",
      "loss for batch 286 is 0.34899560826691983\n",
      "loss for batch 286 is 0.3557938389693335\n",
      "Epoch 286/300, Average Loss: 0.37221688379676293\n",
      "loss for batch 287 is 0.37602913066638327\n",
      "loss for batch 287 is 0.37396638161548446\n",
      "loss for batch 287 is 0.38104471873667345\n",
      "loss for batch 287 is 0.37201228454399393\n",
      "loss for batch 287 is 0.3725327212565825\n",
      "loss for batch 287 is 0.38395885668652024\n",
      "loss for batch 287 is 0.3755866595388049\n",
      "loss for batch 287 is 0.38310175996684986\n",
      "loss for batch 287 is 0.3491508279545027\n",
      "loss for batch 287 is 0.35590249968779547\n",
      "Epoch 287/300, Average Loss: 0.37232858406535907\n",
      "loss for batch 288 is 0.3760811730341146\n",
      "loss for batch 288 is 0.3740492414395867\n",
      "loss for batch 288 is 0.3811446498863142\n",
      "loss for batch 288 is 0.37215166332933247\n",
      "loss for batch 288 is 0.3725494732307314\n",
      "loss for batch 288 is 0.3840597179447765\n",
      "loss for batch 288 is 0.3757313263712629\n",
      "loss for batch 288 is 0.3830834406785311\n",
      "loss for batch 288 is 0.3493027105288639\n",
      "loss for batch 288 is 0.3560048193901821\n",
      "Epoch 288/300, Average Loss: 0.3724158215833696\n",
      "loss for batch 289 is 0.3761094041785282\n",
      "loss for batch 289 is 0.3741079512122228\n",
      "loss for batch 289 is 0.3812485549270272\n",
      "loss for batch 289 is 0.3722828458471602\n",
      "loss for batch 289 is 0.37255890814047965\n",
      "loss for batch 289 is 0.38414873521628495\n",
      "loss for batch 289 is 0.3758748519058965\n",
      "loss for batch 289 is 0.3830508204917135\n",
      "loss for batch 289 is 0.34945690666750084\n",
      "loss for batch 289 is 0.3561043409967396\n",
      "Epoch 289/300, Average Loss: 0.3724943319583553\n",
      "loss for batch 290 is 0.3761349208098701\n",
      "loss for batch 290 is 0.37416403437175627\n",
      "loss for batch 290 is 0.3813636296144033\n",
      "loss for batch 290 is 0.3724202105736105\n",
      "loss for batch 290 is 0.3725874250687816\n",
      "loss for batch 290 is 0.3842583224659222\n",
      "loss for batch 290 is 0.3760391111539133\n",
      "loss for batch 290 is 0.3830561085669161\n",
      "loss for batch 290 is 0.34962303392923477\n",
      "loss for batch 290 is 0.3562175351082019\n",
      "Epoch 290/300, Average Loss: 0.372586433166261\n",
      "loss for batch 291 is 0.37619450110749647\n",
      "loss for batch 291 is 0.37424741948693424\n",
      "loss for batch 291 is 0.3815067733990064\n",
      "loss for batch 291 is 0.37257536052622015\n",
      "loss for batch 291 is 0.3726400312949585\n",
      "loss for batch 291 is 0.38438157813007645\n",
      "loss for batch 291 is 0.3762118584412643\n",
      "loss for batch 291 is 0.38306471827487737\n",
      "loss for batch 291 is 0.34979232876533034\n",
      "loss for batch 291 is 0.35633231159496515\n",
      "Epoch 291/300, Average Loss: 0.3726946881021129\n",
      "loss for batch 292 is 0.37625446161599424\n",
      "loss for batch 292 is 0.37432654652173286\n",
      "loss for batch 292 is 0.3816627796669779\n",
      "loss for batch 292 is 0.3727346557618037\n",
      "loss for batch 292 is 0.37270593886264175\n",
      "loss for batch 292 is 0.38451573551461476\n",
      "loss for batch 292 is 0.3763994538048026\n",
      "loss for batch 292 is 0.3830962385086512\n",
      "loss for batch 292 is 0.3499715904238833\n",
      "loss for batch 292 is 0.3564581430725362\n",
      "Epoch 292/300, Average Loss: 0.3728125543753638\n",
      "loss for batch 293 is 0.3763403471206473\n",
      "loss for batch 293 is 0.37442649659332544\n",
      "loss for batch 293 is 0.38184252824977977\n",
      "loss for batch 293 is 0.37291114459705854\n",
      "loss for batch 293 is 0.37279860807263654\n",
      "loss for batch 293 is 0.3846699298533762\n",
      "loss for batch 293 is 0.376600754976562\n",
      "loss for batch 293 is 0.3831476775827166\n",
      "loss for batch 293 is 0.35015812233432037\n",
      "loss for batch 293 is 0.35659176817213944\n",
      "Epoch 293/300, Average Loss: 0.3729487377552562\n",
      "loss for batch 294 is 0.3764405682554286\n",
      "loss for batch 294 is 0.3745347340412531\n",
      "loss for batch 294 is 0.3820403948485713\n",
      "loss for batch 294 is 0.3730988806134191\n",
      "loss for batch 294 is 0.37291193925088995\n",
      "loss for batch 294 is 0.38484360192115763\n",
      "loss for batch 294 is 0.37682175828034387\n",
      "loss for batch 294 is 0.38323651508883183\n",
      "loss for batch 294 is 0.3503583240230645\n",
      "loss for batch 294 is 0.3567439393778709\n",
      "Epoch 294/300, Average Loss: 0.37310306557008305\n",
      "loss for batch 295 is 0.37658583162792547\n",
      "loss for batch 295 is 0.3746865440771312\n",
      "loss for batch 295 is 0.38227664120694305\n",
      "loss for batch 295 is 0.3733235828334598\n",
      "loss for batch 295 is 0.3730773655586644\n",
      "loss for batch 295 is 0.3850689920979002\n",
      "loss for batch 295 is 0.3770753090533293\n",
      "loss for batch 295 is 0.3833809995485772\n",
      "loss for batch 295 is 0.35057218365290466\n",
      "loss for batch 295 is 0.35691276849747344\n",
      "Epoch 295/300, Average Loss: 0.37329602181543087\n",
      "loss for batch 296 is 0.3767633896733073\n",
      "loss for batch 296 is 0.3748613893729122\n",
      "loss for batch 296 is 0.3825329872506676\n",
      "loss for batch 296 is 0.3735613049364116\n",
      "loss for batch 296 is 0.37325985995976124\n",
      "loss for batch 296 is 0.3853065156713402\n",
      "loss for batch 296 is 0.3773379889526751\n",
      "loss for batch 296 is 0.3835378720207448\n",
      "loss for batch 296 is 0.3507916622432918\n",
      "loss for batch 296 is 0.3570866986553242\n",
      "Epoch 296/300, Average Loss: 0.37350396687364357\n",
      "loss for batch 297 is 0.37694984702875434\n",
      "loss for batch 297 is 0.375041960217242\n",
      "loss for batch 297 is 0.3828009193377997\n",
      "loss for batch 297 is 0.37380795250850135\n",
      "loss for batch 297 is 0.373458712691994\n",
      "loss for batch 297 is 0.3855611374088008\n",
      "loss for batch 297 is 0.37761688881439553\n",
      "loss for batch 297 is 0.3837289875182913\n",
      "loss for batch 297 is 0.35102327528139515\n",
      "loss for batch 297 is 0.35727860486791696\n",
      "Epoch 297/300, Average Loss: 0.3737268285675091\n",
      "loss for batch 298 is 0.3771822784327759\n",
      "loss for batch 298 is 0.3752698543324706\n",
      "loss for batch 298 is 0.3831071610380227\n",
      "loss for batch 298 is 0.374088792646833\n",
      "loss for batch 298 is 0.37370769143785904\n",
      "loss for batch 298 is 0.3858632452880064\n",
      "loss for batch 298 is 0.3779274219253646\n",
      "loss for batch 298 is 0.3839706173657429\n",
      "loss for batch 298 is 0.35126259267874543\n",
      "loss for batch 298 is 0.3574804592658108\n",
      "Epoch 298/300, Average Loss: 0.37398601144116317\n",
      "loss for batch 299 is 0.3774359199829514\n",
      "loss for batch 299 is 0.3755077775302362\n",
      "loss for batch 299 is 0.3834206500420089\n",
      "loss for batch 299 is 0.37437104949002714\n",
      "loss for batch 299 is 0.3739621118626636\n",
      "loss for batch 299 is 0.38616579191215017\n",
      "loss for batch 299 is 0.3782377182144924\n",
      "loss for batch 299 is 0.3842113890841132\n",
      "loss for batch 299 is 0.351501985348767\n",
      "loss for batch 299 is 0.35768155874978136\n",
      "Epoch 299/300, Average Loss: 0.3742495952217192\n"
     ]
    }
   ],
   "source": [
    "#training the module\n",
    "r = 5\n",
    "m = 2\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2*r - 1\n",
    "n_iter = 300\n",
    "alpha = 0.00005\n",
    "num_of_samples = 250\n",
    "num_train_batches = 10\n",
    "num_test_batches = 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, num_of_samples, num_train_batches,num_test_batches)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1,feed_forward1,attention2, feed_forward2, un_embed_pos, softmax]\n",
    "nueralnetsort = NeuralNetwork(layers)\n",
    "\n",
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "x_t = data['x_test'][0]\n",
    "y_t = data['y_test'][0]\n",
    "\n",
    "per, _ = sorting(nueralnetsort, x_t, y_t,m)\n",
    "print(f'prosent av antall riktige sorteringer før trening er {per*100}%')\n",
    "\n",
    "arr = algorithm_4_sort(x, y, n_iter, alpha, m, nueralnetsort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prosent av antall riktige sorteringer etter trening er 0.0%\n"
     ]
    }
   ],
   "source": [
    "# plt.plot(np.arange(len(arr)),arr)\n",
    "# plt.xlabel('Iterasjoner')\n",
    "# plt.ylabel('Logaritmen av losset')\n",
    "# plt.title('Plot av feil for sortering')\n",
    "# plt.show()\n",
    "\n",
    "per, z_hat = sorting(nueralnetsort, x_t, y_t,m)\n",
    "\n",
    "\n",
    "print(f'prosent av antall riktige sorteringer etter trening er {per*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapunkter = 250\n",
    "batches = 20\n",
    "data_add = get_train_test_addition(L, datapunkter, batches)\n",
    "\n",
    "d = 30\n",
    "k = 20\n",
    "p = 40 \n",
    "L = 3\n",
    "m = 10\n",
    "n_max = 9\n",
    "n_iter = 150\n",
    "alpha = 0.0001\n",
    "\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "feed_forward3 = FeedForward(d,p)\n",
    "attention3 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1,feed_forward1, attention2,feed_forward2, attention3,feed_forward3, un_embed_pos, softmax]\n",
    "nueralnetadd = NeuralNetwork(layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sorting() missing 1 required positional argument: 'm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m y_add_t \u001b[38;5;241m=\u001b[39m data_add[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m x_add_t \u001b[38;5;241m=\u001b[39m data_add[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m per \u001b[38;5;241m=\u001b[39m \u001b[43msorting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnueralnetadd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_add_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_add_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprosent av antall riktige addisjoner før trening er \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mper\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m arr3 \u001b[38;5;241m=\u001b[39m algorithm_4_sort(x_add, y_add, n_iter, alpha, m, nueralnetadd)\n",
      "\u001b[0;31mTypeError\u001b[0m: sorting() missing 1 required positional argument: 'm'"
     ]
    }
   ],
   "source": [
    "\n",
    "x_add = data_add['x_train']\n",
    "y_add = data_add['y_train']\n",
    "\n",
    "y_add_t = data_add['y_test']\n",
    "x_add_t = data_add['x_test']\n",
    "\n",
    "per = sorting(nueralnetadd, x_add_t, y_add_t)\n",
    "print(f'prosent av antall riktige addisjoner før trening er {per*100}%')\n",
    "\n",
    "arr3 = algorithm_4_sort(x_add, y_add, n_iter, alpha, m, nueralnetadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(arr2)),arr2)\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Logaritmen av losset')\n",
    "plt.title('Plot av feil for addisjon')\n",
    "plt.show()\n",
    "\n",
    "y_add_t = data_add['y_test']\n",
    "x_add_t = data_add['x_test']\n",
    "\n",
    "per = sorting(nueralnetadd, x_add_t, y_add_t)\n",
    "print(f'prosent av antall riktige addisjoner er {per*100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
