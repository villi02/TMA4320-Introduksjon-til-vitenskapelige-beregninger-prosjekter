{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "from data_generators import get_train_test_sorting\n",
    "from data_generators import get_train_test_addition\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppgave 1 - Forstå hvordan datasettene og transformermodellen er strukturert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.1 Gi et eksempel (som i likning $(10)$) på hvordan et datasett ${x, y}$ ville sett ut for å trene en transformermodell for å predikere et heltall $d$ gitt $d = a · b + c$ der $a, c$ er tosifrede heltall, mens $b$ er et ettsifret heltall, altså $9 ≥ b ∈ Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et sett av treningsdata kan genereres ved å la x bestå av sifrene i $a, b, c$ og alle sifrene i $d$ med unntak av det siste og lar $y = d$. Dermed vil formen for x være gitt ved $x = [a_0 , \\cdot \\cdot \\cdot, a_{r-1}, b_0, \\cdot \\cdot \\cdot, b_{r-1}, c_0, \\cdot \\cdot \\cdot, c_{r-1}, d_0, \\cdot \\cdot \\cdot, d_{r-1}]$. Gitt betingelsene i oppgaven over, la $r$ = 2, $a$ = 24, $b$ = 4, $c$ = 15 og dermed <br> $d$ = 111.  som gir oss x = $[2, 4, 4, 1, 5, 1, 1]$ og $y = [1, 1, 1]$. Merk at siste siffer i $d$ ikke er del av datasettet i x.  Modellen skal da gi $\\hat{z}$. Lengden av $\\hat{z}$, $n$, vil være gitt av lengden av x som har med lengden $n$. $\\hat{z}$ = [$\\hat{z}_0$, \\cdot \\cdot \\cdot, $\\hat{z}_5$] =  $f_{\\theta}([2, 4, 4, 1, 5, 1, 1])$. Ideelt er $\\theta$ optimert til en slik grad at <Br> $\\hat{y} = [\\hat{z}_3, \\hat{z}_4, \\hat{z}_5] = [1, 1, 1] = y$ er korrekt predikert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.2) Når optimeringen er ferdig, hvordan kan vi bruke modellen $f_{\\theta}$  til å predikere $d$ gitt $a, b, c$? Vis dette med et eksempel, på samme måte som i likning $(11)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gitt at optmeringen er ferdig, kan modellen korrekt predikere $d$. Denne prossesen av å predikere de neste sifferene i seqvensen gjøres fra å lære av de oppgitte datasettene. Følgende tabell viser hvordan dette fungerer. La verdiene være det samme som i forrige oppgave, $r = 2, a = 24, b = 4,$<Br> $c = 15$\n",
    "\n",
    "| Iterasjoner | Modell |\n",
    "|----------|----------|\n",
    "| $x^{(0)} = [2, 4, 0, 4, 1, 5]$ | $[\\hat{z}_0^{(0)}, \\hat{z}_1^{(0)}, \\hat{z}_2^{(0)}, \\hat{z}_3^{(0)}] = f_{\\theta}(x^{(0)})$|\n",
    "| $x^{(1)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}]$ | $[\\hat{z}_0^{(1)}, \\cdot \\cdot \\cdot, \\hat{z}_4^{(1)}] = f_{\\theta}(x^{(1)})$ |\n",
    "| $x^{(2)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}]$ | $[\\hat{z}_0^{(2)}, \\cdot \\cdot \\cdot, \\hat{z}_5^{(2)}] = f_{\\theta}(x^{(2)})$  |\n",
    "| $x^{(3)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}, \\hat{z}_5^{(2)}]$ |  |\n",
    "\n",
    "Disse predikasjonene hentes ut og returneres som $\\hat{y} = [\\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}, \\hat{z}_5^{(2)}]$ som bør være likt $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.3) Anta at vi bruker cross-entropy som objektfunksjon, at $m = 5$ og $y = [4, 3, 2, 1]$. Hvilke diskret sannsynlighetsfordeling $\\hat{Y}$ ville gitt en objektfunksjon $L(θ, D) = 0$? Hva ville $\\hat{y}$ vært i dette tilfellet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy er gitt ved $L(θ, D) = -\\frac{1}{D \\cdot n} \\sum_{i=0}^{D-1} \\sum_{j=0}^{n-1} \\log \\hat{Y}_{k,j}^{(i)}$ hvor $D$ er datapunktene, $\\theta $\n",
    "er parameterne, og $\\hat{Y}$ er sannsynlighetsfordelingen til den predikterte modellen, samt er $j$ og $i$ dimensjonene til $\\hat{Y}$. Det objektfunksjonen gjør er å sammenligne onehot(y) med $\\hat{Y}$. Hvis $L(θ, D) = 0$ vil den optimerte modellen og onehot(y) være identiske. Når dette inntreffer vil $argmax_{\\text{col}}(\\hat{Y})$ = $\\hat{y}$ som igjen er lik $y$. I dette tilfellet er $y = [4,3,2,1]$, som også vil være lik $\\hat{y}$.\n",
    "$\\hat{Y}$ vil være gitt av den diskrete sannsynlighetsfordelingen:<Br><Br> $\\hat{Y}$ =\n",
    "$\\left[\\begin{array}{ccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 0 & 0\\\\\n",
    "\\end{array}\\right]$ , <Br><Br> som er lik onehot($[4,3,2,1]$). Dette betyr i praksis at paramtetrene i transformenmodellen klarer å prediktere hva som kommer videre i sekvensen og vi ender opp med samme antatt løsning ($\\hat{y}$) som faktisk løsning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.4) Gitt $d, m, n_{max}, k, p$ og $L$. Hvor mange enkeltparametre har en transformermodell? Med enkeltparametre mener vi hvor mange tall $w ∈ R$ vi må bestemme ved optimering. En matrise $W ∈ R^{m×n}$ består av $m · n$ tall eller enkeltparametre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med gitte variabler for $d, m, n_{max}, k, p$ og  $L$ er antall enkeltparametre mulig å bestemme. Enkeltparametre er gitt med $w \\in \\mathbb{R}$ noe som uttrykkes med å se på dimensjoner for ulike parametermatriser i transformermodellen.\n",
    "\n",
    "$W_E$ og $W_P$ har henholdsvis dimensjonene $W_E \\in \\mathbb{R}^{d \\times m}$ og $ W_P \\in \\mathbb{R}^{d \\times n_{max}} $ som representerer parametermatrisen til en sekvens for x med lengde n, som skrives som $z_0$. I tilegg ønskes det å gjøre $L$ paramtriserte trasformasjoner på $z_0$, så man ender opp med $L \\cdot (d \\times m + d \\times n_{max})$ for embedding delen av enkeltparamtrene. Under unenbeddingen oppstår en ny parametermatrise $W_U$ som er en sekvens med lengde $n$ med heltall opp til $m$, den har dimensjonene $ W_U \\in \\mathbb{R}^{d \\times m} $. Attention-lag bidrar også til antall enkeltparamtre for transformmodellen, der har man 4 parametermatriser; $W_O, W_V, W_Q, W_K$ alle med samme dimensjon $\\mathbb{R}^{k \\times d} $. Transformermodellen har også en $feed$-$forward $ del som bidrar med to paramtermatriser $W_1$ og $W_2$ begge med dimensjoner $\\mathbb{R}^{p \\times d} $\n",
    "\n",
    "\n",
    "Hvis man tar disse parametermatrisene i betrakning og antar at $k < d < p$ vil man ha: \n",
    "$w = d \\times m+L\\cdot (d \\times m + d \\times n_{max}) + 4 \\cdot k \\times d + 2 \\cdot p \\times d $, enkeltparametre. (siden k og p er heltall man bestemmer selv er dette en rimelig antagelse å ta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.5 Transformermodellen er gitt i likningene $(4) - (9)$. La $n = n_{max} = 1,$  $m = d = k = p=2$ og $L=1$. Anta videre at $W_O = W_V = W_Q = W_K = W_1 = W_2 = W_U = I_{2×2}$ og at $σ(x) = Relu(x) = max(0, x)$. Dersom <Br> $W_E = \\left[\\begin{array}{ccc} 1 & 0 \\\\ 0 & \\alpha \\end{array} \\right]$ , og $W_P$ = $\\left[\\begin{array}{ccc} 1 \\\\ 0 \\end{array} \\right]$ vis at vi må ha $\\alpha > 1$ for å få  $\\hat{z} = [1]$ som output når input er $x = [1]$.\n",
    "*  *  *  * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med variablene oppgitt i oppgaven $L=n = n_{max}= x = 1$ og $m=d=k=p=d = 2$ og alle parametermatrisene lik\n",
    "\n",
    "$\\left[\\begin{array}{ccc}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{array}\\right]$ = $ I_{2\\times 2}$ , utenom $W_E = \\left[\\begin{array}{ccc}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{array}\\right]$ , og $W_P = \\left[\\begin{array}{ccc}\n",
    "1 \\\\\n",
    "0 \n",
    "\\end{array}\\right]$\n",
    "\n",
    "Med dette oppgitt vil  $ X = onehot(x) = \\left[\\begin{array}{ccc}0 \\\\1 \n",
    "\\end{array}\\right]$ som resulterer i en $z_0 = \\left[\\begin{array}{ccc}0 \\\\ \\alpha \n",
    "\\end{array}\\right]+ \\left[\\begin{array}{ccc}1 \\\\ 0 \n",
    "\\end{array}\\right]$ =$\\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$. For å videre bestemme et uttrykk for $\\hat{z}$ må vi se på hva transformermodellen gjør med $z_0$. \n",
    "\n",
    "Videre er $z_{1/2}$ = $z_0 + W_O^T  W_V  z_0 A(z_0)$, hvor $A(z_0)$ = $softmax_{col}(z_0^T W_Q^T W_K z_0+D)$ og D sørger for at den strengt nedre delen av A er 0.\n",
    "Ved å løse $A(z_0)$ får man utrykket $(1+ \\alpha ^2)$ i softmax funksjonen.\n",
    "\n",
    "$z_{1/2} = \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right] + I_{2 \\times 2} I_{2 \\times 2} \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right] softmax(1+ \\alpha ^2)$ = $ 2 \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$ ettersom softmax av et utrykk tilsvarer å dele på seg selv i e-potens, som resulterer at utrykket blir lik 1.\n",
    "\n",
    "for $z_1$ får vi et uttrykk som er $z_{1/2} + W_2^T \\sigma (W_1 z_{1/2})$, $\\sigma$ er en aktiveringsfunskjon, i dette tilfelle kan man bruke $relu(W_1 z_{1/2})$.\n",
    "Utrykket blir da:\n",
    "\n",
    "$z_1 = 2  \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]+ I_{2 \\times 2} max(0,I_{2 \\times 2} 2 \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]) $ = $ 4  \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$\n",
    " \n",
    "Ved hjelp av $z_1$ kan man ta i bruk likning $(8)$ for å finne sannsynlighetsfunksjonen $Z$.\n",
    "\n",
    " $Z = softmax_{col}(W_U^T Z_1)$ $,$ her vil argumentet $W_U^T z_1$ bli lik $z_1$, og softmax vil returnere $Z = \\frac{1}{e^4 + e^{4 \\alpha}} \\left[\\begin{array}{ccc}e^4 \\\\ e^{4 \\alpha }\n",
    "\\end{array}\\right]$\n",
    "\n",
    "for å få $\\hat{z} = [1]$ må $argmax(Z)$ bli 1, og dette krever at verdien på indeks [1] må være større enn den på indeks [0], da må $e^4 < e^{4 \\alpha}$ og dette impliserer at $\\alpha >1$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "## Oppgave 2 - Objektorientert programmering for transformermodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 I den utdelte koden layers.py og neural network.py finnes en objektorientert implementering av et nevralt nettverk som kan ha lineære lag og en Relu- aktiveringsfunksjon. I tillegg er embedding og posisjonsenkoding samt feed-forward lag implementert.<Br> Forklar hvordan NeuralNetwork bruker arv, eller inheritance, for å utføre en iterasjon av gradient descent (stepgd()) hvis vi antar det er initiert med minst ett LinearLayer i listen layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et nevralt nettverk lærer gjennom mange små gradevis forbedringer gjort gjennom å prøve og feile. Denne prossen består av mange ulike funksjoner og operasjoner, som beskrevet i forrige oppgave. Dette gjøres med formålet om å utføre en gradient descent. Før at det nevralenettverket skal fungere må layers initieres.\n",
    " \n",
    "Layers er en klasse som fungerer som en base klasse for alle andre typer layers i nettverket, som representerer de ulike prossesene det nevralenettveket utfører. Her implementeres en basis versjon av metoder som forward(), backward() og step_gd(). Dette vil si at hvis et objekt arver fra Layers klassen så vil det objektet har metodene forward(), backward() og step_gd(). Derved vil alle layers som arver fra Layers base klassen implementere eller overskrive disse metodene med kode tilrettet hver individuelle layer. Denne strukturen tillater også at et lag har en egen spesifiserte step_gd() med at det kan overskrive metoden til å være mest hensiktsmessig for det spesifikke laget. Resultatet av dette gjør at neural_network kan operere på et høyere abstraksjonsnivå og kan implementere \"universelle\" metoder som step_gd() uten å ta hensyn til de ulike spesifikke detaljene til hvert lag. \n",
    "\n",
    "Mer spesifikt bruker neural_network arv til å kunne behandle alle sine layers på samme måte, selv om de kan ha forskjellig implementerte step_gd() metoder. Polymorfisme lar da neural_network kalle samme funksjon (step_gd()) på samme måte for hvert lag uten å vite hvilken subklasse hvert layer tilhører. Dermed kan hver operasjon som nural_network utfører kalles gjennom bruk av forward(), for å finne objektfunksjonen, backwards(), for å resette og oppdatere verdiene i nettverket, og step_gd() for å optimalisere vektingen og biasene i treningen av nettverket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 0.30415141554949937\n",
      "Epoch 2/10, Average Loss: 0.30407156890592957\n",
      "Epoch 3/10, Average Loss: 0.3040088239372396\n",
      "Epoch 4/10, Average Loss: 0.3039594958913475\n",
      "Epoch 5/10, Average Loss: 0.30391896205390034\n",
      "Epoch 6/10, Average Loss: 0.30388430446592557\n",
      "Epoch 7/10, Average Loss: 0.3038537346631771\n",
      "Epoch 8/10, Average Loss: 0.30382612290189626\n",
      "Epoch 9/10, Average Loss: 0.30380073364327514\n",
      "Epoch 10/10, Average Loss: 0.30377705159475255\n"
     ]
    }
   ],
   "source": [
    "#training the module\n",
    "r = 5\n",
    "m = 2\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "n_max = 2*r - 1\n",
    "n_iter = 10\n",
    "alpha = 0.00001\n",
    "num_of_samples = 250\n",
    "num_train_batches = 10\n",
    "num_test_batches = 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, num_of_samples, num_train_batches,num_test_batches)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1, feed_forward1,attention2, feed_forward2, un_embed_pos, softmax]\n",
    "nueralnetsort = NeuralNetwork(layers)\n",
    "\n",
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "x_t = data['x_test'][0]\n",
    "y_t = data['y_test'][0]\n",
    "\n",
    "\n",
    "arr2 = algorithm_4(x, y, n_iter, alpha, m, nueralnetsort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,len(arr2),len(arr2)),np.log(arr2))\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Logaritmen av losset')\n",
    "plt.title('Plot av feil')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "##### 3.3 Implementer en funksjon som sorterer en sekvens a, på samme måte som kapittel 3.1.2 og rapporter hvor stor andel av disse sekvensene du klarte å sortere riktig. Hvis du regner ut hvor mange mulige sekvenser av lengde r = 5 med 0 og 1 det er mulig å generere, ser du kanskje at det er umulig å teste på nye sekvenser.\n",
    "\n",
    "Før et nevraltnettverk kan returnere korrekt output til en gitt input må den trenes. Denne treningen gjøres gjennom å prossesen beskrevet i de tidligere oppgavene ved å bruke et datasett for optimalisere outputet. Ideelt skal det nevralenettverket trenes på et stort datasett før det testes med ny data som ikke ble brukt i testing fasen. Dette gjøre for å kunne observere om det nevralenettverket kan korrekt predikere et resultat fra ny data istedet for å kun gjengi et resultat det tidligere har sett at er riktig respons til et gitt input.\n",
    "\n",
    "Dette er desverre vaskelig for et nevraltnettverk som skal lære seg å sortere en sekvens som $a$. Sekvensen $a$ er gitt ved parameterene $r = 5$ og $m = 2$. Dette betyr at sekvensen består av fem siffer og har to mulige siffer hvert element i sekvensen kan være, her 0 og 1. Dermed er antallet, $n$, for unike sekvenser som $a$ kan bestå av, være gitt ved $n = 2^5 = 32$. I tilegg er det kun 6 sekvenser som kan være riktig svar. I en tidligere iterasjon av koden ville programmet optimaliseres inn i et \"lokalt optima\" hvor den gjette $z = [0, 0, 0, 1, 1]$ på alle  ettersom dette er svaret som forekommer oftest i test dataen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prosent av antall riktige sorteringer før trening er 0.4%\n",
      "xt = [1. 2. 2. 1. 0.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 0. 1. 2.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 1. 2. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 0. 2.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 2. 0. 1.] yt = [0. 0. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 0. 2. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 0. 1. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 0. 1. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 2. 2. 2.] yt = [2. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 1. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 1. 1. 0.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 1. 2. 2.] yt = [1. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 2. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 2. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 2. 2. 2. 2.] yt = [1. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 2. 0.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 1. 2. 0. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 1. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 0. 1. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 0. 1. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 0. 0. 1.] yt = [0. 0. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 2. 1. 0.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 1. 2.] yt = [0. 1. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 0. 2. 1.] yt = [0. 1. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 1. 2. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 0. 2. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 2. 2. 2.] yt = [0. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 1. 1. 1.] yt = [1. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 0. 0. 0.] yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 1. 1. 2.] yt = [1. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 1. 2. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 2. 0. 2.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 2. 1. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 0. 1. 0. 2.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 1. 0. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 1. 0. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 1. 2. 0.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 2. 0. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 0. 0. 2.] yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 2. 1. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 2. 2. 1.] yt = [0. 0. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 2. 1. 2.] yt = [1. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 2. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 2. 1. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 2. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 0. 0. 1. 0.] yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 2. 0. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 2. 1. 0.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 2. 1.] yt = [1. 1. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 2. 0. 0. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 2. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 2. 2. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 1. 1. 0.] yt = [0. 1. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 2. 0. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 0. 0. 1.] yt = [0. 0. 0. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 0. 2. 1.] yt = [0. 1. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 2. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 0. 2. 0.] yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 0. 1. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 2. 1. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 2. 1. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 2. 0. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 1. 2. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 2. 2. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 0. 0. 0. 1.] yt = [0. 0. 0. 0. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 1. 2. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 0. 2. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 1. 2. 2.] yt = [1. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 2. 2. 1.] yt = [0. 0. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 2. 2. 2.] yt = [0. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 2. 2. 1.] yt = [1. 1. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 0. 2. 2.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 0. 1. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 0. 1. 2.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 0. 2. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 2. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 1. 2. 0.] yt = [0. 1. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 0. 1. 2.] yt = [0. 1. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 0. 1. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 2. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 2. 1. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 1. 0. 2.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 2. 0. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 1. 2. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 0. 2. 2.] yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 2. 0. 2.] yt = [0. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 2. 0. 1.] yt = [0. 0. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 0. 1. 2. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 1. 2.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 2. 0. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 0. 0. 1.] yt = [0. 0. 0. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 0. 2. 0. 1.] yt = [0. 0. 0. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 1. 1. 2. 2.] yt = [1. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 2. 2. 0.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 0. 1. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 1. 1. 2. 0.] yt = [0. 1. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 2. 2. 2.] yt = [1. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 1. 0. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 1. 2. 2.] yt = [1. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 0. 0. 1.] yt = [0. 0. 0. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 0. 0. 0. 2.] yt = [0. 0. 0. 0. 2.] and z = [2. 2. 1. 2. 1.]\n",
      "xt = [1. 2. 0. 0. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 1. 2. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 2. 2. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 1. 1. 1. 2.] yt = [1. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 2. 0.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 2. 2. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 2. 1. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 1. 1. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 1. 2. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 0. 1. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 2. 1. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 0.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 2. 0. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 0. 1. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 1. 2. 0.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 0. 2. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 1. 1. 2.] yt = [1. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 1. 1. 2.] yt = [0. 1. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 2. 1. 1.] yt = [1. 1. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 1. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 1. 1. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 2. 2. 0.] yt = [0. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 0. 2. 0.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 2. 2. 2. 0.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 2. 0. 2.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 2. 2. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 0. 1. 2.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 1. 2.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 2. 2. 0.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 1. 2.] yt = [1. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 1. 2.] yt = [0. 1. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 2. 2. 1.] yt = [0. 0. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 0. 2. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 0. 2. 2. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 2. 0. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 2. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 2. 2. 2.] yt = [0. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 1. 1. 2.] yt = [1. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 1. 1. 2.] yt = [1. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 1. 2. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 2. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 2. 0. 1.] yt = [0. 0. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 1. 2. 2. 0.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 1. 2. 2.] yt = [1. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 2. 2. 1.] yt = [1. 2. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 2. 1. 2.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 2. 2. 2.] yt = [0. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 0. 0. 2.] yt = [0. 0. 0. 0. 2.] and z = [2. 2. 1. 2. 1.]\n",
      "xt = [2. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 0. 2. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 1. 0. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 1. 2. 2.] yt = [1. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 0. 1. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 0. 0. 1.] yt = [0. 0. 0. 0. 1.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 1. 1. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 2. 1. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 2. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 2. 1. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 2. 1. 1.] yt = [1. 1. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 0. 2. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 2. 2. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 2. 0. 0. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 0. 2. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 2. 2. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 0. 0. 1.] yt = [0. 0. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 2. 1.] yt = [1. 1. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 2. 1. 0. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 1. 0. 2.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 1. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 1. 0. 2. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 2. 0. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 1. 2. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 2. 2. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 2. 0. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 1. 2. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 2. 2. 1.] yt = [0. 1. 2. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 0. 2. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 0. 1. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 0. 2. 0. 2.] yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 0. 0. 2.] yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 0. 2.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 2. 2. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 2. 2. 2.] yt = [2. 2. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 0. 1. 1. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 1. 1. 2.] yt = [1. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 0. 0. 0.] yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 0. 2. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 2. 0. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 1. 2.] yt = [1. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 2. 0.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 1. 2. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 0. 2. 2. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 0. 2. 2.] yt = [0. 0. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 0. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 2. 1. 1.] yt = [0. 1. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 0. 1. 2.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 2. 2. 2.] yt = [1. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 0. 0. 2.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 2. 1. 1. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 2. 0. 1. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [0. 1. 2. 0. 0.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 0. 0. 2. 1.] yt = [0. 0. 1. 1. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 1. 1. 2. 0.] yt = [0. 1. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 1. 0. 2. 0.] yt = [0. 0. 1. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [2. 1. 2. 0. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [2. 1. 0. 2. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 2. 1. 0.] yt = [0. 0. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 2. 1. 2. 2.] yt = [0. 1. 2. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 1. 0.] yt = [0. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [0. 1. 0. 0. 2.] yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 0. 2. 1.] yt = [0. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "xt = [1. 2. 1. 1. 2.] yt = [1. 1. 1. 2. 2.] and z = [0. 0. 0. 0. 0.]\n",
      "xt = [1. 2. 2. 1. 1.] yt = [1. 1. 1. 2. 2.] and z = [2. 1. 1. 2. 1.]\n",
      "Epoch 1/300, Average Loss: 0.45833307275856167\n",
      "Epoch 2/300, Average Loss: 0.3846429276122848\n",
      "Epoch 3/300, Average Loss: 0.2758154947754927\n",
      "Epoch 4/300, Average Loss: 0.204596070965711\n",
      "Epoch 5/300, Average Loss: 0.1655296759577315\n",
      "Epoch 6/300, Average Loss: 0.12425823178525892\n",
      "Epoch 7/300, Average Loss: 0.08959570004099046\n",
      "Epoch 8/300, Average Loss: 0.08684153344114162\n",
      "Epoch 9/300, Average Loss: 0.07255408953454932\n",
      "Epoch 10/300, Average Loss: 0.08216616087243236\n",
      "Epoch 11/300, Average Loss: 0.10317031287925896\n",
      "Epoch 12/300, Average Loss: 0.09860597666200632\n",
      "Epoch 13/300, Average Loss: 0.0863059803505404\n",
      "Epoch 14/300, Average Loss: 0.08041107329572131\n",
      "Epoch 15/300, Average Loss: 0.08080672966224768\n",
      "Epoch 16/300, Average Loss: 0.09298459505751994\n",
      "Epoch 17/300, Average Loss: 0.09791198823721782\n",
      "Epoch 18/300, Average Loss: 0.11756215525240385\n",
      "Epoch 19/300, Average Loss: 0.14146375597841424\n",
      "Epoch 20/300, Average Loss: 0.19668652730985037\n",
      "Epoch 21/300, Average Loss: 0.18642722576407778\n",
      "Epoch 22/300, Average Loss: 0.16866357050538072\n",
      "Epoch 23/300, Average Loss: 0.15616677855404082\n",
      "Epoch 24/300, Average Loss: 0.1497331311250621\n",
      "Epoch 25/300, Average Loss: 0.22263473984937807\n",
      "Epoch 26/300, Average Loss: 0.2915541231644577\n",
      "Epoch 27/300, Average Loss: 0.34556976428357683\n",
      "Epoch 28/300, Average Loss: 0.2943988457682519\n",
      "Epoch 29/300, Average Loss: 0.2563460602822604\n",
      "Epoch 30/300, Average Loss: 0.23198787270485743\n",
      "Epoch 31/300, Average Loss: 0.21648745336411582\n",
      "Epoch 32/300, Average Loss: 0.20240237650937226\n",
      "Epoch 33/300, Average Loss: 0.19022860717481282\n",
      "Epoch 34/300, Average Loss: 0.17941976690889946\n",
      "Epoch 35/300, Average Loss: 0.167398390116882\n",
      "Epoch 36/300, Average Loss: 0.15899291695558865\n",
      "Epoch 37/300, Average Loss: 0.15070474375345588\n",
      "Epoch 38/300, Average Loss: 0.14506108649527202\n",
      "Epoch 39/300, Average Loss: 0.14110505874972035\n",
      "Epoch 40/300, Average Loss: 0.138351074302722\n",
      "Epoch 41/300, Average Loss: 0.13659955417288758\n",
      "Epoch 42/300, Average Loss: 0.13548263363925722\n",
      "Epoch 43/300, Average Loss: 0.13433758321506134\n",
      "Epoch 44/300, Average Loss: 0.13302541739562243\n",
      "Epoch 45/300, Average Loss: 0.13180062759514274\n",
      "Epoch 46/300, Average Loss: 0.13066075721986198\n",
      "Epoch 47/300, Average Loss: 0.12937647563838298\n",
      "Epoch 48/300, Average Loss: 0.12789139859057158\n",
      "Epoch 49/300, Average Loss: 0.1259834760845337\n",
      "Epoch 50/300, Average Loss: 0.12385904776610827\n",
      "Epoch 51/300, Average Loss: 0.12124398079678027\n",
      "Epoch 52/300, Average Loss: 0.11895821224531437\n",
      "Epoch 53/300, Average Loss: 0.11700199932626236\n",
      "Epoch 54/300, Average Loss: 0.11526678744829674\n",
      "Epoch 55/300, Average Loss: 0.1135915844419525\n",
      "Epoch 56/300, Average Loss: 0.1123319025625106\n",
      "Epoch 57/300, Average Loss: 0.1116038941607361\n",
      "Epoch 58/300, Average Loss: 0.11084416398516969\n",
      "Epoch 59/300, Average Loss: 0.11025849787437789\n",
      "Epoch 60/300, Average Loss: 0.1096392813864798\n",
      "Epoch 61/300, Average Loss: 0.10905107641910608\n",
      "Epoch 62/300, Average Loss: 0.11239343213116493\n",
      "Epoch 63/300, Average Loss: 0.11092028853417935\n",
      "Epoch 64/300, Average Loss: 0.11143156844555732\n",
      "Epoch 65/300, Average Loss: 0.10936803531749648\n",
      "Epoch 66/300, Average Loss: 0.1057419858029415\n",
      "Epoch 67/300, Average Loss: 0.10443108473696441\n",
      "Epoch 68/300, Average Loss: 0.10422391508184885\n",
      "Epoch 69/300, Average Loss: 0.10363615725885658\n",
      "Epoch 70/300, Average Loss: 0.1033644090019838\n",
      "Epoch 71/300, Average Loss: 0.10319954670281943\n",
      "Epoch 72/300, Average Loss: 0.10302553791656095\n",
      "Epoch 73/300, Average Loss: 0.10277395476791443\n",
      "Epoch 74/300, Average Loss: 0.10221334240404265\n",
      "Epoch 75/300, Average Loss: 0.10142688395656865\n",
      "Epoch 76/300, Average Loss: 0.10065339748476172\n",
      "Epoch 77/300, Average Loss: 0.0999994333732483\n",
      "Epoch 78/300, Average Loss: 0.09948354545666571\n",
      "Epoch 79/300, Average Loss: 0.09874176619816717\n",
      "Epoch 80/300, Average Loss: 0.09777092712438754\n",
      "Epoch 81/300, Average Loss: 0.0969178654145724\n",
      "Epoch 82/300, Average Loss: 0.09614959906269285\n",
      "Epoch 83/300, Average Loss: 0.09534293486866877\n",
      "Epoch 84/300, Average Loss: 0.09451196076489991\n",
      "Epoch 85/300, Average Loss: 0.09367642953763189\n",
      "Epoch 86/300, Average Loss: 0.09280818459227297\n",
      "Epoch 87/300, Average Loss: 0.09192448800367367\n",
      "Epoch 88/300, Average Loss: 0.09104964424154156\n",
      "Epoch 89/300, Average Loss: 0.09016879986446753\n",
      "Epoch 90/300, Average Loss: 0.08926654507082557\n",
      "Epoch 91/300, Average Loss: 0.08846944357852418\n",
      "Epoch 92/300, Average Loss: 0.08776707136741731\n",
      "Epoch 93/300, Average Loss: 0.08711221018862517\n",
      "Epoch 94/300, Average Loss: 0.08658461913655127\n",
      "Epoch 95/300, Average Loss: 0.08609866986064839\n",
      "Epoch 96/300, Average Loss: 0.08555345652499799\n",
      "Epoch 97/300, Average Loss: 0.08493239225451041\n",
      "Epoch 98/300, Average Loss: 0.08446592960830292\n",
      "Epoch 99/300, Average Loss: 0.08404635189960247\n",
      "Epoch 100/300, Average Loss: 0.08374284731631228\n",
      "Epoch 101/300, Average Loss: 0.08354809400272349\n",
      "Epoch 102/300, Average Loss: 0.08336146298740124\n",
      "Epoch 103/300, Average Loss: 0.08424586105434975\n",
      "Epoch 104/300, Average Loss: 0.08401382796330228\n",
      "Epoch 105/300, Average Loss: 0.08301719585259791\n",
      "Epoch 106/300, Average Loss: 0.08306656872435739\n",
      "Epoch 107/300, Average Loss: 0.08229007872690297\n",
      "Epoch 108/300, Average Loss: 0.08232841785248608\n",
      "Epoch 109/300, Average Loss: 0.0838051553557063\n",
      "Epoch 110/300, Average Loss: 0.0831205713896866\n",
      "Epoch 111/300, Average Loss: 0.0816667560278879\n",
      "Epoch 112/300, Average Loss: 0.08182057943181743\n",
      "Epoch 113/300, Average Loss: 0.0810935594606633\n",
      "Epoch 114/300, Average Loss: 0.08084952425661543\n",
      "Epoch 115/300, Average Loss: 0.08103998352385525\n",
      "Epoch 116/300, Average Loss: 0.08234711486606357\n",
      "Epoch 117/300, Average Loss: 0.0860014712485013\n",
      "Epoch 118/300, Average Loss: 0.08703926072255416\n",
      "Epoch 119/300, Average Loss: 0.08405114120076838\n",
      "Epoch 120/300, Average Loss: 0.08500037390318929\n",
      "Epoch 121/300, Average Loss: 0.08837480745953497\n",
      "Epoch 122/300, Average Loss: 0.09022484266238019\n",
      "Epoch 123/300, Average Loss: 0.09665739461825772\n",
      "Epoch 124/300, Average Loss: 0.10600648298880751\n",
      "Epoch 125/300, Average Loss: 0.10572817164729294\n",
      "Epoch 126/300, Average Loss: 0.11279817702968623\n",
      "Epoch 127/300, Average Loss: 0.1075931006744956\n",
      "Epoch 128/300, Average Loss: 0.09606827935647592\n",
      "Epoch 129/300, Average Loss: 0.08974830929883919\n",
      "Epoch 130/300, Average Loss: 0.0868028603889838\n",
      "Epoch 131/300, Average Loss: 0.08459008891039932\n",
      "Epoch 132/300, Average Loss: 0.08355808012980645\n",
      "Epoch 133/300, Average Loss: 0.08307401680576834\n",
      "Epoch 134/300, Average Loss: 0.08286778045757527\n",
      "Epoch 135/300, Average Loss: 0.08333153122354983\n",
      "Epoch 136/300, Average Loss: 0.08509778765484262\n",
      "Epoch 137/300, Average Loss: 0.08867719870076243\n",
      "Epoch 138/300, Average Loss: 0.099320803041935\n",
      "Epoch 139/300, Average Loss: 0.1297972953578867\n",
      "Epoch 140/300, Average Loss: 0.16983995871405852\n",
      "Epoch 141/300, Average Loss: 0.26118084286107146\n",
      "Epoch 142/300, Average Loss: 0.28350098699896187\n",
      "Epoch 143/300, Average Loss: 0.2776069342171529\n",
      "Epoch 144/300, Average Loss: 0.2448782511674037\n",
      "Epoch 145/300, Average Loss: 0.22655320610881807\n",
      "Epoch 146/300, Average Loss: 0.1922253037941423\n",
      "Epoch 147/300, Average Loss: 0.15370154405887265\n",
      "Epoch 148/300, Average Loss: 0.1322521475303026\n",
      "Epoch 149/300, Average Loss: 0.12890740819606525\n",
      "Epoch 150/300, Average Loss: 0.12825494499955886\n",
      "Epoch 151/300, Average Loss: 0.12251682962745311\n",
      "Epoch 152/300, Average Loss: 0.11852776389466627\n",
      "Epoch 153/300, Average Loss: 0.11819474888213796\n",
      "Epoch 154/300, Average Loss: 0.11807670831798463\n",
      "Epoch 155/300, Average Loss: 0.11727333903542733\n",
      "Epoch 156/300, Average Loss: 0.1162413706550299\n",
      "Epoch 157/300, Average Loss: 0.11505199063151991\n",
      "Epoch 158/300, Average Loss: 0.1136868888169535\n",
      "Epoch 159/300, Average Loss: 0.11221240343277065\n",
      "Epoch 160/300, Average Loss: 0.11083680668325664\n",
      "Epoch 161/300, Average Loss: 0.10938588201509267\n",
      "Epoch 162/300, Average Loss: 0.10823381858921363\n",
      "Epoch 163/300, Average Loss: 0.10720867279179383\n",
      "Epoch 164/300, Average Loss: 0.1063238188436243\n",
      "Epoch 165/300, Average Loss: 0.10528766631155058\n",
      "Epoch 166/300, Average Loss: 0.10418551356646244\n",
      "Epoch 167/300, Average Loss: 0.10296750269823478\n",
      "Epoch 168/300, Average Loss: 0.1016261335820696\n",
      "Epoch 169/300, Average Loss: 0.1003713096926738\n",
      "Epoch 170/300, Average Loss: 0.09931358188540931\n",
      "Epoch 171/300, Average Loss: 0.09838020154358319\n",
      "Epoch 172/300, Average Loss: 0.09733364416404991\n",
      "Epoch 173/300, Average Loss: 0.09622474066592848\n",
      "Epoch 174/300, Average Loss: 0.09517159469574844\n",
      "Epoch 175/300, Average Loss: 0.0941137207531986\n",
      "Epoch 176/300, Average Loss: 0.09321953148055884\n",
      "Epoch 177/300, Average Loss: 0.09245423185069675\n",
      "Epoch 178/300, Average Loss: 0.09168467850851617\n",
      "Epoch 179/300, Average Loss: 0.09094253549828696\n",
      "Epoch 180/300, Average Loss: 0.09008793856084654\n",
      "Epoch 181/300, Average Loss: 0.08907313064255269\n",
      "Epoch 182/300, Average Loss: 0.08793032609575502\n",
      "Epoch 183/300, Average Loss: 0.08694479742143604\n",
      "Epoch 184/300, Average Loss: 0.08558422532858531\n",
      "Epoch 185/300, Average Loss: 0.08440710246445508\n",
      "Epoch 186/300, Average Loss: 0.08327519110423778\n",
      "Epoch 187/300, Average Loss: 0.08201585194908133\n",
      "Epoch 188/300, Average Loss: 0.08098451494393019\n",
      "Epoch 189/300, Average Loss: 0.0800503584983285\n",
      "Epoch 190/300, Average Loss: 0.07916897666133928\n",
      "Epoch 191/300, Average Loss: 0.07839908791755779\n",
      "Epoch 192/300, Average Loss: 0.07804950102367025\n",
      "Epoch 193/300, Average Loss: 0.07822917923214483\n",
      "Epoch 194/300, Average Loss: 0.07838598559767494\n",
      "Epoch 195/300, Average Loss: 0.07817666057559954\n",
      "Epoch 196/300, Average Loss: 0.07920758101598606\n",
      "Epoch 197/300, Average Loss: 0.08045264120228152\n",
      "Epoch 198/300, Average Loss: 0.08054351277883698\n",
      "Epoch 199/300, Average Loss: 0.08108715668684292\n",
      "Epoch 200/300, Average Loss: 0.07938201280139519\n",
      "Epoch 201/300, Average Loss: 0.08015853465690803\n",
      "Epoch 202/300, Average Loss: 0.083427904119279\n",
      "Epoch 203/300, Average Loss: 0.08279343278242707\n",
      "Epoch 204/300, Average Loss: 0.08881768112198615\n",
      "Epoch 205/300, Average Loss: 0.09056955873925811\n",
      "Epoch 206/300, Average Loss: 0.09067802665112437\n",
      "Epoch 207/300, Average Loss: 0.0861501430719386\n",
      "Epoch 208/300, Average Loss: 0.08287793482607293\n",
      "Epoch 209/300, Average Loss: 0.07828433963832801\n",
      "Epoch 210/300, Average Loss: 0.07456693580476662\n",
      "Epoch 211/300, Average Loss: 0.0728554318989275\n",
      "Epoch 212/300, Average Loss: 0.0729748562873202\n",
      "Epoch 213/300, Average Loss: 0.07382027518142516\n",
      "Epoch 214/300, Average Loss: 0.07306563718970702\n",
      "Epoch 215/300, Average Loss: 0.07176925884875598\n",
      "Epoch 216/300, Average Loss: 0.07090914861026898\n",
      "Epoch 217/300, Average Loss: 0.07023065444769096\n",
      "Epoch 218/300, Average Loss: 0.06912807977786345\n",
      "Epoch 219/300, Average Loss: 0.06734558127358103\n",
      "Epoch 220/300, Average Loss: 0.06614730953746611\n",
      "Epoch 221/300, Average Loss: 0.06511874394208053\n",
      "Epoch 222/300, Average Loss: 0.06406006599917943\n",
      "Epoch 223/300, Average Loss: 0.06303730228111526\n",
      "Epoch 224/300, Average Loss: 0.06241132354430644\n",
      "Epoch 225/300, Average Loss: 0.06250348701541167\n",
      "Epoch 226/300, Average Loss: 0.06557798102911588\n",
      "Epoch 227/300, Average Loss: 0.07177233177860129\n",
      "Epoch 228/300, Average Loss: 0.07058542697670278\n",
      "Epoch 229/300, Average Loss: 0.06848666872803243\n",
      "Epoch 230/300, Average Loss: 0.06742838281330962\n",
      "Epoch 231/300, Average Loss: 0.06523068722463642\n",
      "Epoch 232/300, Average Loss: 0.06311291713734954\n",
      "Epoch 233/300, Average Loss: 0.06118089924838736\n",
      "Epoch 234/300, Average Loss: 0.05924636908690234\n",
      "Epoch 235/300, Average Loss: 0.05735942263772774\n",
      "Epoch 236/300, Average Loss: 0.05554201787068169\n",
      "Epoch 237/300, Average Loss: 0.05413489539253662\n",
      "Epoch 238/300, Average Loss: 0.05286903535839353\n",
      "Epoch 239/300, Average Loss: 0.051844491648818956\n",
      "Epoch 240/300, Average Loss: 0.05086636609398119\n",
      "Epoch 241/300, Average Loss: 0.05017109500341922\n",
      "Epoch 242/300, Average Loss: 0.050254489136532\n",
      "Epoch 243/300, Average Loss: 0.049764872448751885\n",
      "Epoch 244/300, Average Loss: 0.050490481894305314\n",
      "Epoch 245/300, Average Loss: 0.050807319516827563\n",
      "Epoch 246/300, Average Loss: 0.04947749564162733\n",
      "Epoch 247/300, Average Loss: 0.04613917267997416\n",
      "Epoch 248/300, Average Loss: 0.04516125849557743\n",
      "Epoch 249/300, Average Loss: 0.043760667213727705\n",
      "Epoch 250/300, Average Loss: 0.043199643591391106\n",
      "Epoch 251/300, Average Loss: 0.0428379533713328\n",
      "Epoch 252/300, Average Loss: 0.04252405271702432\n",
      "Epoch 253/300, Average Loss: 0.042354168846783335\n",
      "Epoch 254/300, Average Loss: 0.04202942294916569\n",
      "Epoch 255/300, Average Loss: 0.04161504201062219\n",
      "Epoch 256/300, Average Loss: 0.040768750132439566\n",
      "Epoch 257/300, Average Loss: 0.04001137044078433\n",
      "Epoch 258/300, Average Loss: 0.03944494692526058\n",
      "Epoch 259/300, Average Loss: 0.03878836179605616\n",
      "Epoch 260/300, Average Loss: 0.038209122932044995\n",
      "Epoch 261/300, Average Loss: 0.03789965151293721\n",
      "Epoch 262/300, Average Loss: 0.03798662128356851\n",
      "Epoch 263/300, Average Loss: 0.038224730245954484\n",
      "Epoch 264/300, Average Loss: 0.03877327379438332\n",
      "Epoch 265/300, Average Loss: 0.03853922992548321\n",
      "Epoch 266/300, Average Loss: 0.0381682224793974\n",
      "Epoch 267/300, Average Loss: 0.03815930073474599\n",
      "Epoch 268/300, Average Loss: 0.03792270673870209\n",
      "Epoch 269/300, Average Loss: 0.03749631977942752\n",
      "Epoch 270/300, Average Loss: 0.03697574554749028\n",
      "Epoch 271/300, Average Loss: 0.03684873008342378\n",
      "Epoch 272/300, Average Loss: 0.03623198426344963\n",
      "Epoch 273/300, Average Loss: 0.03550771489038629\n",
      "Epoch 274/300, Average Loss: 0.03570669761566745\n",
      "Epoch 275/300, Average Loss: 0.03643001975891568\n",
      "Epoch 276/300, Average Loss: 0.03734353348807009\n",
      "Epoch 277/300, Average Loss: 0.03764988603094657\n",
      "Epoch 278/300, Average Loss: 0.038168067432759877\n",
      "Epoch 279/300, Average Loss: 0.037835799851798246\n",
      "Epoch 280/300, Average Loss: 0.036603820405352434\n",
      "Epoch 281/300, Average Loss: 0.0367850327744161\n",
      "Epoch 282/300, Average Loss: 0.03774070750094973\n",
      "Epoch 283/300, Average Loss: 0.03754266006346476\n",
      "Epoch 284/300, Average Loss: 0.037097637045123706\n",
      "Epoch 285/300, Average Loss: 0.040654018867924746\n",
      "Epoch 286/300, Average Loss: 0.04047650654341998\n",
      "Epoch 287/300, Average Loss: 0.04052058350070212\n",
      "Epoch 288/300, Average Loss: 0.03921061686618001\n",
      "Epoch 289/300, Average Loss: 0.03882844193412292\n",
      "Epoch 290/300, Average Loss: 0.039344563777472995\n",
      "Epoch 291/300, Average Loss: 0.04138135728572715\n",
      "Epoch 292/300, Average Loss: 0.041341202419476475\n",
      "Epoch 293/300, Average Loss: 0.03846042083074178\n",
      "Epoch 294/300, Average Loss: 0.03602674357541372\n",
      "Epoch 295/300, Average Loss: 0.03418329403630872\n",
      "Epoch 296/300, Average Loss: 0.032772228634593156\n",
      "Epoch 297/300, Average Loss: 0.03236100279258851\n",
      "Epoch 298/300, Average Loss: 0.03197794071172463\n",
      "Epoch 299/300, Average Loss: 0.03179542280947806\n"
     ]
    }
   ],
   "source": [
    "#training the module\n",
    "r = 5\n",
    "m = 3\n",
    "d = 20\n",
    "k = 5\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2*r - 1\n",
    "n_iter = 300\n",
    "alpha = 0.001\n",
    "num_of_samples = 250\n",
    "num_train_batches = 10\n",
    "num_test_batches = 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, num_of_samples, num_train_batches,num_test_batches)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1,feed_forward1,attention2, feed_forward2, un_embed_pos, softmax]\n",
    "nueralnetsort = NeuralNetwork(layers)\n",
    "\n",
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "x_t = data['x_test'][0]\n",
    "y_t = data['y_test'][0]\n",
    "\n",
    "per, y_hat = sorting(nueralnetsort, x_t, y_t,m, r)\n",
    "print(f'prosent av antall riktige sorteringer før trening er {per*100}%')\n",
    "for i in range(y_hat.shape[0]):\n",
    "    print(f'xt = {x_t[i]} yt = {y_t[i]} and z = {y_hat[i]}')\n",
    " \n",
    "arr = algorithm_4_sort(x, y, n_iter, alpha, m, nueralnetsort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [2. 2. 2. 2. 2.] and z = [2. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [1. 2. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [1. 2. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 2. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [1. 1. 1. 1. 2.] and z = [1. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 2. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [1. 2. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 1. 0. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [1. 2. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 2. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 2. 2.]\n",
      "yt = [0. 2. 2. 2. 2.] and z = [0. 2. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 1. 0. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 1. 0. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [1. 2. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 1. 0. 2.]\n",
      "yt = [0. 0. 0. 0. 2.] and z = [0. 0. 0. 0. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 0.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 2. 2. 2. 2.] and z = [0. 2. 2. 2. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 2. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [1. 2. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [1. 2. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 2. 2. 2. 2.] and z = [0. 2. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 2.] and z = [0. 0. 0. 0. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 0. 2.] and z = [0. 0. 0. 0. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 2. 2.]\n",
      "yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [2. 2. 2. 2. 2.] and z = [2. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 2. 2.] and z = [0. 0. 0. 2. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [0. 1. 2. 2. 2.]\n",
      "yt = [1. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 2. 2. 2.] and z = [0. 0. 2. 2. 2.]\n",
      "yt = [0. 0. 0. 0. 2.] and z = [0. 0. 0. 0. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [1. 1. 2. 2. 2.] and z = [1. 1. 2. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 1. 2.] and z = [0. 1. 1. 1. 2.]\n",
      "yt = [0. 0. 1. 1. 2.] and z = [0. 0. 1. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 0. 1. 2. 2.] and z = [0. 0. 1. 2. 2.]\n",
      "yt = [0. 1. 2. 2. 2.] and z = [1. 2. 2. 2. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [0. 0. 0. 1. 2.] and z = [0. 0. 0. 1. 2.]\n",
      "yt = [0. 1. 1. 2. 2.] and z = [0. 1. 1. 2. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "yt = [1. 1. 1. 2. 2.] and z = [1. 1. 1. 2. 2.]\n",
      "prosent av antall riktige sorteringer etter trening er 87.2%\n"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(len(arr)),arr)\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Logaritmen av losset')\n",
    "plt.title('Plot av feil for sortering')\n",
    "plt.show()\n",
    "per, z_hat = sorting(nueralnetsort, x_t, y_t,m, r)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "for i in range(250):\n",
    "    print(f'yt = {y_t[i]} and z = {z_hat[i]}')\n",
    "print(f'prosent av antall riktige sorteringer etter trening er {per*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapunkter = 250\n",
    "batches = 20\n",
    "\n",
    "d = 30\n",
    "k = 20\n",
    "p = 40 \n",
    "L = 3\n",
    "m = 10\n",
    "r = 2\n",
    "n_max = 9\n",
    "n_iter = 150\n",
    "alpha = 0.0001\n",
    "\n",
    "data_add = get_train_test_addition(r+1, datapunkter, batches)\n",
    "\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "feed_forward3 = FeedForward(d,p)\n",
    "attention3 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1,feed_forward1, attention2,feed_forward2, attention3,feed_forward3, un_embed_pos, softmax]\n",
    "nueralnetadd = NeuralNetwork(layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prosent av antall riktige addisjoner før trening er 0.0%\n",
      "yt = [0 1 3 3] and z = [9 6 8]\n",
      "yt = [1 1 0 2] and z = [9 6 8]\n",
      "yt = [0 8 1 9] and z = [6 9 6]\n",
      "yt = [0 9 8 3] and z = [4 9 6]\n",
      "yt = [1 1 0 8] and z = [4 9 6]\n",
      "yt = [0 1 5 0] and z = [4 9 6]\n",
      "yt = [1 0 8 8] and z = [9 6 8]\n",
      "yt = [0 4 2 0] and z = [4 9 6]\n",
      "yt = [0 9 9 2] and z = [6 9 6]\n",
      "yt = [1 2 1 3] and z = [9 6 8]\n",
      "yt = [1 4 1 9] and z = [4 9 6]\n",
      "yt = [0 8 0 2] and z = [9 6 8]\n",
      "yt = [0 8 5 9] and z = [6 9 6]\n",
      "yt = [1 6 5 7] and z = [6 9 6]\n",
      "yt = [0 4 6 1] and z = [6 9 6]\n",
      "yt = [1 1 5 2] and z = [9 6 8]\n",
      "yt = [0 9 0 9] and z = [6 9 6]\n",
      "yt = [0 6 6 1] and z = [4 9 6]\n",
      "yt = [0 8 8 3] and z = [9 6 8]\n",
      "yt = [0 6 8 1] and z = [9 6 8]\n",
      "yt = [0 7 5 0] and z = [2 9 6]\n",
      "yt = [0 9 4 6] and z = [4 9 6]\n",
      "yt = [0 6 4 7] and z = [4 9 6]\n",
      "yt = [1 4 6 0] and z = [9 6 8]\n",
      "yt = [0 8 1 5] and z = [4 9 6]\n",
      "yt = [1 6 6 7] and z = [4 9 6]\n",
      "yt = [0 8 4 9] and z = [6 9 6]\n",
      "yt = [0 4 6 0] and z = [9 6 8]\n",
      "yt = [1 2 7 3] and z = [4 9 6]\n",
      "yt = [1 7 0 0] and z = [9 6 8]\n",
      "yt = [1 2 1 5] and z = [6 9 6]\n",
      "yt = [0 7 5 4] and z = [6 9 6]\n",
      "yt = [0 7 3 2] and z = [6 9 6]\n",
      "yt = [1 1 7 3] and z = [9 6 8]\n",
      "yt = [1 7 4 7] and z = [9 6 8]\n",
      "yt = [1 0 0 9] and z = [4 9 1]\n",
      "yt = [1 3 9 2] and z = [6 9 6]\n",
      "yt = [1 3 7 7] and z = [6 9 6]\n",
      "yt = [1 1 2 5] and z = [2 9 6]\n",
      "yt = [0 8 5 2] and z = [8 8 8]\n",
      "yt = [0 7 4 0] and z = [4 9 6]\n",
      "yt = [1 1 1 9] and z = [9 6 8]\n",
      "yt = [1 0 8 7] and z = [4 9 6]\n",
      "yt = [0 7 5 2] and z = [9 6 8]\n",
      "yt = [1 1 7 4] and z = [6 9 6]\n",
      "yt = [0 7 2 4] and z = [9 6 8]\n",
      "yt = [1 1 3 8] and z = [4 9 6]\n",
      "yt = [1 2 2 5] and z = [9 6 8]\n",
      "yt = [0 7 3 7] and z = [9 6 8]\n",
      "yt = [0 7 2 7] and z = [2 9 6]\n",
      "yt = [1 3 7 3] and z = [4 9 6]\n",
      "yt = [0 9 2 8] and z = [6 9 6]\n",
      "yt = [1 0 0 4] and z = [4 9 6]\n",
      "yt = [1 0 1 8] and z = [8 8 8]\n",
      "yt = [0 2 4 8] and z = [8 8 8]\n",
      "yt = [0 9 6 8] and z = [9 6 8]\n",
      "yt = [0 7 3 4] and z = [9 6 1]\n",
      "yt = [1 3 1 1] and z = [2 9 6]\n",
      "yt = [0 9 3 7] and z = [4 9 6]\n",
      "yt = [1 0 1 3] and z = [4 9 6]\n",
      "yt = [1 0 9 2] and z = [4 9 6]\n",
      "yt = [0 4 7 6] and z = [2 9 6]\n",
      "yt = [1 0 6 7] and z = [4 9 6]\n",
      "yt = [0 7 6 0] and z = [6 9 6]\n",
      "yt = [0 1 4 0] and z = [9 6 8]\n",
      "yt = [1 3 5 7] and z = [6 9 6]\n",
      "yt = [0 5 5 3] and z = [9 6 8]\n",
      "yt = [0 9 4 1] and z = [9 6 8]\n",
      "yt = [1 0 3 3] and z = [4 9 6]\n",
      "yt = [1 9 7 8] and z = [9 6 1]\n",
      "yt = [0 2 5 1] and z = [6 9 6]\n",
      "yt = [1 1 0 8] and z = [8 8 8]\n",
      "yt = [1 1 4 7] and z = [4 9 6]\n",
      "yt = [0 9 2 4] and z = [9 6 8]\n",
      "yt = [0 5 1 4] and z = [9 6 8]\n",
      "yt = [0 6 7 5] and z = [9 6 8]\n",
      "yt = [1 0 3 1] and z = [4 9 6]\n",
      "yt = [0 5 2 4] and z = [4 9 6]\n",
      "yt = [0 7 8 7] and z = [9 6 8]\n",
      "yt = [1 6 6 2] and z = [6 9 6]\n",
      "yt = [1 3 0 6] and z = [9 6 8]\n",
      "yt = [0 2 7 4] and z = [2 9 6]\n",
      "yt = [1 0 5 9] and z = [9 6 1]\n",
      "yt = [1 3 4 0] and z = [6 9 6]\n",
      "yt = [0 0 6 9] and z = [9 6 8]\n",
      "yt = [1 5 5 7] and z = [9 6 8]\n",
      "yt = [1 2 5 7] and z = [4 9 6]\n",
      "yt = [1 1 7 1] and z = [6 9 6]\n",
      "yt = [1 3 6 9] and z = [4 9 6]\n",
      "yt = [1 3 7 9] and z = [4 9 6]\n",
      "yt = [1 2 2 7] and z = [6 9 6]\n",
      "yt = [0 6 3 4] and z = [9 6 8]\n",
      "yt = [0 8 2 9] and z = [6 9 1]\n",
      "yt = [0 7 1 2] and z = [4 9 6]\n",
      "yt = [1 0 0 3] and z = [4 9 6]\n",
      "yt = [0 8 5 3] and z = [9 6 8]\n",
      "yt = [0 6 5 6] and z = [4 9 6]\n",
      "yt = [0 7 1 4] and z = [6 9 6]\n",
      "yt = [1 0 4 7] and z = [4 9 6]\n",
      "yt = [0 2 2 3] and z = [6 9 6]\n",
      "yt = [1 4 9 5] and z = [4 9 1]\n",
      "yt = [0 8 9 2] and z = [6 9 6]\n",
      "yt = [0 7 4 9] and z = [2 9 6]\n",
      "yt = [0 3 9 4] and z = [4 9 6]\n",
      "yt = [0 9 7 0] and z = [4 9 6]\n",
      "yt = [0 7 8 0] and z = [9 6 8]\n",
      "yt = [1 1 8 7] and z = [9 6 8]\n",
      "yt = [0 9 6 5] and z = [4 9 6]\n",
      "yt = [1 6 9 6] and z = [4 9 6]\n",
      "yt = [1 2 9 3] and z = [4 9 6]\n",
      "yt = [0 9 4 7] and z = [4 9 6]\n",
      "yt = [0 9 2 8] and z = [9 6 8]\n",
      "yt = [1 4 1 2] and z = [4 9 6]\n",
      "yt = [0 6 2 7] and z = [4 9 6]\n",
      "yt = [0 5 3 8] and z = [4 9 6]\n",
      "yt = [1 5 4 3] and z = [6 9 6]\n",
      "yt = [1 0 0 5] and z = [2 9 6]\n",
      "yt = [1 3 7 9] and z = [4 9 6]\n",
      "yt = [0 4 9 7] and z = [6 9 6]\n",
      "yt = [0 6 4 4] and z = [4 9 6]\n",
      "yt = [0 5 2 9] and z = [4 9 6]\n",
      "yt = [0 8 5 2] and z = [4 9 6]\n",
      "yt = [0 3 9 1] and z = [6 9 6]\n",
      "yt = [0 3 8 7] and z = [2 9 6]\n",
      "yt = [0 8 5 3] and z = [4 9 6]\n",
      "yt = [0 6 4 3] and z = [4 9 6]\n",
      "yt = [0 3 5 0] and z = [9 6 8]\n",
      "yt = [1 0 0 8] and z = [9 6 1]\n",
      "yt = [0 8 9 9] and z = [9 6 8]\n",
      "yt = [1 5 2 4] and z = [4 9 6]\n",
      "yt = [0 4 8 1] and z = [4 9 6]\n",
      "yt = [0 5 7 9] and z = [4 9 6]\n",
      "yt = [1 7 1 7] and z = [4 9 6]\n",
      "yt = [0 3 1 9] and z = [4 9 6]\n",
      "yt = [1 3 6 8] and z = [6 9 6]\n",
      "yt = [1 3 3 7] and z = [9 6 8]\n",
      "yt = [1 6 4 2] and z = [4 9 6]\n",
      "yt = [0 9 1 3] and z = [6 9 6]\n",
      "yt = [0 7 7 0] and z = [4 9 6]\n",
      "yt = [0 1 1 4] and z = [6 9 6]\n",
      "yt = [1 2 0 8] and z = [4 9 6]\n",
      "yt = [1 1 9 8] and z = [9 6 8]\n",
      "yt = [0 9 0 0] and z = [2 9 6]\n",
      "yt = [1 8 2 9] and z = [9 6 1]\n",
      "yt = [0 9 8 8] and z = [6 9 6]\n",
      "yt = [1 0 3 7] and z = [9 6 8]\n",
      "yt = [0 6 4 6] and z = [4 9 6]\n",
      "yt = [1 4 1 5] and z = [9 6 8]\n",
      "yt = [0 4 4 8] and z = [6 9 6]\n",
      "yt = [0 8 6 9] and z = [4 9 6]\n",
      "yt = [0 4 4 5] and z = [9 6 8]\n",
      "yt = [0 9 2 7] and z = [9 6 8]\n",
      "yt = [1 4 3 3] and z = [9 6 8]\n",
      "yt = [1 3 4 3] and z = [4 9 6]\n",
      "yt = [1 0 6 1] and z = [4 9 6]\n",
      "yt = [1 7 8 3] and z = [9 6 8]\n",
      "yt = [0 9 5 8] and z = [4 9 6]\n",
      "yt = [1 1 3 0] and z = [9 6 8]\n",
      "yt = [0 2 6 6] and z = [9 6 8]\n",
      "yt = [1 3 8 1] and z = [9 6 8]\n",
      "yt = [1 5 9 0] and z = [9 6 8]\n",
      "yt = [0 8 5 5] and z = [6 9 6]\n",
      "yt = [1 6 9 4] and z = [4 9 6]\n",
      "yt = [0 4 9 5] and z = [6 9 6]\n",
      "yt = [0 8 6 8] and z = [9 6 8]\n",
      "yt = [0 4 1 4] and z = [4 9 6]\n",
      "yt = [0 7 8 6] and z = [2 9 6]\n",
      "yt = [1 6 1 3] and z = [9 6 8]\n",
      "yt = [0 4 5 2] and z = [6 9 1]\n",
      "yt = [0 4 5 0] and z = [8 8 8]\n",
      "yt = [1 4 3 8] and z = [4 9 6]\n",
      "yt = [1 0 7 2] and z = [4 9 6]\n",
      "yt = [0 9 1 5] and z = [6 9 6]\n",
      "yt = [1 4 3 3] and z = [4 9 6]\n",
      "yt = [0 3 6 3] and z = [4 9 6]\n",
      "yt = [1 2 1 4] and z = [6 9 6]\n",
      "yt = [0 9 5 5] and z = [9 6 8]\n",
      "yt = [0 5 9 2] and z = [4 9 6]\n",
      "yt = [0 6 9 9] and z = [9 6 8]\n",
      "yt = [0 9 1 2] and z = [6 9 6]\n",
      "yt = [1 0 0 5] and z = [9 6 8]\n",
      "yt = [0 3 8 1] and z = [2 9 6]\n",
      "yt = [0 5 6 9] and z = [9 6 8]\n",
      "yt = [0 2 0 3] and z = [5 9 6]\n",
      "yt = [0 6 1 1] and z = [2 9 6]\n",
      "yt = [0 9 3 7] and z = [9 6 8]\n",
      "yt = [1 0 8 2] and z = [4 9 6]\n",
      "yt = [1 2 4 0] and z = [4 9 6]\n",
      "yt = [0 8 6 9] and z = [9 6 8]\n",
      "yt = [1 5 0 4] and z = [4 9 6]\n",
      "yt = [1 6 5 0] and z = [6 9 6]\n",
      "yt = [0 9 3 0] and z = [9 6 8]\n",
      "yt = [0 1 7 8] and z = [2 9 6]\n",
      "yt = [1 0 3 1] and z = [9 6 8]\n",
      "yt = [0 7 8 1] and z = [9 6 8]\n",
      "yt = [0 4 0 0] and z = [6 9 6]\n",
      "yt = [0 4 4 1] and z = [9 6 8]\n",
      "yt = [0 9 8 8] and z = [2 9 6]\n",
      "yt = [0 8 5 1] and z = [2 9 6]\n",
      "yt = [0 6 0 0] and z = [9 6 8]\n",
      "yt = [0 9 4 3] and z = [4 9 6]\n",
      "yt = [1 4 3 5] and z = [4 9 6]\n",
      "yt = [1 3 9 0] and z = [6 9 1]\n",
      "yt = [0 1 2 6] and z = [5 9 6]\n",
      "yt = [0 4 8 3] and z = [2 9 6]\n",
      "yt = [0 6 6 0] and z = [4 9 6]\n",
      "yt = [0 7 2 3] and z = [9 6 8]\n",
      "yt = [1 1 0 5] and z = [6 9 6]\n",
      "yt = [1 3 9 6] and z = [9 6 8]\n",
      "yt = [1 3 6 1] and z = [9 6 8]\n",
      "yt = [0 9 8 4] and z = [9 6 8]\n",
      "yt = [1 3 9 1] and z = [9 6 8]\n",
      "yt = [1 1 1 2] and z = [2 9 6]\n",
      "yt = [1 0 4 7] and z = [6 9 6]\n",
      "yt = [0 5 0 9] and z = [6 9 6]\n",
      "yt = [0 7 9 5] and z = [4 9 6]\n",
      "yt = [1 0 8 2] and z = [5 9 6]\n",
      "yt = [1 3 8 0] and z = [9 6 8]\n",
      "yt = [0 5 8 2] and z = [6 9 6]\n",
      "yt = [1 3 5 9] and z = [9 6 8]\n",
      "yt = [1 0 7 7] and z = [4 9 6]\n",
      "yt = [1 5 5 3] and z = [4 9 6]\n",
      "yt = [1 6 3 9] and z = [9 6 8]\n",
      "yt = [1 5 4 1] and z = [4 9 6]\n",
      "yt = [0 8 3 1] and z = [6 9 6]\n",
      "yt = [0 9 5 7] and z = [9 6 8]\n",
      "yt = [0 8 3 5] and z = [9 6 8]\n",
      "yt = [0 5 9 3] and z = [6 9 6]\n",
      "yt = [0 5 6 7] and z = [2 9 6]\n",
      "yt = [0 9 0 0] and z = [4 9 6]\n",
      "yt = [0 7 1 4] and z = [6 9 6]\n",
      "yt = [1 3 9 0] and z = [2 9 6]\n",
      "yt = [0 3 9 9] and z = [9 6 8]\n",
      "yt = [1 1 7 1] and z = [9 6 8]\n",
      "yt = [0 1 6 5] and z = [4 9 6]\n",
      "yt = [1 1 7 1] and z = [4 9 6]\n",
      "yt = [1 4 7 0] and z = [2 9 6]\n",
      "yt = [0 3 4 6] and z = [6 9 6]\n",
      "yt = [0 4 8 7] and z = [4 9 6]\n",
      "yt = [1 5 5 6] and z = [9 6 8]\n",
      "yt = [0 5 0 1] and z = [4 9 6]\n",
      "yt = [1 3 6 1] and z = [4 9 6]\n",
      "yt = [1 5 1 4] and z = [6 9 6]\n",
      "yt = [1 4 4 8] and z = [4 9 6]\n",
      "yt = [0 6 4 0] and z = [8 8 8]\n",
      "yt = [1 6 9 2] and z = [4 9 6]\n",
      "yt = [0 2 3 2] and z = [6 9 6]\n",
      "yt = [0 5 7 1] and z = [4 9 6]\n",
      "yt = [0 4 7 8] and z = [9 6 8]\n",
      "yt = [0 8 1 5] and z = [9 1 6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_add = data_add['x_train']\n",
    "y_add = data_add['y_train']\n",
    "\n",
    "y_add_t = data_add['y_test'][0]\n",
    "x_add_t = data_add['x_test'][0]\n",
    "\n",
    "per, z_hat = sorting(nueralnetadd, x_add_t, y_add_t, m,r+1)\n",
    "print(f'prosent av antall riktige addisjoner før trening er {per*100}%')\n",
    "\n",
    "for i in range(y_add_t.shape[0]):\n",
    "    print(f'yt = {y_add_t[i]} and z = {z_hat[i]}')\n",
    "\n",
    "#arr3 = algorithm_4_add(x_add, y_add, n_iter, alpha, m,r,  nueralnetadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[43marr3\u001b[49m)),arr3)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIterasjoner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogaritmen av losset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr3' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(len(arr3)),arr3)\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Logaritmen av losset')\n",
    "plt.title('Plot av feil for addisjon')\n",
    "plt.show()\n",
    "\n",
    "y_add_t = data_add['y_test'][0]\n",
    "x_add_t = data_add['x_test'][0]\n",
    "\n",
    "per, z_hat = sorting(nueralnetadd, x_add_t, y_add_t,m,r+1)\n",
    "print(f'prosent av antall riktige addisjoner er {per*100}%')\n",
    "\n",
    "for i in range(y_add_t.shape[0]):\n",
    "    #print(f'x train {x[0][i]} y train{y[0][i]}')\n",
    "    print(f'yt = {y_add_t[i]} and z = {z_hat[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
