{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m1/0pv5jqcs6h34_x160t92550h0000gn/T/ipykernel_74922/1927056692.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "from data_generators import get_train_test_sorting\n",
    "from data_generators import get_train_test_addition\n",
    "from training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppgave 1 - Forstå hvordan datasettene og transformermodellen er strukturert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.1 Gi et eksempel (som i likning $(10)$) på hvordan et datasett ${x, y}$ ville sett ut for å trene en transformermodell for å predikere et heltall $d$ gitt $d = a · b + c$ der $a, c$ er tosifrede heltall, mens $b$ er et ettsifret heltall, altså $9 ≥ b ∈ Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et sett av treningsdata kan genereres ved å la x bestå av sifrene i $a, b, c$ og alle sifrene i $d$ med unntak av det siste og lar $y = d$. Dermed vil formen for x være gitt ved $x = [a_0 , \\cdot \\cdot \\cdot, a_{r-1}, b_0, \\cdot \\cdot \\cdot, b_{r-1}, c_0, \\cdot \\cdot \\cdot, c_{r-1}, d_0, \\cdot \\cdot \\cdot, d_{r-1}]$. Gitt betingelsene i oppgaven over, la $r$ = 2, $a$ = 24, $b$ = 4, $c$ = 15 og dermed <br> $d$ = 111.  som gir oss x = $[2, 4, 4, 1, 5, 1, 1]$ og $y = [1, 1, 1]$. Merk at siste siffer i $d$ ikke er del av datasettet i x.  Modellen skal da gi $\\hat{z}$. Lengden av $\\hat{z}$, $n$, vil være gitt av lengden av x som har med lengden $n$. $\\hat{z}$ = [$\\hat{z}_0$, \\cdot \\cdot \\cdot, $\\hat{z}_5$] =  $f_{\\theta}([2, 4, 4, 1, 5, 1, 1])$. Ideelt er $\\theta$ optimert til en slik grad at <Br> $\\hat{y} = [\\hat{z}_3, \\hat{z}_4, \\hat{z}_5] = [1, 1, 1] = y$ er korrekt predikert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.2) Når optimeringen er ferdig, hvordan kan vi bruke modellen $f_{\\theta}$  til å predikere $d$ gitt $a, b, c$? Vis dette med et eksempel, på samme måte som i likning $(11)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gitt at optmeringen er ferdig, kan modellen korrekt predikere $d$. Denne prossesen av å predikere de neste sifferene i seqvensen gjøres fra å lære av de oppgitte datasettene. Følgende tabell viser hvordan dette fungerer. La verdiene være det samme som i forrige oppgave, $r = 2, a = 24, b = 4,$<Br> $c = 15$\n",
    "\n",
    "| Iterasjoner | Modell |\n",
    "|----------|----------|\n",
    "| $x^{(0)} = [2, 4, 0, 4, 1, 5]$ | $[\\hat{z}_0^{(0)}, \\hat{z}_1^{(0)}, \\hat{z}_2^{(0)}, \\hat{z}_3^{(0)}] = f_{\\theta}(x^{(0)})$|\n",
    "| $x^{(1)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}]$ | $[\\hat{z}_0^{(1)}, \\cdot \\cdot \\cdot, \\hat{z}_4^{(1)}] = f_{\\theta}(x^{(1)})$ |\n",
    "| $x^{(2)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}]$ | $[\\hat{z}_0^{(2)}, \\cdot \\cdot \\cdot, \\hat{z}_5^{(2)}] = f_{\\theta}(x^{(2)})$  |\n",
    "| $x^{(3)} = [2, 4, 0, 4, 1, 5, \\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}, \\hat{z}_5^{(2)}]$ |  |\n",
    "\n",
    "Disse predikasjonene hentes ut og returneres som $\\hat{y} = [\\hat{z}_3^{(0)}, \\hat{z}_4^{(1)}, \\hat{z}_5^{(2)}]$ som bør være likt $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.3) Anta at vi bruker cross-entropy som objektfunksjon, at $m = 5$ og $y = [4, 3, 2, 1]$. Hvilke diskret sannsynlighetsfordeling $\\hat{Y}$ ville gitt en objektfunksjon $L(θ, D) = 0$? Hva ville $\\hat{y}$ vært i dette tilfellet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy er gitt ved $L(θ, D) = -\\frac{1}{D \\cdot n} \\sum_{i=0}^{D-1} \\sum_{j=0}^{n-1} \\log \\hat{Y}_{k,j}^{(i)}$ hvor $D$ er datapunktene, $\\theta $\n",
    "er parameterne, og $\\hat{Y}$ er sannsynlighetsfordelingen til den predikterte modellen, samt er $j$ og $i$ dimensjonene til $\\hat{Y}$. Det objektfunksjonen gjør er å sammenligne onehot(y) med $\\hat{Y}$. Hvis $L(θ, D) = 0$ vil den optimerte modellen og onehot(y) være identiske. Når dette inntreffer vil $argmax_{\\text{col}}(\\hat{Y})$ = $\\hat{y}$ som igjen er lik $y$. I dette tilfellet er $y = [4,3,2,1]$, som også vil være lik $\\hat{y}$.\n",
    "$\\hat{Y}$ vil være gitt av den diskrete sannsynlighetsfordelingen:<Br><Br> $\\hat{Y}$ =\n",
    "$\\left[\\begin{array}{ccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "1 & 0 & 0 & 0\\\\\n",
    "\\end{array}\\right]$ , <Br><Br> som er lik onehot($[4,3,2,1]$). Dette betyr i praksis at paramtetrene i transformenmodellen klarer å prediktere hva som kommer videre i sekvensen og vi ender opp med samme antatt løsning ($\\hat{y}$) som faktisk løsning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.4) Gitt $d, m, n_{max}, k, p$ og $L$. Hvor mange enkeltparametre har en transformermodell? Med enkeltparametre mener vi hvor mange tall $w ∈ R$ vi må bestemme ved optimering. En matrise $W ∈ R^{m×n}$ består av $m · n$ tall eller enkeltparametre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med gitte variabler for $d, m, n_{max}, k, p$ og  $L$ er antall enkeltparametre mulig å bestemme. Enkeltparametre er gitt med $w \\in \\mathbb{R}$ noe som uttrykkes med å se på dimensjoner for ulike parametermatriser i transformermodellen.\n",
    "\n",
    "$W_E$ og $W_P$ har henholdsvis dimensjonene $W_E \\in \\mathbb{R}^{d \\times m}$ og $ W_P \\in \\mathbb{R}^{d \\times n_{max}} $ som representerer parametermatrisen til en sekvens for x med lengde n, som skrives som $z_0$. I tilegg ønskes det å gjøre $L$ paramtriserte trasformasjoner på $z_0$, så man ender opp med $L \\cdot (d \\times m + d \\times n_{max})$ for embedding delen av enkeltparamtrene. Under unenbeddingen oppstår en ny parametermatrise $W_U$ som er en sekvens med lengde $n$ med heltall opp til $m$, den har dimensjonene $ W_U \\in \\mathbb{R}^{d \\times m} $. Attention-lag bidrar også til antall enkeltparamtre for transformmodellen, der har man 4 parametermatriser; $W_O, W_V, W_Q, W_K$ alle med samme dimensjon $\\mathbb{R}^{k \\times d} $. Transformermodellen har også en $feed$-$forward $ del som bidrar med to paramtermatriser $W_1$ og $W_2$ begge med dimensjoner $\\mathbb{R}^{p \\times d} $\n",
    "\n",
    "\n",
    "Hvis man tar disse parametermatrisene i betrakning og antar at $k < d < p$ vil man ha: \n",
    "$w = d \\times m+L\\cdot (d \\times m + d \\times n_{max}) + 4 \\cdot k \\times d + 2 \\cdot p \\times d $, enkeltparametre. (siden k og p er heltall man bestemmer selv er dette en rimelig antagelse å ta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "##### 1.5 Transformermodellen er gitt i likningene $(4) - (9)$. La $n = n_{max} = 1,$  $m = d = k = p=2$ og $L=1$. Anta videre at $W_O = W_V = W_Q = W_K = W_1 = W_2 = W_U = I_{2×2}$ og at $σ(x) = Relu(x) = max(0, x)$. Dersom <Br> $W_E = \\left[\\begin{array}{ccc} 1 & 0 \\\\ 0 & \\alpha \\end{array} \\right]$ , og $W_P$ = $\\left[\\begin{array}{ccc} 1 \\\\ 0 \\end{array} \\right]$ vis at vi må ha $\\alpha > 1$ for å få  $\\hat{z} = [1]$ som output når input er $x = [1]$.\n",
    "*  *  *  * * * * * * * * * * * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med variablene oppgitt i oppgaven $L=n = n_{max}= x = 1$ og $m=d=k=p=d = 2$ og alle parametermatrisene lik\n",
    "\n",
    "$\\left[\\begin{array}{ccc}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{array}\\right]$ = $ I_{2\\times 2}$ , utenom $W_E = \\left[\\begin{array}{ccc}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha\n",
    "\\end{array}\\right]$ , og $W_P = \\left[\\begin{array}{ccc}\n",
    "1 \\\\\n",
    "0 \n",
    "\\end{array}\\right]$\n",
    "\n",
    "Med dette oppgitt vil  $ X = onehot(x) = \\left[\\begin{array}{ccc}0 \\\\1 \n",
    "\\end{array}\\right]$ som resulterer i en $z_0 = \\left[\\begin{array}{ccc}0 \\\\ \\alpha \n",
    "\\end{array}\\right]+ \\left[\\begin{array}{ccc}1 \\\\ 0 \n",
    "\\end{array}\\right]$ =$\\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$. For å videre bestemme et uttrykk for $\\hat{z}$ må vi se på hva transformermodellen gjør med $z_0$. \n",
    "\n",
    "Videre er $z_{1/2}$ = $z_0 + W_O^T  W_V  z_0 A(z_0)$, hvor $A(z_0)$ = $softmax_{col}(z_0^T W_Q^T W_K z_0+D)$ og D sørger for at den strengt nedre delen av A er 0.\n",
    "Ved å løse $A(z_0)$ får man utrykket $(1+ \\alpha ^2)$ i softmax funksjonen.\n",
    "\n",
    "$z_{1/2} = \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right] + I_{2 \\times 2} I_{2 \\times 2} \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right] softmax(1+ \\alpha ^2)$ = $ 2 \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$ ettersom softmax av et utrykk tilsvarer å dele på seg selv i e-potens, som resulterer at utrykket blir lik 1.\n",
    "\n",
    "for $z_1$ får vi et uttrykk som er $z_{1/2} + W_2^T \\sigma (W_1 z_{1/2})$, $\\sigma$ er en aktiveringsfunskjon, i dette tilfelle kan man bruke $relu(W_1 z_{1/2})$.\n",
    "Utrykket blir da:\n",
    "\n",
    "$z_1 = 2  \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]+ I_{2 \\times 2} max(0,I_{2 \\times 2} 2 \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]) $ = $ 4  \\left[\\begin{array}{ccc}1 \\\\ \\alpha \n",
    "\\end{array}\\right]$\n",
    " \n",
    "Ved hjelp av $z_1$ kan man ta i bruk likning $(8)$ for å finne sannsynlighetsfunksjonen $Z$.\n",
    "\n",
    " $Z = softmax_{col}(W_U^T Z_1)$ $,$ her vil argumentet $W_U^T z_1$ bli lik $z_1$, og softmax vil returnere $Z = \\frac{1}{e^4 + e^{4 \\alpha}} \\left[\\begin{array}{ccc}e^4 \\\\ e^{4 \\alpha }\n",
    "\\end{array}\\right]$\n",
    "\n",
    "for å få $\\hat{z} = [1]$ må $argmax(Z)$ bli 1, og dette krever at verdien på indeks [1] må være større enn den på indeks [0], da må $e^4 < e^{4 \\alpha}$ og dette impliserer at $\\alpha >1$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "## Oppgave 2 - Objektorientert programmering for transformermodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 I den utdelte koden layers.py og neural network.py finnes en objektorientert implementering av et nevralt nettverk som kan ha lineære lag og en Relu- aktiveringsfunksjon. I tillegg er embedding og posisjonsenkoding samt feed-forward lag implementert.<Br> Forklar hvordan NeuralNetwork bruker arv, eller inheritance, for å utføre en iterasjon av gradient descent (stepgd()) hvis vi antar det er initiert med minst ett LinearLayer i listen layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et nevralt nettverk lærer gjennom mange små gradevis forbedringer gjort gjennom å prøve og feile. Denne prossen består av mange ulike funksjoner og operasjoner, som beskrevet i forrige oppgave. Dette gjøres med formålet om å utføre en gradient descent. Før at det nevralenettverket skal fungere må layers initieres.\n",
    " \n",
    "Layers er en klasse som fungerer som en base klasse for alle andre typer layers i nettverket, som representerer de ulike prossesene det nevralenettveket utfører. Her implementeres en basis versjon av metoder som forward(), backward() og step_gd(). Dette vil si at hvis et objekt arver fra Layers klassen så vil det objektet har metodene forward(), backward() og step_gd(). Derved vil alle layers som arver fra Layers base klassen implementere eller overskrive disse metodene med kode tilrettet hver individuelle layer. Denne strukturen tillater også at et lag har en egen spesifiserte step_gd() med at det kan overskrive metoden til å være mest hensiktsmessig for det spesifikke laget. Resultatet av dette gjør at neural_network kan operere på et høyere abstraksjonsnivå og kan implementere \"universelle\" metoder som step_gd() uten å ta hensyn til de ulike spesifikke detaljene til hvert lag. \n",
    "\n",
    "Mer spesifikt bruker neural_network arv til å kunne behandle alle sine layers på samme måte, selv om de kan ha forskjellig implementerte step_gd() metoder. Polymorfisme lar da neural_network kalle samme funksjon (step_gd()) på samme måte for hvert lag uten å vite hvilken subklasse hvert layer tilhører. Dermed kan hver operasjon som nural_network utfører kalles gjennom bruk av forward(), for å finne objektfunksjonen, backwards(), for å resette og oppdatere verdiene i nettverket, og step_gd() for å optimalisere vektingen og biasene i treningen av nettverket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "#### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "##### 3.3 Implementer en funksjon som sorterer en sekvens a, på samme måte som kapittel 3.1.2 og rapporter hvor stor andel av disse sekvensene du klarte å sortere riktig. Hvis du regner ut hvor mange mulige sekvenser av lengde r = 5 med 0 og 1 det er mulig å generere, ser du kanskje at det er umulig å teste på nye sekvenser.\n",
    "\n",
    "Før et nevraltnettverk kan returnere korrekt output til en gitt input må den trenes. Denne treningen gjøres gjennom å prossesen beskrevet i de tidligere oppgavene ved å bruke et datasett for optimalisere outputet. Ideelt skal det nevralenettverket trenes på et stort datasett før det testes med ny data som ikke ble brukt i testing fasen. Dette gjøre for å kunne observere om det nevralenettverket kan korrekt predikere et resultat fra ny data istedet for å kun gjengi et resultat det tidligere har sett at er riktig respons til et gitt input.\n",
    "\n",
    "Dette er desverre vaskelig for et nevraltnettverk som skal lære seg å sortere en sekvens som $a$. Sekvensen $a$ er gitt ved parameterene $r = 5$ og $m = 2$. Dette betyr at sekvensen består av fem siffer og har to mulige siffer hvert element i sekvensen kan være, her 0 og 1. Dermed er antallet, $n$, for unike sekvenser som $a$ kan bestå av, være gitt ved $n = 2^5 = 32$. I tilegg er det kun 6 sekvenser som kan være riktig svar. I en tidligere iterasjon av koden ville programmet optimaliseres inn i et \"lokalt optima\" hvor den gjette $z = [0, 0, 0, 1, 1]$ på alle  ettersom dette er svaret som forekommer oftest i test dataen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training 0.0%\n",
      "\n",
      "Accuracy after training 0.0%\n"
     ]
    }
   ],
   "source": [
    "#training the module\n",
    "r = 5\n",
    "m = 3\n",
    "d = 20\n",
    "k = 5\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2*r - 1\n",
    "n_iter = 300\n",
    "alpha = 0.001\n",
    "num_of_samples = 250\n",
    "num_train_batches = 10\n",
    "num_test_batches = 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, num_of_samples, num_train_batches,num_test_batches)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1, feed_forward1,attention2, feed_forward2, un_embed_pos, softmax]\n",
    "nueralnetsortwil = NeuralNetwork(layers)\n",
    "\n",
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "x_t = data['x_test'][0]\n",
    "y_t = data['y_test'][0]\n",
    "\n",
    "per, y_hat = sorting(nueralnetsortwil, x_t, y_t,m, r)\n",
    "print(f'Accuracy before training {per*100}%')\n",
    "\n",
    "#arr2 = algorithm_4_sort_finished(x, y, n_iter, alpha, m, nueralnetsortwil, r)\n",
    "\n",
    "per_after, y_hat_after = sorting(nueralnetsortwil, x_t, y_t,m, r)\n",
    "print(f'\\nAccuracy after training {per_after*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy after training 0.0%\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAccuracy after training {per_after*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(\u001b[43marr2\u001b[49m),\u001b[38;5;28mlen\u001b[39m(arr2)),arr2)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIterasjoner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogaritmen av losset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr2' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,len(arr2),len(arr2)),arr2)\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Logaritmen av losset')\n",
    "plt.title('Plot av feil')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 med r=7 og m=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prosent av antall riktige sorteringer før trening er 0.0%\n",
      "Epoch 10/300, Average Loss: 0.3592121665482431\n",
      "Epoch 20/300, Average Loss: 0.2877153168209644\n",
      "Epoch 30/300, Average Loss: 0.25032171803019054\n",
      "Epoch 40/300, Average Loss: 0.4434108694727277\n",
      "Epoch 50/300, Average Loss: 0.26445036516634207\n",
      "Epoch 60/300, Average Loss: 0.2519110586972068\n",
      "Epoch 70/300, Average Loss: 0.24134652289514286\n",
      "Epoch 80/300, Average Loss: 0.24263920566083405\n",
      "Epoch 90/300, Average Loss: 0.22968500102833547\n",
      "Epoch 100/300, Average Loss: 0.21354567046626824\n",
      "Epoch 110/300, Average Loss: 0.19889640333513328\n",
      "Epoch 120/300, Average Loss: 0.17417893466467452\n",
      "Epoch 130/300, Average Loss: 0.15172908345014052\n",
      "Epoch 140/300, Average Loss: 0.12857561118677846\n",
      "Epoch 150/300, Average Loss: 0.10644648180444516\n",
      "Epoch 160/300, Average Loss: 0.08517477447152212\n",
      "Epoch 170/300, Average Loss: 0.06794600777754327\n",
      "Epoch 180/300, Average Loss: 0.057250163787984656\n",
      "Epoch 190/300, Average Loss: 0.048974743683200324\n",
      "Epoch 200/300, Average Loss: 0.04228747307362192\n",
      "Epoch 210/300, Average Loss: 0.036547435312281135\n",
      "Epoch 220/300, Average Loss: 0.03251424383301825\n",
      "Epoch 230/300, Average Loss: 0.029544633886326648\n",
      "Epoch 240/300, Average Loss: 0.026529255431734504\n",
      "Epoch 250/300, Average Loss: 0.024624953674733295\n",
      "Epoch 260/300, Average Loss: 0.02331010388704601\n",
      "Epoch 270/300, Average Loss: 0.026453816858519198\n",
      "Epoch 280/300, Average Loss: 0.02381092857941678\n",
      "Epoch 290/300, Average Loss: 0.023594130348751456\n",
      "Epoch 300/300, Average Loss: 0.022233167858989086\n"
     ]
    }
   ],
   "source": [
    "#training the module\n",
    "r = 7\n",
    "m = 5\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2*r - 1\n",
    "n_iter = 300\n",
    "alpha = 0.001\n",
    "num_of_samples = 250\n",
    "num_train_batches = 10\n",
    "num_test_batches = 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, num_of_samples, num_train_batches,num_test_batches)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1,feed_forward1,attention2, feed_forward2, un_embed_pos, softmax]\n",
    "nueralnetsortlong = NeuralNetwork(layers)\n",
    "\n",
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "x_t = data['x_test'][0]\n",
    "y_t = data['y_test'][0]\n",
    "\n",
    "per, y_hat = sorting(nueralnetsortlong, x_t, y_t,m, r)\n",
    "print(f'prosent av antall riktige sorteringer før trening er {per*100}%')\n",
    "\n",
    "#arr332 = algorithm_4_sort_finished(x, y, n_iter, alpha, m, nueralnetsortlong, r)\n",
    "\n",
    "per_after, y_hat_after = sorting(nueralnetsortlong, x_t, y_t,m, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy after training 88.4%\n",
      "xt = [2. 0. 1. 0. 2. 2. 3.] yt = [0. 0. 1. 2. 2. 2. 3.] and z = [0. 0. 1. 2. 2. 2. 3.]\n",
      "xt = [4. 2. 4. 3. 0. 4. 3.] yt = [0. 2. 3. 3. 4. 4. 4.] and z = [0. 2. 3. 3. 4. 4. 4.]\n",
      "xt = [1. 1. 4. 2. 0. 4. 2.] yt = [0. 1. 1. 2. 2. 4. 4.] and z = [0. 1. 1. 1. 2. 4. 4.]\n",
      "xt = [2. 0. 0. 0. 1. 4. 0.] yt = [0. 0. 0. 0. 1. 2. 4.] and z = [0. 0. 0. 0. 1. 2. 4.]\n",
      "xt = [1. 1. 0. 0. 4. 3. 3.] yt = [0. 0. 1. 1. 3. 3. 4.] and z = [0. 0. 1. 1. 3. 3. 4.]\n",
      "xt = [2. 2. 4. 2. 4. 2. 0.] yt = [0. 2. 2. 2. 2. 4. 4.] and z = [0. 2. 2. 2. 2. 4. 4.]\n",
      "xt = [1. 0. 0. 3. 4. 1. 0.] yt = [0. 0. 0. 1. 1. 3. 4.] and z = [0. 0. 0. 1. 1. 3. 4.]\n",
      "xt = [4. 4. 3. 2. 2. 4. 0.] yt = [0. 2. 2. 3. 4. 4. 4.] and z = [0. 2. 2. 3. 4. 4. 4.]\n",
      "xt = [4. 2. 0. 2. 4. 0. 1.] yt = [0. 0. 1. 2. 2. 4. 4.] and z = [0. 0. 1. 2. 2. 4. 4.]\n",
      "xt = [4. 1. 2. 0. 3. 4. 2.] yt = [0. 1. 2. 2. 3. 4. 4.] and z = [0. 1. 2. 2. 3. 4. 4.]\n",
      "xt = [2. 2. 2. 2. 4. 0. 1.] yt = [0. 1. 2. 2. 2. 2. 4.] and z = [0. 1. 2. 2. 2. 2. 4.]\n",
      "xt = [3. 2. 0. 1. 4. 3. 2.] yt = [0. 1. 2. 2. 3. 3. 4.] and z = [0. 1. 2. 2. 3. 3. 4.]\n",
      "xt = [4. 0. 2. 3. 4. 3. 0.] yt = [0. 0. 2. 3. 3. 4. 4.] and z = [0. 0. 2. 3. 3. 4. 4.]\n",
      "xt = [1. 1. 0. 1. 4. 2. 2.] yt = [0. 1. 1. 1. 2. 2. 4.] and z = [0. 1. 1. 1. 1. 2. 4.]\n",
      "xt = [0. 4. 2. 1. 4. 4. 2.] yt = [0. 1. 2. 2. 4. 4. 4.] and z = [0. 1. 2. 2. 4. 4. 4.]\n",
      "xt = [3. 2. 3. 3. 4. 4. 1.] yt = [1. 2. 3. 3. 3. 4. 4.] and z = [1. 2. 3. 3. 3. 4. 4.]\n",
      "xt = [4. 3. 2. 4. 0. 0. 3.] yt = [0. 0. 2. 3. 3. 4. 4.] and z = [0. 0. 2. 3. 3. 4. 4.]\n",
      "xt = [1. 2. 2. 2. 4. 1. 2.] yt = [1. 1. 2. 2. 2. 2. 4.] and z = [1. 1. 2. 2. 2. 2. 4.]\n",
      "xt = [0. 2. 2. 1. 1. 3. 1.] yt = [0. 1. 1. 1. 2. 2. 3.] and z = [0. 1. 1. 1. 2. 2. 3.]\n",
      "xt = [0. 1. 2. 4. 2. 3. 4.] yt = [0. 1. 2. 2. 3. 4. 4.] and z = [0. 1. 2. 2. 3. 4. 4.]\n",
      "xt = [3. 2. 2. 4. 1. 0. 2.] yt = [0. 1. 2. 2. 2. 3. 4.] and z = [0. 1. 2. 2. 2. 3. 4.]\n",
      "xt = [0. 2. 4. 0. 3. 4. 0.] yt = [0. 0. 0. 2. 3. 4. 4.] and z = [0. 0. 0. 2. 3. 4. 4.]\n",
      "xt = [1. 3. 2. 3. 0. 0. 4.] yt = [0. 0. 1. 2. 3. 3. 4.] and z = [0. 0. 1. 2. 3. 3. 4.]\n",
      "xt = [4. 2. 1. 4. 0. 3. 0.] yt = [0. 0. 1. 2. 3. 4. 4.] and z = [0. 0. 1. 2. 3. 4. 4.]\n",
      "xt = [0. 1. 3. 2. 4. 1. 2.] yt = [0. 1. 1. 2. 2. 3. 4.] and z = [0. 1. 1. 2. 2. 3. 4.]\n",
      "xt = [3. 4. 4. 3. 4. 3. 4.] yt = [3. 3. 3. 4. 4. 4. 4.] and z = [1. 3. 3. 4. 4. 4. 4.]\n",
      "xt = [0. 0. 1. 3. 3. 0. 1.] yt = [0. 0. 0. 1. 1. 3. 3.] and z = [0. 0. 0. 1. 2. 3. 3.]\n",
      "xt = [0. 1. 3. 3. 1. 1. 3.] yt = [0. 1. 1. 1. 3. 3. 3.] and z = [0. 1. 1. 1. 3. 3. 3.]\n",
      "xt = [3. 1. 3. 1. 3. 3. 3.] yt = [1. 1. 3. 3. 3. 3. 3.] and z = [1. 1. 3. 3. 3. 3. 3.]\n",
      "xt = [0. 3. 4. 0. 3. 2. 0.] yt = [0. 0. 0. 2. 3. 3. 4.] and z = [0. 0. 0. 2. 3. 3. 4.]\n",
      "xt = [2. 0. 1. 0. 1. 3. 1.] yt = [0. 0. 1. 1. 1. 2. 3.] and z = [0. 0. 1. 1. 1. 2. 3.]\n",
      "xt = [1. 4. 4. 1. 0. 1. 2.] yt = [0. 1. 1. 1. 2. 4. 4.] and z = [0. 1. 1. 1. 2. 4. 4.]\n",
      "xt = [0. 3. 4. 1. 3. 4. 4.] yt = [0. 1. 3. 3. 4. 4. 4.] and z = [0. 1. 3. 3. 4. 4. 4.]\n",
      "xt = [4. 0. 0. 0. 0. 3. 4.] yt = [0. 0. 0. 0. 3. 4. 4.] and z = [0. 0. 0. 0. 3. 4. 4.]\n",
      "xt = [2. 4. 4. 3. 3. 0. 2.] yt = [0. 2. 2. 3. 3. 4. 4.] and z = [0. 2. 2. 3. 3. 4. 4.]\n",
      "xt = [1. 0. 3. 0. 3. 1. 2.] yt = [0. 0. 1. 1. 2. 3. 3.] and z = [0. 0. 1. 1. 2. 3. 3.]\n",
      "xt = [3. 1. 2. 4. 3. 4. 2.] yt = [1. 2. 2. 3. 3. 4. 4.] and z = [1. 2. 2. 3. 3. 4. 4.]\n",
      "xt = [3. 4. 2. 2. 0. 0. 1.] yt = [0. 0. 1. 2. 2. 3. 4.] and z = [0. 0. 1. 2. 2. 3. 4.]\n",
      "xt = [2. 4. 1. 0. 2. 2. 2.] yt = [0. 1. 2. 2. 2. 2. 4.] and z = [0. 1. 2. 2. 2. 2. 4.]\n",
      "xt = [1. 2. 1. 0. 0. 3. 4.] yt = [0. 0. 1. 1. 2. 3. 4.] and z = [0. 0. 1. 1. 2. 3. 4.]\n",
      "xt = [1. 2. 2. 2. 4. 4. 4.] yt = [1. 2. 2. 2. 4. 4. 4.] and z = [1. 2. 2. 2. 4. 4. 4.]\n",
      "xt = [1. 0. 2. 4. 4. 2. 2.] yt = [0. 1. 2. 2. 2. 4. 4.] and z = [0. 1. 2. 2. 2. 4. 4.]\n",
      "xt = [4. 4. 2. 2. 0. 0. 2.] yt = [0. 0. 2. 2. 2. 4. 4.] and z = [0. 0. 2. 2. 2. 4. 4.]\n",
      "xt = [0. 1. 0. 3. 1. 2. 1.] yt = [0. 0. 1. 1. 1. 2. 3.] and z = [0. 0. 1. 1. 1. 2. 3.]\n",
      "xt = [4. 0. 4. 4. 0. 1. 4.] yt = [0. 0. 1. 4. 4. 4. 4.] and z = [0. 0. 1. 4. 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAccuracy after training {per_after*100}%')\n",
    "for i in range(45):\n",
    "    print(f'xt = {x_t[i]} yt = {y_t[i]} and z = {y_hat_after[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,len(arr332),len(arr332)),arr332)\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Losset')\n",
    "plt.title('Plot av feil')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prosent av antall riktige sorteringer før trening er 0.0%\n",
      "xt = [1. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 1.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 0.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 0.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 1.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 0.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 1.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 1.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 1.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 0.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 0.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 1.] yt = [1. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 1.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 1. 0.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 0. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 0. 1.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 1. 0. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 1. 0. 0.] yt = [0. 0. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 1. 1. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [0. 0. 0. 1. 0.] yt = [0. 0. 0. 0. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 1. 0. 1. 1.] yt = [0. 1. 1. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "xt = [1. 0. 0. 1. 0.] yt = [0. 0. 0. 1. 1.] and z = [1. 0. 1. 1. 0.]\n",
      "Epoch 1/300, Average Loss: 0.2992553152815708\n",
      "Epoch 2/300, Average Loss: 0.29738573818999126\n",
      "Epoch 3/300, Average Loss: 0.2954694352461938\n",
      "Epoch 4/300, Average Loss: 0.29364622158213416\n",
      "Epoch 5/300, Average Loss: 0.29189199042473596\n",
      "Epoch 6/300, Average Loss: 0.29015394566656083\n",
      "Epoch 7/300, Average Loss: 0.28840014758721005\n",
      "Epoch 8/300, Average Loss: 0.2866105506239905\n",
      "Epoch 9/300, Average Loss: 0.28476235773623615\n",
      "Epoch 10/300, Average Loss: 0.28285344629460113\n",
      "Epoch 11/300, Average Loss: 0.2808615348853587\n",
      "Epoch 12/300, Average Loss: 0.27879184668054324\n",
      "Epoch 13/300, Average Loss: 0.27664659622468857\n",
      "Epoch 14/300, Average Loss: 0.27442456754620437\n",
      "Epoch 15/300, Average Loss: 0.27212849948890383\n",
      "Epoch 16/300, Average Loss: 0.26975385137840413\n",
      "Epoch 17/300, Average Loss: 0.2672949688171476\n",
      "Epoch 18/300, Average Loss: 0.2647408826189813\n",
      "Epoch 19/300, Average Loss: 0.26208375008076434\n",
      "Epoch 20/300, Average Loss: 0.25933119473309546\n",
      "Epoch 21/300, Average Loss: 0.25648507159169825\n",
      "Epoch 22/300, Average Loss: 0.2535425505029109\n",
      "Epoch 23/300, Average Loss: 0.2505033401154638\n",
      "Epoch 24/300, Average Loss: 0.2473714387666713\n",
      "Epoch 25/300, Average Loss: 0.2441503279698083\n",
      "Epoch 26/300, Average Loss: 0.2408434821287569\n",
      "Epoch 27/300, Average Loss: 0.23745635093235923\n",
      "Epoch 28/300, Average Loss: 0.23399670073511789\n",
      "Epoch 29/300, Average Loss: 0.23048554346589456\n",
      "Epoch 30/300, Average Loss: 0.2269349942684081\n",
      "Epoch 31/300, Average Loss: 0.22334917707668583\n",
      "Epoch 32/300, Average Loss: 0.219757115632976\n",
      "Epoch 33/300, Average Loss: 0.21619772958221\n",
      "Epoch 34/300, Average Loss: 0.21268474134512344\n",
      "Epoch 35/300, Average Loss: 0.20921527156557498\n",
      "Epoch 36/300, Average Loss: 0.20579889963577855\n",
      "Epoch 37/300, Average Loss: 0.2024080693367673\n",
      "Epoch 38/300, Average Loss: 0.19898202299386897\n",
      "Epoch 39/300, Average Loss: 0.19555713753762188\n",
      "Epoch 40/300, Average Loss: 0.19223437601193377\n",
      "Epoch 41/300, Average Loss: 0.18910710350264398\n",
      "Epoch 42/300, Average Loss: 0.1861765547147182\n",
      "Epoch 43/300, Average Loss: 0.18339974194228312\n",
      "Epoch 44/300, Average Loss: 0.18071701591102057\n",
      "Epoch 45/300, Average Loss: 0.17814860084158984\n",
      "Epoch 46/300, Average Loss: 0.17568364837013742\n",
      "Epoch 47/300, Average Loss: 0.1732041441187499\n",
      "Epoch 48/300, Average Loss: 0.17085234241999242\n",
      "Epoch 49/300, Average Loss: 0.1686386705313975\n",
      "Epoch 50/300, Average Loss: 0.16653475392008416\n",
      "Epoch 51/300, Average Loss: 0.16448592105294785\n",
      "Epoch 52/300, Average Loss: 0.16241141214780935\n",
      "Epoch 53/300, Average Loss: 0.1603727805522952\n",
      "Epoch 54/300, Average Loss: 0.15835693836787684\n",
      "Epoch 55/300, Average Loss: 0.1563655624286948\n",
      "Epoch 56/300, Average Loss: 0.15435935575727458\n",
      "Epoch 57/300, Average Loss: 0.15229446478936035\n",
      "Epoch 58/300, Average Loss: 0.1503030381760331\n",
      "Epoch 59/300, Average Loss: 0.14833341996875604\n",
      "Epoch 60/300, Average Loss: 0.14633337552163847\n",
      "Epoch 61/300, Average Loss: 0.1443537620568683\n",
      "Epoch 62/300, Average Loss: 0.14243312179654516\n",
      "Epoch 63/300, Average Loss: 0.14051891267024436\n",
      "Epoch 64/300, Average Loss: 0.13859645922216732\n",
      "Epoch 65/300, Average Loss: 0.13670639682105118\n",
      "Epoch 66/300, Average Loss: 0.13482157948528398\n",
      "Epoch 67/300, Average Loss: 0.1329296223710559\n",
      "Epoch 68/300, Average Loss: 0.13105928048915855\n",
      "Epoch 69/300, Average Loss: 0.12916149229570412\n",
      "Epoch 70/300, Average Loss: 0.1272729138872722\n",
      "Epoch 71/300, Average Loss: 0.12544275756362774\n",
      "Epoch 72/300, Average Loss: 0.12363682063083885\n",
      "Epoch 73/300, Average Loss: 0.12182758781009229\n",
      "Epoch 74/300, Average Loss: 0.12000477967177947\n",
      "Epoch 75/300, Average Loss: 0.11813725241045916\n",
      "Epoch 76/300, Average Loss: 0.11622277149913392\n",
      "Epoch 77/300, Average Loss: 0.11427787064894772\n",
      "Epoch 78/300, Average Loss: 0.11228163140783007\n",
      "Epoch 79/300, Average Loss: 0.11023148318914677\n",
      "Epoch 80/300, Average Loss: 0.10821497880666406\n",
      "Epoch 81/300, Average Loss: 0.10626684615536719\n",
      "Epoch 82/300, Average Loss: 0.10440995273574236\n",
      "Epoch 83/300, Average Loss: 0.10260563039147616\n",
      "Epoch 84/300, Average Loss: 0.10079801640243405\n",
      "Epoch 85/300, Average Loss: 0.0990426900128281\n",
      "Epoch 86/300, Average Loss: 0.09733962048677487\n",
      "Epoch 87/300, Average Loss: 0.09564953598828564\n",
      "Epoch 88/300, Average Loss: 0.09397660972463974\n",
      "Epoch 89/300, Average Loss: 0.09235895977067114\n",
      "Epoch 90/300, Average Loss: 0.09079269194558572\n",
      "Epoch 91/300, Average Loss: 0.08926378639118612\n",
      "Epoch 92/300, Average Loss: 0.08776938687638322\n",
      "Epoch 93/300, Average Loss: 0.08631656140963959\n",
      "Epoch 94/300, Average Loss: 0.08493021725012492\n",
      "Epoch 95/300, Average Loss: 0.083505195822174\n",
      "Epoch 96/300, Average Loss: 0.08207432474832017\n",
      "Epoch 97/300, Average Loss: 0.08063434578302052\n",
      "Epoch 98/300, Average Loss: 0.07918469517188688\n",
      "Epoch 99/300, Average Loss: 0.07777465369274969\n",
      "Epoch 100/300, Average Loss: 0.07633373451838768\n",
      "Epoch 101/300, Average Loss: 0.0748596811636084\n",
      "Epoch 102/300, Average Loss: 0.07334194397408363\n",
      "Epoch 103/300, Average Loss: 0.07179831297611505\n",
      "Epoch 104/300, Average Loss: 0.07021943058655164\n",
      "Epoch 105/300, Average Loss: 0.06864723634040835\n",
      "Epoch 106/300, Average Loss: 0.0670686091251852\n",
      "Epoch 107/300, Average Loss: 0.06539773533902941\n",
      "Epoch 108/300, Average Loss: 0.06377576542968275\n",
      "Epoch 109/300, Average Loss: 0.06221687402780444\n",
      "Epoch 110/300, Average Loss: 0.060692429417078586\n",
      "Epoch 111/300, Average Loss: 0.05928231052055509\n",
      "Epoch 112/300, Average Loss: 0.05788533628324821\n",
      "Epoch 113/300, Average Loss: 0.056613067144960304\n",
      "Epoch 114/300, Average Loss: 0.0554132150766727\n",
      "Epoch 115/300, Average Loss: 0.05431476078772642\n",
      "Epoch 116/300, Average Loss: 0.05326418233372039\n",
      "Epoch 117/300, Average Loss: 0.05229120002076589\n",
      "Epoch 118/300, Average Loss: 0.051408323729732495\n",
      "Epoch 119/300, Average Loss: 0.05055527660044736\n",
      "Epoch 120/300, Average Loss: 0.0498029619949035\n",
      "Epoch 121/300, Average Loss: 0.049120586156057224\n",
      "Epoch 122/300, Average Loss: 0.04853453183538399\n",
      "Epoch 123/300, Average Loss: 0.047956130698362964\n",
      "Epoch 124/300, Average Loss: 0.04747893828814684\n",
      "Epoch 125/300, Average Loss: 0.04704606942601565\n",
      "Epoch 126/300, Average Loss: 0.04669199134857643\n",
      "Epoch 127/300, Average Loss: 0.04645450656347252\n",
      "Epoch 128/300, Average Loss: 0.04632430983794802\n",
      "Epoch 129/300, Average Loss: 0.046342203745364\n",
      "Epoch 130/300, Average Loss: 0.046504483999314075\n",
      "Epoch 131/300, Average Loss: 0.046748191545375524\n",
      "Epoch 132/300, Average Loss: 0.0470915835651909\n",
      "Epoch 133/300, Average Loss: 0.04746853922497372\n",
      "Epoch 134/300, Average Loss: 0.04797864808710098\n",
      "Epoch 135/300, Average Loss: 0.04847900485336128\n",
      "Epoch 136/300, Average Loss: 0.04907934991508386\n",
      "Epoch 137/300, Average Loss: 0.04951439299884662\n",
      "Epoch 138/300, Average Loss: 0.04987011121321635\n",
      "Epoch 139/300, Average Loss: 0.0501012999588332\n",
      "Epoch 140/300, Average Loss: 0.05033774317959879\n",
      "Epoch 141/300, Average Loss: 0.05052114065296598\n",
      "Epoch 142/300, Average Loss: 0.050703867009553996\n",
      "Epoch 143/300, Average Loss: 0.050874059489200395\n",
      "Epoch 144/300, Average Loss: 0.05107654860013984\n",
      "Epoch 145/300, Average Loss: 0.05127581327354138\n",
      "Epoch 146/300, Average Loss: 0.051482177489091764\n",
      "Epoch 147/300, Average Loss: 0.051685355389361434\n",
      "Epoch 148/300, Average Loss: 0.051890934799992385\n",
      "Epoch 149/300, Average Loss: 0.05207353834976458\n",
      "Epoch 150/300, Average Loss: 0.052247303619038066\n",
      "Epoch 151/300, Average Loss: 0.05241034339769658\n",
      "Epoch 152/300, Average Loss: 0.05259641974556396\n",
      "Epoch 153/300, Average Loss: 0.052778399596659975\n",
      "Epoch 154/300, Average Loss: 0.0529673360859387\n",
      "Epoch 155/300, Average Loss: 0.053160603855456436\n",
      "Epoch 156/300, Average Loss: 0.05334240790529245\n",
      "Epoch 157/300, Average Loss: 0.05351464416805083\n",
      "Epoch 158/300, Average Loss: 0.0536897278775306\n",
      "Epoch 159/300, Average Loss: 0.0538554358150105\n",
      "Epoch 160/300, Average Loss: 0.05402309492768341\n",
      "Epoch 161/300, Average Loss: 0.05420744942563943\n",
      "Epoch 162/300, Average Loss: 0.054412826242094425\n",
      "Epoch 163/300, Average Loss: 0.054648114359301135\n",
      "Epoch 164/300, Average Loss: 0.05495491856274341\n",
      "Epoch 165/300, Average Loss: 0.055359878501727966\n",
      "Epoch 166/300, Average Loss: 0.05585266598503204\n",
      "Epoch 167/300, Average Loss: 0.05640233815650404\n",
      "Epoch 168/300, Average Loss: 0.05713411167725071\n",
      "Epoch 169/300, Average Loss: 0.05817720995909811\n",
      "Epoch 170/300, Average Loss: 0.05942651920780795\n",
      "Epoch 171/300, Average Loss: 0.060753869782447345\n",
      "Epoch 172/300, Average Loss: 0.06198849757669359\n",
      "Epoch 173/300, Average Loss: 0.06302294292114448\n",
      "Epoch 174/300, Average Loss: 0.06383266976932792\n",
      "Epoch 175/300, Average Loss: 0.06439730365379621\n",
      "Epoch 176/300, Average Loss: 0.06447159342037202\n",
      "Epoch 177/300, Average Loss: 0.06406492262160948\n",
      "Epoch 178/300, Average Loss: 0.06351048765655073\n",
      "Epoch 179/300, Average Loss: 0.06294650147597806\n",
      "Epoch 180/300, Average Loss: 0.062439683467545415\n",
      "Epoch 181/300, Average Loss: 0.06207093991078484\n",
      "Epoch 182/300, Average Loss: 0.06169214301048552\n",
      "Epoch 183/300, Average Loss: 0.06128529185361892\n",
      "Epoch 184/300, Average Loss: 0.06090145313697849\n",
      "Epoch 185/300, Average Loss: 0.060567857406971105\n",
      "Epoch 186/300, Average Loss: 0.0602697144067997\n",
      "Epoch 187/300, Average Loss: 0.06002769730901712\n",
      "Epoch 188/300, Average Loss: 0.059825348694914626\n",
      "Epoch 189/300, Average Loss: 0.059626369163868875\n",
      "Epoch 190/300, Average Loss: 0.059423552508447094\n",
      "Epoch 191/300, Average Loss: 0.059230831753969704\n",
      "Epoch 192/300, Average Loss: 0.05904097920541749\n",
      "Epoch 193/300, Average Loss: 0.05882288935021025\n",
      "Epoch 194/300, Average Loss: 0.05856528420247455\n",
      "Epoch 195/300, Average Loss: 0.058301235551828465\n",
      "Epoch 196/300, Average Loss: 0.058079205169669745\n",
      "Epoch 197/300, Average Loss: 0.05788577160182688\n",
      "Epoch 198/300, Average Loss: 0.05769137006030707\n",
      "Epoch 199/300, Average Loss: 0.05748773176325213\n",
      "Epoch 200/300, Average Loss: 0.057263322499908176\n",
      "Epoch 201/300, Average Loss: 0.05703247733700162\n",
      "Epoch 202/300, Average Loss: 0.056779152213963194\n",
      "Epoch 203/300, Average Loss: 0.05651454031899179\n",
      "Epoch 204/300, Average Loss: 0.05624145857169773\n",
      "Epoch 205/300, Average Loss: 0.05595689229539401\n",
      "Epoch 206/300, Average Loss: 0.05566066519120848\n",
      "Epoch 207/300, Average Loss: 0.055357143826461286\n",
      "Epoch 208/300, Average Loss: 0.055074605648048024\n",
      "Epoch 209/300, Average Loss: 0.05480437767787224\n",
      "Epoch 210/300, Average Loss: 0.05453429285652421\n",
      "Epoch 211/300, Average Loss: 0.05424986435622142\n",
      "Epoch 212/300, Average Loss: 0.05395976476665108\n",
      "Epoch 213/300, Average Loss: 0.05366160288205344\n",
      "Epoch 214/300, Average Loss: 0.053349785962064855\n",
      "Epoch 215/300, Average Loss: 0.053025435414604236\n",
      "Epoch 216/300, Average Loss: 0.052695443804150165\n",
      "Epoch 217/300, Average Loss: 0.05236223570847963\n",
      "Epoch 218/300, Average Loss: 0.05202250811086876\n",
      "Epoch 219/300, Average Loss: 0.05165581892810352\n",
      "Epoch 220/300, Average Loss: 0.051274365458451855\n",
      "Epoch 221/300, Average Loss: 0.05088238155914565\n",
      "Epoch 222/300, Average Loss: 0.050497742140509824\n",
      "Epoch 223/300, Average Loss: 0.05010982690706308\n",
      "Epoch 224/300, Average Loss: 0.04972774005445556\n",
      "Epoch 225/300, Average Loss: 0.049349119284229484\n",
      "Epoch 226/300, Average Loss: 0.04897462525271091\n",
      "Epoch 227/300, Average Loss: 0.04860227053033261\n",
      "Epoch 228/300, Average Loss: 0.048235981196534994\n",
      "Epoch 229/300, Average Loss: 0.047875865002375374\n",
      "Epoch 230/300, Average Loss: 0.047520403402258785\n",
      "Epoch 231/300, Average Loss: 0.04716696931249535\n",
      "Epoch 232/300, Average Loss: 0.04680910578240492\n",
      "Epoch 233/300, Average Loss: 0.04644749078614109\n",
      "Epoch 234/300, Average Loss: 0.04612185894031129\n",
      "Epoch 235/300, Average Loss: 0.04583176164989175\n",
      "Epoch 236/300, Average Loss: 0.04554318696131468\n",
      "Epoch 237/300, Average Loss: 0.04524900360845148\n",
      "Epoch 238/300, Average Loss: 0.04495336712591471\n",
      "Epoch 239/300, Average Loss: 0.04466924144779754\n",
      "Epoch 240/300, Average Loss: 0.044386518168446715\n",
      "Epoch 241/300, Average Loss: 0.044095936680132654\n",
      "Epoch 242/300, Average Loss: 0.043798383168039724\n",
      "Epoch 243/300, Average Loss: 0.04349492325836123\n",
      "Epoch 244/300, Average Loss: 0.04318632946732023\n",
      "Epoch 245/300, Average Loss: 0.04287196869486944\n",
      "Epoch 246/300, Average Loss: 0.04255326677657989\n",
      "Epoch 247/300, Average Loss: 0.04222982912521135\n",
      "Epoch 248/300, Average Loss: 0.041904538810454764\n",
      "Epoch 249/300, Average Loss: 0.0415798241302642\n",
      "Epoch 250/300, Average Loss: 0.041255617388283095\n",
      "Epoch 251/300, Average Loss: 0.04093072898461049\n",
      "Epoch 252/300, Average Loss: 0.04060410247822608\n",
      "Epoch 253/300, Average Loss: 0.04027822310491959\n",
      "Epoch 254/300, Average Loss: 0.03995335567038535\n",
      "Epoch 255/300, Average Loss: 0.03963014208626395\n",
      "Epoch 256/300, Average Loss: 0.03929466563985619\n",
      "Epoch 257/300, Average Loss: 0.03894794397839521\n",
      "Epoch 258/300, Average Loss: 0.038596245464495735\n",
      "Epoch 259/300, Average Loss: 0.03824095967825133\n",
      "Epoch 260/300, Average Loss: 0.037881380315802377\n",
      "Epoch 261/300, Average Loss: 0.03751624041367294\n",
      "Epoch 262/300, Average Loss: 0.03715543550872763\n",
      "Epoch 263/300, Average Loss: 0.03679012075418148\n",
      "Epoch 264/300, Average Loss: 0.03641392678962515\n",
      "Epoch 265/300, Average Loss: 0.03602909194421762\n",
      "Epoch 266/300, Average Loss: 0.035636855006081394\n",
      "Epoch 267/300, Average Loss: 0.03525251565588914\n",
      "Epoch 268/300, Average Loss: 0.03486260841827809\n",
      "Epoch 269/300, Average Loss: 0.03445764780941173\n",
      "Epoch 270/300, Average Loss: 0.03403437974454614\n",
      "Epoch 271/300, Average Loss: 0.03359532784039378\n",
      "Epoch 272/300, Average Loss: 0.03315752799199628\n",
      "Epoch 273/300, Average Loss: 0.03272901388209903\n",
      "Epoch 274/300, Average Loss: 0.032283889230830747\n",
      "Epoch 275/300, Average Loss: 0.03183159188629506\n",
      "Epoch 276/300, Average Loss: 0.031415043081671465\n",
      "Epoch 277/300, Average Loss: 0.031014136675742633\n",
      "Epoch 278/300, Average Loss: 0.03059476261321652\n",
      "Epoch 279/300, Average Loss: 0.030142440603806005\n",
      "Epoch 280/300, Average Loss: 0.02965470042824999\n",
      "Epoch 281/300, Average Loss: 0.029129819494351904\n",
      "Epoch 282/300, Average Loss: 0.02857147597161407\n",
      "Epoch 283/300, Average Loss: 0.02797777387367408\n",
      "Epoch 284/300, Average Loss: 0.027350577728620255\n",
      "Epoch 285/300, Average Loss: 0.026692461617924757\n",
      "Epoch 286/300, Average Loss: 0.02600759685852016\n",
      "Epoch 287/300, Average Loss: 0.025299745287851096\n",
      "Epoch 288/300, Average Loss: 0.02457125121306944\n",
      "Epoch 289/300, Average Loss: 0.023831524492409342\n",
      "Epoch 290/300, Average Loss: 0.0230603790849359\n",
      "Epoch 291/300, Average Loss: 0.021850996976387656\n",
      "Epoch 292/300, Average Loss: 0.020179274151959924\n",
      "Epoch 293/300, Average Loss: 0.019205183574005042\n",
      "Epoch 294/300, Average Loss: 0.018617625321039256\n",
      "Epoch 295/300, Average Loss: 0.01823786646512729\n",
      "Epoch 296/300, Average Loss: 0.01796991673795851\n",
      "Epoch 297/300, Average Loss: 0.01775029918123909\n",
      "Epoch 298/300, Average Loss: 0.017550142289313585\n",
      "Epoch 299/300, Average Loss: 0.017368928507315935\n"
     ]
    }
   ],
   "source": [
    "#training the module\n",
    "r = 7\n",
    "m = 5\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "n_max = 2*r - 1\n",
    "n_iter = 300\n",
    "alpha = 0.001\n",
    "num_of_samples = 250\n",
    "num_train_batches = 10\n",
    "num_test_batches = 1\n",
    "\n",
    "data = get_train_test_sorting(r, m, num_of_samples, num_train_batches,num_test_batches)\n",
    "\n",
    "loss = CrossEntropy()\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1,feed_forward1,attention2, feed_forward2, un_embed_pos, softmax]\n",
    "nueralnetsort = NeuralNetwork(layers)\n",
    "\n",
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "x_t = data['x_test'][0]\n",
    "y_t = data['y_test'][0]\n",
    "\n",
    "per, y_hat = sorting(nueralnetsort, x_t, y_t,m, r)\n",
    "print(f'prosent av antall riktige sorteringer før trening er {per*100}%')\n",
    "for i in range(y_hat.shape[0]):\n",
    "    print(f'xt = {x_t[i]} yt = {y_t[i]} and z = {y_hat[i]}')\n",
    " \n",
    "#arr = algorithm_4_sort(x, y, n_iter, alpha, m, nueralnetsort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 0.] and z = [0. 0. 0. 0. 0.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 0.] and z = [0. 0. 0. 0. 0.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 0. 0.] and z = [0. 0. 0. 0. 0.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 0.] and z = [0. 0. 0. 0. 0.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 0.] and z = [0. 0. 0. 0. 0.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [1. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 0. 1. 1. 1.] and z = [0. 0. 1. 1. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [0. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 0. 1.] and z = [0. 0. 0. 0. 1.]\n",
      "yt = [0. 1. 1. 1. 1.] and z = [1. 1. 1. 1. 1.]\n",
      "yt = [0. 0. 0. 1. 1.] and z = [0. 0. 0. 1. 1.]\n",
      "prosent av antall riktige sorteringer etter trening er 90.8%\n"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(len(arr)),arr)\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Logaritmen av losset')\n",
    "plt.title('Plot av feil for sortering')\n",
    "plt.show()\n",
    "per, z_hat = sorting(nueralnetsort, x_t, y_t,m, r)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "for i in range(250):\n",
    "    print(f'yt = {y_t[i]} and z = {z_hat[i]}')\n",
    "print(f'prosent av antall riktige sorteringer etter trening er {per*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 - Addisjon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapunkter = 250\n",
    "batches = 20\n",
    "\n",
    "d = 30\n",
    "k = 20\n",
    "p = 40\n",
    "L = 3\n",
    "m = 10\n",
    "r = 2\n",
    "n_max = 9\n",
    "n_iter = 150\n",
    "alpha = 0.001\n",
    "\n",
    "data_add = get_train_test_addition(r+1, datapunkter, batches)\n",
    "\n",
    "feed_forward1 = FeedForward(d,p)\n",
    "attention1 = Attention(d,k)\n",
    "feed_forward2 = FeedForward(d,p)\n",
    "attention2 = Attention(d,k)\n",
    "feed_forward3 = FeedForward(d,p)\n",
    "attention3 = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed_pos = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "layers = [embed_pos, attention1,feed_forward1, attention2,feed_forward2, attention3,feed_forward3, un_embed_pos, softmax]\n",
    "nueralnetadd = NeuralNetwork(layers)\n",
    "\n",
    "x_add = data_add['x_train']\n",
    "y_add = data_add['y_train']\n",
    "y_add_t = data_add['y_test'][0]\n",
    "x_add_t = data_add['x_test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/150, Average Loss: 0.8274848446676886\n",
      "Epoch 20/150, Average Loss: 0.7074447681327886\n",
      "Epoch 30/150, Average Loss: 0.6597833592174249\n",
      "Epoch 40/150, Average Loss: 0.652777551137774\n",
      "Epoch 50/150, Average Loss: 0.6486561300065838\n",
      "Epoch 60/150, Average Loss: 0.6429299629514411\n",
      "Epoch 70/150, Average Loss: 0.6276093767492481\n",
      "Epoch 80/150, Average Loss: 0.5823118187921213\n",
      "Epoch 90/150, Average Loss: 0.5490500036646744\n",
      "Epoch 100/150, Average Loss: 0.534552276942425\n",
      "Epoch 110/150, Average Loss: 0.5408016516023457\n",
      "Epoch 120/150, Average Loss: 0.5396911107114852\n",
      "Epoch 130/150, Average Loss: 0.8140022118794457\n",
      "Epoch 140/150, Average Loss: 0.6271675818878961\n",
      "Epoch 150/150, Average Loss: 0.5946213689256854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9827070328821674,\n",
       " 0.965398790784462,\n",
       " 0.9506777968625274,\n",
       " 0.9361254263526503,\n",
       " 0.9208066787650019,\n",
       " 0.9044016995836403,\n",
       " 0.8866808720208565,\n",
       " 0.8677282658832077,\n",
       " 0.8478688281629987,\n",
       " 0.8274848446676886,\n",
       " 0.8073136451748633,\n",
       " 0.7884264673103456,\n",
       " 0.7719392614194549,\n",
       " 0.7586634884385195,\n",
       " 0.7483699737327677,\n",
       " 0.7398231622229595,\n",
       " 0.7317381270296139,\n",
       " 0.7236240718860234,\n",
       " 0.7154926828403425,\n",
       " 0.7074447681327886,\n",
       " 0.6996284405627758,\n",
       " 0.6922454708778882,\n",
       " 0.68552841247771,\n",
       " 0.6796405562556398,\n",
       " 0.6745842223437152,\n",
       " 0.6703102056137724,\n",
       " 0.6667853014834579,\n",
       " 0.6639294704480225,\n",
       " 0.6616310434562641,\n",
       " 0.6597833592174249,\n",
       " 0.658297585233942,\n",
       " 0.6570972347379157,\n",
       " 0.656122455703126,\n",
       " 0.6553291296898175,\n",
       " 0.6546802683394233,\n",
       " 0.6541521372000245,\n",
       " 0.6537224728925584,\n",
       " 0.6533706286792567,\n",
       " 0.6530662072475204,\n",
       " 0.652777551137774,\n",
       " 0.6524827580211001,\n",
       " 0.652165986481005,\n",
       " 0.6518208345081736,\n",
       " 0.6514432778871164,\n",
       " 0.6510352754741308,\n",
       " 0.6505974333634645,\n",
       " 0.6501450503201058,\n",
       " 0.6496766292999957,\n",
       " 0.6491786970739085,\n",
       " 0.6486561300065838,\n",
       " 0.6481068477363501,\n",
       " 0.6475490381095597,\n",
       " 0.6470038884082527,\n",
       " 0.6464690785642002,\n",
       " 0.645941423526448,\n",
       " 0.6453978021933773,\n",
       " 0.6448382763662971,\n",
       " 0.6442452225244535,\n",
       " 0.6436136260642101,\n",
       " 0.6429299629514411,\n",
       " 0.6421742360839918,\n",
       " 0.6413335509833639,\n",
       " 0.6403953651776303,\n",
       " 0.6393284249060212,\n",
       " 0.6380942425416726,\n",
       " 0.6366555383419791,\n",
       " 0.6349534073451145,\n",
       " 0.6329262912070875,\n",
       " 0.6305108176328973,\n",
       " 0.6276093767492481,\n",
       " 0.6241744328768324,\n",
       " 0.6202092180725541,\n",
       " 0.6157287880036636,\n",
       " 0.6107597307483593,\n",
       " 0.6055940556312913,\n",
       " 0.6004739428780902,\n",
       " 0.595574912455463,\n",
       " 0.5909746604474073,\n",
       " 0.586574115663692,\n",
       " 0.5823118187921213,\n",
       " 0.578089940365302,\n",
       " 0.5739721439509928,\n",
       " 0.5700103114490414,\n",
       " 0.5662889063732613,\n",
       " 0.5629006752689734,\n",
       " 0.5597667167619982,\n",
       " 0.5568590597305565,\n",
       " 0.5541018697777627,\n",
       " 0.5515017085940734,\n",
       " 0.5490500036646744,\n",
       " 0.5467126975328928,\n",
       " 0.5444857405749962,\n",
       " 0.5424090662719793,\n",
       " 0.5405376974356634,\n",
       " 0.5389476740915402,\n",
       " 0.5376350626654516,\n",
       " 0.5365841309779371,\n",
       " 0.5357615644440984,\n",
       " 0.5350953402324634,\n",
       " 0.534552276942425,\n",
       " 0.5341249984757503,\n",
       " 0.533805887858802,\n",
       " 0.5336209943324891,\n",
       " 0.5336537153221474,\n",
       " 0.5340475696122844,\n",
       " 0.5350217528155348,\n",
       " 0.5367745650487613,\n",
       " 0.5390248249540662,\n",
       " 0.5407006924173502,\n",
       " 0.5408016516023457,\n",
       " 0.5398293396397315,\n",
       " 0.5386250769931299,\n",
       " 0.5373524790202454,\n",
       " 0.5360833750620881,\n",
       " 0.534948725809424,\n",
       " 0.534639790175401,\n",
       " 0.5350180490313929,\n",
       " 0.5359195694083667,\n",
       " 0.5374232677180364,\n",
       " 0.5396911107114852,\n",
       " 0.5429743885453016,\n",
       " 0.5475504563964226,\n",
       " 0.5538385320017387,\n",
       " 0.5631901745256276,\n",
       " 0.5789491375479746,\n",
       " 0.6080075223338662,\n",
       " 0.6556298588637404,\n",
       " 0.715936078745716,\n",
       " 0.7743097919331531,\n",
       " 0.8140022118794457,\n",
       " 0.8222645832610574,\n",
       " 0.8002868171517876,\n",
       " 0.765006732019007,\n",
       " 0.7307852907210949,\n",
       " 0.7029028838631698,\n",
       " 0.6812360165572118,\n",
       " 0.6638979019427114,\n",
       " 0.6495735243861318,\n",
       " 0.6374374090424231,\n",
       " 0.6271675818878961,\n",
       " 0.618612722285954,\n",
       " 0.6114564415023624,\n",
       " 0.6056240553612832,\n",
       " 0.6012041448119556,\n",
       " 0.5983176514121601,\n",
       " 0.596828202182457,\n",
       " 0.59628975891727,\n",
       " 0.596027573784183,\n",
       " 0.5955006963475373,\n",
       " 0.5946213689256854]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm_4_addition_finished(x_add, y_add, n_iter, alpha, m, nueralnetadd, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prosent av antall riktige addisjoner før trening er 0.0%\n",
      "x = [3 3 9 8 7 0] y = [1 2 0 9], z = [1 1]\n",
      "x = [0 8 7 2 2 8] y = [0 3 1 5], z = [1 0]\n",
      "x = [6 6 8 6 7 2] y = [1 3 4 0], z = [1 0]\n",
      "x = [6 0 6 2 0 3] y = [0 8 0 9], z = [0 7]\n",
      "x = [3 3 3 8 8 0] y = [1 2 1 3], z = [1 1]\n",
      "x = [8 4 2 1 7 9] y = [1 0 2 1], z = [1 0]\n",
      "x = [0 5 7 9 6 3] y = [1 0 2 0], z = [0 0]\n",
      "x = [2 0 3 5 1 6] y = [0 7 1 9], z = [0 0]\n",
      "x = [5 0 2 9 3 8] y = [1 4 4 0], z = [1 0]\n",
      "x = [6 1 7 8 1 9] y = [1 4 3 6], z = [1 0]\n"
     ]
    }
   ],
   "source": [
    "per, z_hat = sorting(nueralnetadd, x_add_t, y_add_t, m,r)\n",
    "print(f'prosent av antall riktige addisjoner før trening er {per*100}%')\n",
    "\n",
    "# reverse the z_hat to get the correct output\n",
    "z_hat = np.flip(z_hat, axis=1)\n",
    "\n",
    "# print first 10 datapoints\n",
    "for i in range(10):\n",
    "    print(f'x = {x_add_t[i]} y = {y_add_t[i]}, z = {z_hat[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prosent av antall riktige addisjoner før trening er 0.0%\n",
      "yt = [0 9 3 8] and z = [7 1 0]\n",
      "yt = [1 2 9 8] and z = [6 2 3]\n",
      "yt = [1 3 7 5] and z = [7 1 0]\n",
      "yt = [0 9 4 8] and z = [7 1 0]\n",
      "yt = [0 9 2 1] and z = [7 1 0]\n",
      "yt = [0 5 7 5] and z = [2 3 0]\n",
      "yt = [1 1 5 3] and z = [3 3 4]\n",
      "yt = [0 6 0 7] and z = [6 3 0]\n",
      "yt = [0 9 2 4] and z = [7 1 0]\n",
      "yt = [1 3 3 3] and z = [7 1 0]\n",
      "yt = [0 8 2 9] and z = [7 1 0]\n",
      "yt = [1 2 1 3] and z = [6 2 3]\n",
      "yt = [0 5 2 3] and z = [2 3 0]\n",
      "yt = [1 3 8 3] and z = [2 3 0]\n",
      "yt = [0 3 6 9] and z = [2 3 0]\n",
      "yt = [1 8 1 7] and z = [2 3 0]\n",
      "yt = [1 8 1 3] and z = [7 1 0]\n",
      "yt = [1 0 0 4] and z = [7 1 0]\n",
      "yt = [0 5 0 2] and z = [2 3 4]\n",
      "yt = [0 9 3 5] and z = [3 3 0]\n",
      "yt = [0 8 8 0] and z = [7 1 0]\n",
      "yt = [1 5 8 2] and z = [6 5 5]\n",
      "yt = [1 2 4 1] and z = [7 1 0]\n",
      "yt = [1 6 4 0] and z = [6 3 0]\n",
      "yt = [1 0 5 6] and z = [2 3 0]\n",
      "yt = [0 8 2 2] and z = [7 1 0]\n",
      "yt = [1 0 9 8] and z = [6 5 5]\n",
      "yt = [0 6 4 0] and z = [6 3 0]\n",
      "yt = [0 8 3 7] and z = [7 1 0]\n",
      "yt = [1 0 7 3] and z = [2 3 0]\n",
      "yt = [1 0 5 0] and z = [6 3 0]\n",
      "yt = [1 5 1 1] and z = [7 1 0]\n",
      "yt = [0 5 2 0] and z = [2 3 0]\n",
      "yt = [0 6 5 0] and z = [7 1 0]\n",
      "yt = [0 8 5 3] and z = [7 1 0]\n",
      "yt = [0 8 7 5] and z = [2 3 0]\n",
      "yt = [1 0 9 4] and z = [7 1 0]\n",
      "yt = [0 6 8 8] and z = [2 3 4]\n",
      "yt = [1 2 0 0] and z = [7 1 0]\n",
      "yt = [1 0 4 8] and z = [2 3 4]\n",
      "yt = [1 5 2 6] and z = [7 1 0]\n",
      "yt = [1 1 7 5] and z = [7 1 0]\n",
      "yt = [0 5 6 9] and z = [7 1 0]\n",
      "yt = [0 9 1 9] and z = [7 1 0]\n",
      "yt = [0 4 8 1] and z = [3 3 0]\n",
      "yt = [0 4 4 4] and z = [2 3 0]\n",
      "yt = [0 8 4 9] and z = [7 1 0]\n",
      "yt = [1 8 2 5] and z = [2 3 0]\n",
      "yt = [0 6 0 9] and z = [7 1 0]\n",
      "yt = [0 8 8 8] and z = [7 1 0]\n",
      "yt = [0 8 9 0] and z = [7 1 0]\n",
      "yt = [1 2 9 5] and z = [2 3 0]\n",
      "yt = [1 7 9 1] and z = [2 3 0]\n",
      "yt = [1 1 2 8] and z = [2 3 0]\n",
      "yt = [0 9 4 6] and z = [7 1 0]\n",
      "yt = [1 3 4 3] and z = [7 1 0]\n",
      "yt = [1 3 1 7] and z = [7 1 6]\n",
      "yt = [1 4 3 7] and z = [6 3 0]\n",
      "yt = [0 4 9 3] and z = [2 3 0]\n",
      "yt = [0 5 9 0] and z = [7 1 0]\n",
      "yt = [0 4 7 5] and z = [6 3 5]\n",
      "yt = [0 4 1 1] and z = [7 1 0]\n",
      "yt = [1 2 3 1] and z = [7 1 0]\n",
      "yt = [1 2 5 6] and z = [7 1 0]\n",
      "yt = [1 2 3 1] and z = [7 1 0]\n",
      "yt = [1 7 5 8] and z = [7 1 0]\n",
      "yt = [0 6 9 9] and z = [7 1 0]\n",
      "yt = [0 9 5 4] and z = [7 1 0]\n",
      "yt = [0 9 6 1] and z = [2 3 0]\n",
      "yt = [0 2 6 5] and z = [6 3 0]\n",
      "yt = [1 1 1 1] and z = [7 1 0]\n",
      "yt = [1 0 4 8] and z = [2 3 0]\n",
      "yt = [1 5 1 2] and z = [2 3 4]\n",
      "yt = [0 8 9 5] and z = [7 1 0]\n",
      "yt = [1 7 4 7] and z = [2 3 0]\n",
      "yt = [1 1 6 0] and z = [7 1 0]\n",
      "yt = [1 1 9 1] and z = [6 6 6]\n",
      "yt = [0 3 5 9] and z = [6 3 0]\n",
      "yt = [0 9 4 5] and z = [6 3 0]\n",
      "yt = [0 7 7 3] and z = [6 3 0]\n",
      "yt = [1 0 1 8] and z = [2 3 0]\n",
      "yt = [1 2 3 7] and z = [6 3 0]\n",
      "yt = [0 6 4 5] and z = [7 1 0]\n",
      "yt = [0 8 3 3] and z = [7 1 6]\n",
      "yt = [1 4 1 0] and z = [7 1 0]\n",
      "yt = [0 9 2 2] and z = [7 1 0]\n",
      "yt = [1 2 2 7] and z = [2 3 0]\n",
      "yt = [1 2 3 0] and z = [6 3 0]\n",
      "yt = [1 0 7 1] and z = [3 3 0]\n",
      "yt = [0 6 6 3] and z = [7 1 0]\n",
      "yt = [0 9 4 9] and z = [7 1 0]\n",
      "yt = [0 4 2 2] and z = [2 3 4]\n",
      "yt = [0 5 2 2] and z = [7 1 0]\n",
      "yt = [0 8 1 3] and z = [2 3 0]\n",
      "yt = [1 1 3 9] and z = [7 1 0]\n",
      "yt = [0 5 8 4] and z = [7 1 0]\n",
      "yt = [1 6 5 2] and z = [6 3 0]\n",
      "yt = [0 8 8 2] and z = [7 1 0]\n",
      "yt = [0 9 0 2] and z = [7 1 0]\n",
      "yt = [0 7 4 8] and z = [2 3 4]\n",
      "yt = [1 3 7 0] and z = [2 3 0]\n",
      "yt = [0 7 1 2] and z = [7 1 0]\n",
      "yt = [0 7 9 9] and z = [7 1 0]\n",
      "yt = [0 9 9 5] and z = [7 1 0]\n",
      "yt = [1 7 4 0] and z = [7 1 0]\n",
      "yt = [1 5 9 0] and z = [7 1 0]\n",
      "yt = [0 6 4 1] and z = [7 1 0]\n",
      "yt = [0 5 5 8] and z = [2 3 0]\n",
      "yt = [0 7 0 5] and z = [2 3 0]\n",
      "yt = [0 9 0 2] and z = [7 1 0]\n",
      "yt = [1 3 8 0] and z = [2 3 0]\n",
      "yt = [0 8 8 9] and z = [7 1 0]\n",
      "yt = [1 2 8 5] and z = [7 1 0]\n",
      "yt = [0 8 9 2] and z = [6 3 0]\n",
      "yt = [1 3 4 6] and z = [7 1 0]\n",
      "yt = [1 4 8 9] and z = [7 1 0]\n",
      "yt = [0 7 6 3] and z = [2 3 0]\n",
      "yt = [1 2 6 1] and z = [7 1 0]\n",
      "yt = [0 8 5 0] and z = [7 1 0]\n",
      "yt = [1 5 7 4] and z = [7 1 0]\n",
      "yt = [0 8 4 3] and z = [7 1 0]\n",
      "yt = [0 4 2 5] and z = [1 6 0]\n",
      "yt = [0 8 4 9] and z = [7 1 0]\n",
      "yt = [0 8 0 5] and z = [7 1 0]\n",
      "yt = [1 1 7 0] and z = [6 3 4]\n",
      "yt = [0 9 2 7] and z = [2 3 0]\n",
      "yt = [1 0 7 1] and z = [6 6 6]\n",
      "yt = [1 9 1 6] and z = [7 1 0]\n",
      "yt = [1 7 4 8] and z = [1 6 0]\n",
      "yt = [1 1 3 4] and z = [7 1 0]\n",
      "yt = [0 7 9 8] and z = [7 1 0]\n",
      "yt = [1 3 5 3] and z = [2 3 0]\n",
      "yt = [0 4 4 5] and z = [7 1 0]\n",
      "yt = [1 4 4 1] and z = [2 3 0]\n",
      "yt = [0 8 8 3] and z = [2 3 0]\n",
      "yt = [0 3 7 5] and z = [7 1 0]\n",
      "yt = [0 7 7 2] and z = [7 1 0]\n",
      "yt = [1 0 8 6] and z = [2 3 0]\n",
      "yt = [1 0 4 1] and z = [7 1 0]\n",
      "yt = [1 0 7 9] and z = [7 1 0]\n",
      "yt = [0 8 7 4] and z = [7 1 0]\n",
      "yt = [1 3 3 4] and z = [7 1 0]\n",
      "yt = [0 4 8 0] and z = [7 1 0]\n",
      "yt = [1 7 3 3] and z = [7 1 0]\n",
      "yt = [0 4 8 7] and z = [7 1 0]\n",
      "yt = [1 4 9 5] and z = [7 1 6]\n",
      "yt = [1 7 2 4] and z = [7 1 0]\n",
      "yt = [0 6 3 4] and z = [7 1 0]\n",
      "yt = [0 4 4 0] and z = [7 1 0]\n",
      "yt = [0 0 8 2] and z = [7 1 0]\n",
      "yt = [0 8 0 0] and z = [2 3 0]\n",
      "yt = [0 6 2 2] and z = [7 1 0]\n",
      "yt = [0 9 3 7] and z = [1 6 0]\n",
      "yt = [1 6 6 6] and z = [7 1 0]\n",
      "yt = [0 3 9 0] and z = [7 1 0]\n",
      "yt = [1 7 5 9] and z = [7 1 0]\n",
      "yt = [0 3 5 4] and z = [3 3 0]\n",
      "yt = [1 1 1 4] and z = [2 3 0]\n",
      "yt = [0 2 6 4] and z = [2 3 0]\n",
      "yt = [1 2 7 4] and z = [7 1 0]\n",
      "yt = [1 0 6 6] and z = [2 3 0]\n",
      "yt = [1 0 8 2] and z = [2 3 0]\n",
      "yt = [0 8 0 2] and z = [7 1 0]\n",
      "yt = [0 1 5 5] and z = [1 6 0]\n",
      "yt = [1 0 0 1] and z = [6 3 4]\n",
      "yt = [1 4 6 2] and z = [7 1 0]\n",
      "yt = [0 4 4 0] and z = [7 1 0]\n",
      "yt = [0 9 5 3] and z = [7 1 0]\n",
      "yt = [1 5 0 3] and z = [7 1 0]\n",
      "yt = [1 3 4 3] and z = [2 3 0]\n",
      "yt = [1 2 3 0] and z = [7 1 0]\n",
      "yt = [1 1 4 9] and z = [6 3 0]\n",
      "yt = [1 3 0 3] and z = [3 3 0]\n",
      "yt = [0 7 3 0] and z = [2 3 4]\n",
      "yt = [0 5 5 5] and z = [7 1 0]\n",
      "yt = [0 7 8 2] and z = [3 3 0]\n",
      "yt = [1 2 3 1] and z = [7 1 0]\n",
      "yt = [0 7 1 9] and z = [7 1 0]\n",
      "yt = [0 7 5 7] and z = [7 1 0]\n",
      "yt = [0 9 1 9] and z = [2 3 0]\n",
      "yt = [1 1 4 1] and z = [7 1 0]\n",
      "yt = [0 7 4 6] and z = [7 1 0]\n",
      "yt = [0 6 8 8] and z = [7 1 0]\n",
      "yt = [1 1 3 3] and z = [7 1 0]\n",
      "yt = [0 4 9 9] and z = [7 1 0]\n",
      "yt = [0 2 9 8] and z = [3 3 4]\n",
      "yt = [1 4 1 8] and z = [7 1 0]\n",
      "yt = [1 1 0 7] and z = [7 1 0]\n",
      "yt = [0 9 6 3] and z = [7 1 0]\n",
      "yt = [1 5 8 0] and z = [6 3 5]\n",
      "yt = [1 6 2 7] and z = [7 1 0]\n",
      "yt = [1 8 1 3] and z = [1 6 6]\n",
      "yt = [0 7 0 3] and z = [6 3 0]\n",
      "yt = [0 4 8 8] and z = [7 1 0]\n",
      "yt = [0 9 9 2] and z = [7 1 0]\n",
      "yt = [0 5 4 5] and z = [7 1 0]\n",
      "yt = [1 0 3 6] and z = [7 1 0]\n",
      "yt = [1 5 9 9] and z = [7 1 0]\n",
      "yt = [1 0 7 2] and z = [2 3 4]\n",
      "yt = [0 7 1 0] and z = [2 3 0]\n",
      "yt = [0 3 8 5] and z = [7 1 0]\n",
      "yt = [1 1 9 0] and z = [7 1 0]\n",
      "yt = [1 2 7 7] and z = [2 3 0]\n",
      "yt = [1 1 0 9] and z = [7 1 0]\n",
      "yt = [1 6 4 0] and z = [2 3 0]\n",
      "yt = [1 6 2 3] and z = [7 1 0]\n",
      "yt = [1 3 7 1] and z = [7 1 0]\n",
      "yt = [0 9 6 3] and z = [7 1 0]\n",
      "yt = [1 4 6 0] and z = [2 3 0]\n",
      "yt = [0 5 4 4] and z = [7 1 0]\n",
      "yt = [1 5 8 8] and z = [7 1 0]\n",
      "yt = [1 3 1 4] and z = [7 1 0]\n",
      "yt = [1 4 2 5] and z = [2 3 0]\n",
      "yt = [0 2 0 0] and z = [3 3 0]\n",
      "yt = [0 8 6 9] and z = [7 1 0]\n",
      "yt = [1 0 6 4] and z = [7 1 0]\n",
      "yt = [1 6 1 5] and z = [6 3 0]\n",
      "yt = [0 9 0 4] and z = [2 3 0]\n",
      "yt = [1 0 1 9] and z = [7 1 0]\n",
      "yt = [0 3 6 2] and z = [7 1 0]\n",
      "yt = [1 5 1 2] and z = [6 3 0]\n",
      "yt = [0 6 5 6] and z = [3 3 0]\n",
      "yt = [1 2 5 1] and z = [2 3 0]\n",
      "yt = [0 6 0 6] and z = [2 3 4]\n",
      "yt = [1 4 8 2] and z = [7 1 0]\n",
      "yt = [0 1 2 6] and z = [7 1 0]\n",
      "yt = [0 9 2 0] and z = [7 1 0]\n",
      "yt = [0 5 0 9] and z = [7 1 0]\n",
      "yt = [1 5 2 8] and z = [7 1 0]\n",
      "yt = [0 6 9 7] and z = [2 3 0]\n",
      "yt = [0 9 3 5] and z = [7 1 0]\n",
      "yt = [1 0 3 8] and z = [2 3 0]\n",
      "yt = [1 0 4 4] and z = [7 1 0]\n",
      "yt = [0 4 8 3] and z = [7 1 0]\n",
      "yt = [0 9 4 4] and z = [7 1 0]\n",
      "yt = [0 5 9 5] and z = [7 1 0]\n",
      "yt = [1 1 7 4] and z = [7 1 0]\n",
      "yt = [1 0 0 6] and z = [7 1 0]\n",
      "yt = [0 6 9 3] and z = [3 3 0]\n",
      "yt = [0 2 3 9] and z = [2 3 0]\n",
      "yt = [0 2 6 3] and z = [7 1 0]\n",
      "yt = [0 4 0 8] and z = [7 1 0]\n",
      "yt = [0 9 8 3] and z = [7 1 0]\n",
      "yt = [1 4 9 7] and z = [7 1 0]\n",
      "yt = [1 0 1 2] and z = [7 1 0]\n",
      "yt = [1 4 6 5] and z = [7 1 0]\n",
      "yt = [0 7 6 7] and z = [7 1 0]\n",
      "yt = [0 7 5 9] and z = [7 1 0]\n",
      "yt = [1 3 5 3] and z = [6 3 0]\n",
      "yt = [1 4 6 8] and z = [3 3 0]\n",
      "Epoch 1/150, Average Loss: 0.9952429932072802\n",
      "Epoch 2/150, Average Loss: 0.9794750052300909\n",
      "Epoch 3/150, Average Loss: 0.9604710335173143\n",
      "Epoch 4/150, Average Loss: 0.935536141674632\n",
      "Epoch 5/150, Average Loss: 0.908277390543933\n",
      "Epoch 6/150, Average Loss: 0.8875604290804577\n",
      "Epoch 7/150, Average Loss: 0.8672896432031234\n",
      "Epoch 8/150, Average Loss: 0.8442050280243502\n",
      "Epoch 9/150, Average Loss: 0.8200397501807488\n",
      "Epoch 10/150, Average Loss: 0.7985480372129026\n",
      "Epoch 11/150, Average Loss: 0.7833429632962837\n",
      "Epoch 12/150, Average Loss: 0.7774236165141816\n",
      "Epoch 13/150, Average Loss: 0.7748988630939537\n",
      "Epoch 14/150, Average Loss: 0.7732367748786421\n",
      "Epoch 15/150, Average Loss: 0.7719427104655263\n",
      "Epoch 16/150, Average Loss: 0.7707250518450887\n",
      "Epoch 17/150, Average Loss: 0.7694869494833713\n",
      "Epoch 18/150, Average Loss: 0.7682029321130475\n",
      "Epoch 19/150, Average Loss: 0.7667760622158093\n",
      "Epoch 20/150, Average Loss: 0.7651853329287704\n",
      "Epoch 21/150, Average Loss: 0.7634047453735258\n",
      "Epoch 22/150, Average Loss: 0.7614041154817379\n",
      "Epoch 23/150, Average Loss: 0.7591149971993051\n",
      "Epoch 24/150, Average Loss: 0.7564255101863557\n",
      "Epoch 25/150, Average Loss: 0.7533154938685638\n",
      "Epoch 26/150, Average Loss: 0.749853831959644\n",
      "Epoch 27/150, Average Loss: 0.7461978933247464\n",
      "Epoch 28/150, Average Loss: 0.7424501066825148\n",
      "Epoch 29/150, Average Loss: 0.7386798763173038\n",
      "Epoch 30/150, Average Loss: 0.7349625990203268\n",
      "Epoch 31/150, Average Loss: 0.7313362780912303\n",
      "Epoch 32/150, Average Loss: 0.7278122816394407\n",
      "Epoch 33/150, Average Loss: 0.7244646262970904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y_add_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myt = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_add_t[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and z = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz_hat[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m arr3 \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm_4_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_add\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_add\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnueralnetadd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/UNI 3/TMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregninger/Projects.nosync/TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter/Prosjekt2-IndustriellMatematikk/training.py:77\u001b[0m, in \u001b[0;36malgorithm_4_add\u001b[0;34m(x, y, n_iter, alpha, m, r, neuralnet)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     76\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m onehot(x[k], m)\n\u001b[0;32m---> 77\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[43mneuralnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m neuralnet\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mforward(Z, y[k][:,\u001b[38;5;241m-\u001b[39m(r\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):])\n\u001b[1;32m     79\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mappend(batch_loss)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/UNI 3/TMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregninger/Projects.nosync/TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter/Prosjekt2-IndustriellMatematikk/neural_network.py:18\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#Recursively perform forward pass from initial input x\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 18\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/UNI 3/TMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregninger/Projects.nosync/TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter/Prosjekt2-IndustriellMatematikk/layers.py:137\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD[i1, i2] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf  \u001b[38;5;66;03m# creates D matrix\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    127\u001b[0m     np\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbij, jn, nk, bkt->bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD\n\u001b[1;32m    136\u001b[0m )\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43min, nj, ajk ,akt -> ait\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Bytta k på slutten med t\u001b[39;49;00m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW_O\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW_V\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/UNI 3/TMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregninger/Projects.nosync/TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter/.venv/lib/python3.10/site-packages/numpy/core/einsumfunc.py:1419\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     right_pos\u001b[38;5;241m.\u001b[39mappend(input_right\u001b[38;5;241m.\u001b[39mfind(s))\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# Contract!\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m new_view \u001b[38;5;241m=\u001b[39m \u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mright_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (tensor_result \u001b[38;5;241m!=\u001b[39m results_index) \u001b[38;5;129;01mor\u001b[39;00m handle_out:\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/UNI 3/TMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregningerTMA4320 Introduksjon til vitenskapelige beregninger/Projects.nosync/TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter/.venv/lib/python3.10/site-packages/numpy/core/numeric.py:1121\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1119\u001b[0m at \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose(newaxes_a)\u001b[38;5;241m.\u001b[39mreshape(newshape_a)\n\u001b[1;32m   1120\u001b[0m bt \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mtranspose(newaxes_b)\u001b[38;5;241m.\u001b[39mreshape(newshape_b)\n\u001b[0;32m-> 1121\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mreshape(olda \u001b[38;5;241m+\u001b[39m oldb)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "per, z_hat = sorting(nueralnetadd, x_add_t, y_add_t, m,r+1)\n",
    "print(f'prosent av antall riktige addisjoner før trening er {per*100}%')\n",
    "\n",
    "for i in range(y_add_t.shape[0]):\n",
    "    print(f'yt = {y_add_t[i]} and z = {z_hat[i]}')\n",
    "\n",
    "#arr3 = algorithm_4_add(x_add, y_add, n_iter, alpha, m,r,  nueralnetadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[43marr3\u001b[49m)),arr3)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIterasjoner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogaritmen av losset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'arr3' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(len(arr3)),arr3)\n",
    "plt.xlabel('Iterasjoner')\n",
    "plt.ylabel('Logaritmen av losset')\n",
    "plt.title('Plot av feil for addisjon')\n",
    "plt.show()\n",
    "\n",
    "y_add_t = data_add['y_test'][0]\n",
    "x_add_t = data_add['x_test'][0]\n",
    "\n",
    "per, z_hat = sorting(nueralnetadd, x_add_t, y_add_t,m,r+1)\n",
    "print(f'prosent av antall riktige addisjoner er {per*100}%')\n",
    "\n",
    "for i in range(y_add_t.shape[0]):\n",
    "    #print(f'x train {x[0][i]} y train{y[0][i]}')\n",
    "    print(f'yt = {y_add_t[i]} and z = {z_hat[i]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
