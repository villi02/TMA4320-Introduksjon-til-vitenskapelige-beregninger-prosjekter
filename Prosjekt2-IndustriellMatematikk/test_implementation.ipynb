{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test om koden er riktig implementert\n",
    "\n",
    "Her er et forslag til testfunksjoner for Ã¥ sjekke om koden er riktig implementert.\n",
    "```assert variabel``` vil gi en feilmelding med mindre variabelen ```variabel = True```. For eksempel vil ```assert a == b``` gi en feilmelding med mindre ```a``` og ```b``` er like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "attention = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)\n",
    "\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test the forward pass\n",
    "x = np.random.randint(0, m, (b,n_max))\n",
    "X = onehot(x, m)\n",
    "\n",
    "#we test with a y that is shorter than the maximum length\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0, m, (b,n_y))\n",
    "\n",
    "#initialize a neural network based on the layers above\n",
    "network = NeuralNetwork([embed_pos, feed_forward, attention, un_embed, softmax])\n",
    "#and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#do a forward pass\n",
    "Z = network.forward(X)\n",
    "\n",
    "#compute the loss\n",
    "L = loss.forward(Z, y)\n",
    "\n",
    "#get the derivative of the loss wrt Z\n",
    "grad_Z = loss.backward()\n",
    "#and perform a backward pass\n",
    "_ = network.backward(grad_Z)\n",
    "\n",
    "#and and do a gradient descent step\n",
    "_ = network.step_gd(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere you may add additional tests to for example:\\n- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\\n- Check if the parameters change when you perform a gradient descent step\\n- Check if the loss decreases when you perform a gradient descent step\\n\\nThis is voluntary, but could be useful.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for iteration1: 1.4913976189956055\n",
      "loss for iteration2: 1.4913976189956055\n",
      "loss for iteration3: 1.4913976189956055\n",
      "loss for iteration4: 1.4913976189956055\n",
      "loss for iteration5: 1.4913976189956055\n",
      "loss for iteration6: 1.4913976189956055\n",
      "loss for iteration7: 1.4913976189956055\n",
      "loss for iteration8: 1.4913976189956055\n",
      "loss for iteration9: 1.4913976189956055\n",
      "loss for iteration10: 1.4913976189956055\n",
      "loss for iteration11: 1.4913976189956055\n",
      "loss for iteration12: 1.4913976189956055\n",
      "loss for iteration13: 1.4913976189956055\n",
      "loss for iteration14: 1.4913976189956055\n",
      "loss for iteration15: 1.4913976189956055\n",
      "loss for iteration16: 1.4913976189956055\n",
      "loss for iteration17: 1.4913976189956055\n",
      "loss for iteration18: 1.4913976189956055\n",
      "loss for iteration19: 1.4913976189956055\n",
      "loss for iteration20: 1.4913976189956055\n",
      "loss for iteration21: 1.4913976189956055\n",
      "loss for iteration22: 1.4913976189956055\n",
      "loss for iteration23: 1.4913976189956055\n",
      "loss for iteration24: 1.4913976189956055\n",
      "loss for iteration25: 1.4913976189956055\n",
      "loss for iteration26: 1.4913976189956055\n",
      "loss for iteration27: 1.4913976189956055\n",
      "loss for iteration28: 1.4913976189956055\n",
      "loss for iteration29: 1.4913976189956055\n",
      "loss for iteration30: 1.4913976189956055\n",
      "loss for iteration31: 1.4913976189956055\n",
      "loss for iteration32: 1.4913976189956055\n",
      "loss for iteration33: 1.4913976189956055\n",
      "loss for iteration34: 1.4913976189956055\n",
      "loss for iteration35: 1.4913976189956055\n",
      "loss for iteration36: 1.4913976189956055\n",
      "loss for iteration37: 1.4913976189956055\n",
      "loss for iteration38: 1.4913976189956055\n",
      "loss for iteration39: 1.4913976189956055\n",
      "loss for iteration40: 1.4913976189956055\n",
      "loss for iteration41: 1.4913976189956055\n",
      "loss for iteration42: 1.4913976189956055\n",
      "loss for iteration43: 1.4913976189956055\n",
      "loss for iteration44: 1.4913976189956055\n",
      "loss for iteration45: 1.4913976189956055\n",
      "loss for iteration46: 1.4913976189956055\n",
      "loss for iteration47: 1.4913976189956055\n",
      "loss for iteration48: 1.4913976189956055\n",
      "loss for iteration49: 1.4913976189956055\n",
      "loss for iteration50: 1.4913976189956055\n",
      "loss for iteration51: 1.4913976189956055\n",
      "loss for iteration52: 1.4913976189956055\n",
      "loss for iteration53: 1.4913976189956055\n",
      "loss for iteration54: 1.4913976189956055\n",
      "loss for iteration55: 1.4913976189956055\n",
      "loss for iteration56: 1.4913976189956055\n",
      "loss for iteration57: 1.4913976189956055\n",
      "loss for iteration58: 1.4913976189956055\n",
      "loss for iteration59: 1.4913976189956055\n",
      "loss for iteration60: 1.4913976189956055\n",
      "loss for iteration61: 1.4913976189956055\n",
      "loss for iteration62: 1.4913976189956055\n",
      "loss for iteration63: 1.4913976189956055\n",
      "loss for iteration64: 1.4913976189956055\n",
      "loss for iteration65: 1.4913976189956055\n",
      "loss for iteration66: 1.4913976189956055\n",
      "loss for iteration67: 1.4913976189956055\n",
      "loss for iteration68: 1.4913976189956055\n",
      "loss for iteration69: 1.4913976189956055\n",
      "loss for iteration70: 1.4913976189956055\n",
      "loss for iteration71: 1.4913976189956055\n",
      "loss for iteration72: 1.4913976189956055\n",
      "loss for iteration73: 1.4913976189956055\n",
      "loss for iteration74: 1.4913976189956055\n",
      "loss for iteration75: 1.4913976189956055\n",
      "loss for iteration76: 1.4913976189956055\n",
      "loss for iteration77: 1.4913976189956055\n",
      "loss for iteration78: 1.4913976189956055\n",
      "loss for iteration79: 1.4913976189956055\n",
      "loss for iteration80: 1.4913976189956055\n",
      "loss for iteration81: 1.4913976189956055\n",
      "loss for iteration82: 1.4913976189956055\n",
      "loss for iteration83: 1.4913976189956055\n",
      "loss for iteration84: 1.4913976189956055\n",
      "loss for iteration85: 1.4913976189956055\n",
      "loss for iteration86: 1.4913976189956055\n",
      "loss for iteration87: 1.4913976189956055\n",
      "loss for iteration88: 1.4913976189956055\n",
      "loss for iteration89: 1.4913976189956055\n",
      "loss for iteration90: 1.4913976189956055\n",
      "loss for iteration91: 1.4913976189956055\n",
      "loss for iteration92: 1.4913976189956055\n",
      "loss for iteration93: 1.4913976189956055\n",
      "loss for iteration94: 1.4913976189956055\n",
      "loss for iteration95: 1.4913976189956055\n",
      "loss for iteration96: 1.4913976189956055\n",
      "loss for iteration97: 1.4913976189956055\n",
      "loss for iteration98: 1.4913976189956055\n",
      "loss for iteration99: 1.4913976189956055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed_forward1 = FeedForward(d,p)\n",
    "# attention1 = Attention(d,k)\n",
    "# embed_pos = EmbedPosition(n_max,m,d)\n",
    "# un_embed_pos = LinearLayer(d,m)\n",
    "# softmax = Softmax()\n",
    "# layers = [embed_pos,attention1,feed_forward1,un_embed_pos, softmax]\n",
    "# nueralnet = NeuralNetwork(layers)\n",
    "\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "x = np.random.randint(0, m, (b,n_max))\n",
    "y = np.random.randint(0, m, (b,n_max-1))\n",
    "\n",
    "def algorithm_4(x, y, m , d, p, k, n_max):\n",
    "    n_iter = 100\n",
    "    loss = CrossEntropy()\n",
    "    L_arr = np.zeros(n_iter)\n",
    "    feed_forward1 = FeedForward(d,p)\n",
    "    attention1 = Attention(d,k)\n",
    "    embed_pos = EmbedPosition(n_max,m,d)\n",
    "    un_embed_pos = LinearLayer(d,m)\n",
    "    softmax = Softmax()\n",
    "    layers = [embed_pos, attention1,feed_forward1, un_embed_pos, softmax]\n",
    "    nueralnet = NeuralNetwork(layers)\n",
    "\n",
    "    alpha = 0\n",
    "    for j in range(1,n_iter):\n",
    "        Losses = []\n",
    "        for k in range(x.shape[0]):\n",
    "            X = onehot(x, m)\n",
    "            nueralnet.forward(X) \n",
    "            Losses.append(loss.forward(Z,y))\n",
    "            dLdz = loss.backward()\n",
    "            nueralnet.backward(dLdz)\n",
    "        L_arr[j] = L \n",
    "        print(f'loss for iteration{j}: {loss.forward(Z,y)}') \n",
    "    return L_arr[:-1]   \n",
    "\n",
    "algorithm_4(x, y, m , d, p, k, n_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
