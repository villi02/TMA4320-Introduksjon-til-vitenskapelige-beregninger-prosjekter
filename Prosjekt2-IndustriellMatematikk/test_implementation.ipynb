{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test om koden er riktig implementert\n",
    "\n",
    "Her er et forslag til testfunksjoner for å sjekke om koden er riktig implementert.\n",
    "```assert variabel``` vil gi en feilmelding med mindre variabelen ```variabel = True```. For eksempel vil ```assert a == b``` gi en feilmelding med mindre ```a``` og ```b``` er like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_generators import get_train_test_sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "attention = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)\n",
    "\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test the forward pass\n",
    "x = np.random.randint(0, m, (b,n_max))\n",
    "X = onehot(x, m)\n",
    "\n",
    "#we test with a y that is shorter than the maximum length\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0, m, (b,n_y))\n",
    "\n",
    "#initialize a neural network based on the layers above\n",
    "network = NeuralNetwork([embed_pos, feed_forward, attention, un_embed, softmax])\n",
    "#and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#do a forward pass\n",
    "Z = network.forward(X)\n",
    "\n",
    "#compute the loss\n",
    "L = loss.forward(Z, y)\n",
    "\n",
    "#get the derivative of the loss wrt Z\n",
    "grad_Z = loss.backward()\n",
    "#and perform a backward pass\n",
    "_ = network.backward(grad_Z)\n",
    "\n",
    "#and and do a gradient descent step\n",
    "_ = network.step_gd(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere you may add additional tests to for example:\\n- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\\n- Check if the parameters change when you perform a gradient descent step\\n- Check if the loss decreases when you perform a gradient descent step\\n\\nThis is voluntary, but could be useful.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unittesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her er en rekke unittester for diverse deler av prosjektet som vi programmerte, samt som vi tenkte kunne være lurt å sjekke mens vi holdt på. Noen av disse inneholder metoder fra Tensorflow, som ble brukt til å måle våre egne metoder mot. Kunstig intelligens ble brukt til å generere utkast av disse testene, samt ble brukt til idemyldring for hvilke tester som kunne vært lurt å gjennomføre, vi merket i de fleste tilfeller at var det nødvendig for oss å endre testene som kunstig intelligens genererte for å faktisk få dem til å fungere for vårt prosjekt spesifikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_backward (__main__.TestAttention) ... ok\n",
      "test_parameter_update (__main__.TestAttention) ... ok\n",
      "test_backward_shape (__main__.TestCrossEntropyLayer) ... ok\n",
      "test_backward_shape (__main__.TestSoftmaxLayer) ... ok\n",
      "test_forward_output (__main__.TestSoftmaxLayer) ... ok\n",
      "test_forward_output_example (__main__.TestSoftmaxLayer) ... ok\n",
      "test_gradients (__main__.TestSoftmaxLayer) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.139s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x2cea6c940>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "from neural_network import NeuralNetwork\n",
    "import layers\n",
    "import tensorflow as tf\n",
    "from utils import onehot\n",
    "from tensorflow.keras.layers import Layer, Softmax, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from data_generators import get_train_test_sorting\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming Attention, LinearLayer, and Softmax are defined in attention_module\n",
    "class AttentionTF(Layer):\n",
    "\n",
    "    def __init__(self, d, k, **kwargs):\n",
    "        super(layers.Attention, self).__init__(**kwargs)\n",
    "        # Initializing weights\n",
    "        self.W_O = self.add_weight(\n",
    "            name=\"W_O\", shape=(k, d), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.W_V = self.add_weight(\n",
    "            name=\"W_V\", shape=(k, d), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.W_K = self.add_weight(\n",
    "            name=\"W_K\", shape=(k, d), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.W_Q = self.add_weight(\n",
    "            name=\"W_Q\", shape=(k, d), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        n = tf.shape(inputs)[2]\n",
    "        b = tf.shape(inputs)[0]\n",
    "        x_transpose = tf.transpose(inputs, perm=[0, 2, 1])\n",
    "\n",
    "        D = tf.zeros((n, n))\n",
    "        i1, i2 = tf.linalg.band_part(tf.ones((n, n)), -1, 0) - 1\n",
    "        D = tf.where(i1 == 0, x=tf.constant(-float(\"inf\")), y=D)\n",
    "\n",
    "        A = self.softmax(\n",
    "            tf.einsum(\"bij,jn,nk,bkt->bit\", x_transpose, self.W_Q, self.W_K, inputs) + D\n",
    "        )\n",
    "        output = inputs + tf.einsum(\n",
    "            \"in, nj, ajk,akt->aik\", self.W_O, self.W_V, inputs, A\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TestAttention(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Dimensions for the test\n",
    "        self.d = 4  # Dimension of input\n",
    "        self.k = 3  # Dimension of output\n",
    "        self.batch_size = 2\n",
    "        self.seq_length = 5\n",
    "\n",
    "        # Initialize the Attention layer\n",
    "        self.attention = layers.Attention(self.d, self.k)\n",
    "\n",
    "        # Generate a random input of shape (batch_size, d, seq_length)\n",
    "        self.input = np.random.rand(self.batch_size, self.d, self.seq_length)\n",
    "\n",
    "        # Generate a random gradient of the same shape as input for backward pass\n",
    "        self.grad_output = np.random.rand(self.batch_size, self.d, self.seq_length)\n",
    "\n",
    "    def test_forward_backward(self):\n",
    "        # Forward pass\n",
    "        output = self.attention.forward(self.input)\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.d, self.seq_length))\n",
    "\n",
    "        # Backward pass\n",
    "        grad_input = self.attention.backward(self.grad_output)\n",
    "        self.assertEqual(grad_input.shape, (self.batch_size, self.d, self.seq_length))\n",
    "\n",
    "    def test_parameter_update(self):\n",
    "        # Perform a forward and backward pass\n",
    "        self.attention.forward(self.input)\n",
    "        self.attention.backward(self.grad_output)\n",
    "\n",
    "        # Store old parameters for comparison\n",
    "        old_params = {\n",
    "            key: param[\"w\"].copy() for key, param in self.attention.params.items()\n",
    "        }\n",
    "\n",
    "        # Simple gradient descent update\n",
    "        learning_rate = 0.01\n",
    "        for param in self.attention.params.values():\n",
    "            param[\"w\"] -= learning_rate * param[\"d\"]\n",
    "\n",
    "        # Check if parameters are updated (not equal to old parameters)\n",
    "        for key, old_w in old_params.items():\n",
    "            new_w = self.attention.params[key][\"w\"]\n",
    "            with self.subTest(param=key):\n",
    "                self.assertFalse(\n",
    "                    np.array_equal(old_w, new_w), f\"Parameter {key} was not updated\"\n",
    "                )\n",
    "\n",
    "\n",
    "class TestAttentionLayer:\n",
    "    def setUp(self):\n",
    "        r = 5\n",
    "        m = 2\n",
    "        num_of_samples = 250\n",
    "        num_train_batches = 10\n",
    "        num_test_batches = 1\n",
    "        data = get_train_test_sorting(\n",
    "            r, m, num_of_samples, num_train_batches, num_test_batches\n",
    "        )\n",
    "        self.x = data[\"x_train\"]\n",
    "        self.y = data[\"y_train\"]\n",
    "        self.custom_attention_layer = layers.Attention(d=10, k=5)\n",
    "        self.tf_attention_layer = AttentionTF(d=10, k=5)\n",
    "\n",
    "    def test_attention_output(self):\n",
    "        # Process input through custom attention layer\n",
    "        custom_output = self.custom_attention_layer.forward(self.x)\n",
    "\n",
    "        # Process input through TensorFlow attention layer\n",
    "        # Note: This assumes AttentionTF is integrated within a TensorFlow model\n",
    "        inputs = Input(shape=(None, self.x.shape[-1]))\n",
    "        attention_output = AttentionTF(d=10, k=5)(inputs)\n",
    "        model = Model(inputs=inputs, outputs=attention_output)\n",
    "        tf_output = model.predict(self.x)\n",
    "\n",
    "        # Verify outputs are close enough\n",
    "        np.testing.assert_almost_equal(custom_output, tf_output, decimal=5)\n",
    "\n",
    "\n",
    "class TestSoftmaxLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.softmax = layers.Softmax()\n",
    "        self.input_data = np.random.randn(3, 5)  # Batch size of 3, 5 classes\n",
    "        self.grad_output = np.random.randn(3, 5)\n",
    "\n",
    "    def test_forward_output(self):\n",
    "        output = self.softmax.forward(self.input_data)\n",
    "        # Check if softmax output is correctly normalized\n",
    "        for row in output:\n",
    "            self.assertAlmostEqual(np.sum(row), 1.0)\n",
    "\n",
    "    # Ikke brukt AI\n",
    "    def test_forward_output_example(self):\n",
    "        softb = layers.Softmax()\n",
    "        example_input = np.array(\n",
    "            [[-4.1, 2.2, 3.0, -0.1], [0.1, 0.1, 1.2, 0.3], [-3.6, -1.1, 3.9, -0.1]]\n",
    "        )\n",
    "        output = softb.forward(example_input)\n",
    "        expected_output = tf.nn.softmax(example_input)\n",
    "        self.assertTrue(np.allclose(output, expected_output, atol=1e-5))\n",
    "\n",
    "    def test_backward_shape(self):\n",
    "        self.softmax.forward(self.input_data)  # Forward pass to set up for backward\n",
    "        grad_input = self.softmax.backward(self.grad_output)\n",
    "        self.assertEqual(grad_input.shape, self.input_data.shape)\n",
    "\n",
    "    def test_gradients(self):\n",
    "        self.softmax.forward(self.input_data)\n",
    "        grad_input = self.softmax.backward(self.grad_output)\n",
    "        # This is a basic check. In practice, you might want to check the correctness of the gradient values more thoroughly.\n",
    "        self.assertFalse(np.array_equal(grad_input, np.zeros_like(grad_input)))\n",
    "\n",
    "\n",
    "class TestCrossEntropyLayer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.cross_entropy = layers.CrossEntropy()\n",
    "        self.r = 5\n",
    "        self.m = 2\n",
    "        d = 10\n",
    "        k = 5\n",
    "        p = 15\n",
    "        n_max = 2 * self.r - 1\n",
    "        num_of_samples = 250\n",
    "        num_train_batches = 10\n",
    "        num_test_batches = 1\n",
    "        data = get_train_test_sorting(\n",
    "            self.r, self.m, num_of_samples, num_train_batches, num_test_batches\n",
    "        )\n",
    "        self.x = data[\"x_train\"]\n",
    "        self.y = data[\"y_train\"]\n",
    "        feed_forward1 = layers.FeedForward(d, p)\n",
    "        attention1 = layers.Attention(d, k)\n",
    "        embed_pos = layers.EmbedPosition(n_max, self.m, d)\n",
    "        un_embed_pos = layers.LinearLayer(d, self.m)\n",
    "        softmax = layers.Softmax()\n",
    "        layers_ = [\n",
    "            embed_pos,\n",
    "            attention1,\n",
    "            feed_forward1,\n",
    "            un_embed_pos,\n",
    "            softmax,\n",
    "        ]\n",
    "        self.neuralnet = NeuralNetwork(layers_)\n",
    "\n",
    "    def test_backward_shape(self):\n",
    "        X_batch = onehot(self.x[0], self.m)\n",
    "        Z = self.neuralnet.forward(X_batch)\n",
    "        self.cross_entropy.forward(Z, self.y[0][:, -self.r :])\n",
    "        grad = self.cross_entropy.backward()\n",
    "        # Ensure gradient shape matches the input x shape\n",
    "        self.assertEqual(grad.shape, Z.shape)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
