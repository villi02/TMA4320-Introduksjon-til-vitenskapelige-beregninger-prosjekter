{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test om koden er riktig implementert\n",
    "\n",
    "Her er et forslag til testfunksjoner for Ã¥ sjekke om koden er riktig implementert.\n",
    "```assert variabel``` vil gi en feilmelding med mindre variabelen ```variabel = True```. For eksempel vil ```assert a == b``` gi en feilmelding med mindre ```a``` og ```b``` er like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "attention = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)\n",
    "\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test the forward pass\n",
    "x = np.random.randint(0, m, (b,n_max))\n",
    "X = onehot(x, m)\n",
    "\n",
    "#we test with a y that is shorter than the maximum length\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0, m, (b,n_y))\n",
    "\n",
    "#initialize a neural network based on the layers above\n",
    "network = NeuralNetwork([embed_pos, feed_forward, attention, un_embed, softmax])\n",
    "#and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#do a forward pass\n",
    "Z = network.forward(X)\n",
    "\n",
    "#compute the loss\n",
    "L = loss.forward(Z, y)\n",
    "\n",
    "#get the derivative of the loss wrt Z\n",
    "grad_Z = loss.backward()\n",
    "#and perform a backward pass\n",
    "_ = network.backward(grad_Z)\n",
    "\n",
    "#and and do a gradient descent step\n",
    "_ = network.step_gd(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere you may add additional tests to for example:\\n- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\\n- Check if the parameters change when you perform a gradient descent step\\n- Check if the loss decreases when you perform a gradient descent step\\n\\nThis is voluntary, but could be useful.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hei\n",
      "loss for iteration 1: 1.4853735551144283\n",
      "hei\n",
      "loss for iteration 2: 1.4854363683402017\n",
      "hei\n",
      "loss for iteration 3: 1.4854991317376305\n",
      "hei\n",
      "loss for iteration 4: 1.4855625183520773\n",
      "hei\n",
      "loss for iteration 5: 1.4856268337603016\n",
      "hei\n",
      "loss for iteration 6: 1.4856920485708702\n",
      "hei\n",
      "loss for iteration 7: 1.485758188635956\n",
      "hei\n",
      "loss for iteration 8: 1.485825297837857\n",
      "hei\n",
      "loss for iteration 9: 1.4858934592068453\n",
      "hei\n",
      "loss for iteration 10: 1.4859628108860254\n",
      "hei\n",
      "loss for iteration 11: 1.486033466912036\n",
      "hei\n",
      "loss for iteration 12: 1.4861054145954573\n",
      "hei\n",
      "loss for iteration 13: 1.4861788372739861\n",
      "hei\n",
      "loss for iteration 14: 1.4862535181385124\n",
      "hei\n",
      "loss for iteration 15: 1.4863293157000743\n",
      "hei\n",
      "loss for iteration 16: 1.4864064657250542\n",
      "hei\n",
      "loss for iteration 17: 1.4864849693365905\n",
      "hei\n",
      "loss for iteration 18: 1.4865648833481167\n",
      "hei\n",
      "loss for iteration 19: 1.4866464036045477\n",
      "hei\n",
      "loss for iteration 20: 1.4867293626804166\n",
      "hei\n",
      "loss for iteration 21: 1.486813908170388\n",
      "hei\n",
      "loss for iteration 22: 1.4869000976767173\n",
      "hei\n",
      "loss for iteration 23: 1.4869879805538977\n",
      "hei\n",
      "loss for iteration 24: 1.4870774500279949\n",
      "hei\n",
      "loss for iteration 25: 1.4871682464881897\n",
      "hei\n",
      "loss for iteration 26: 1.4872613665222725\n",
      "hei\n",
      "loss for iteration 27: 1.487356513300514\n",
      "hei\n",
      "loss for iteration 28: 1.4874529657711806\n",
      "hei\n",
      "loss for iteration 29: 1.4875511984131227\n",
      "hei\n",
      "loss for iteration 30: 1.48765103454549\n",
      "hei\n",
      "loss for iteration 31: 1.4877525419806374\n",
      "hei\n",
      "loss for iteration 32: 1.4878560744903329\n",
      "hei\n",
      "loss for iteration 33: 1.4879616032177898\n",
      "hei\n",
      "loss for iteration 34: 1.4880683927558576\n",
      "hei\n",
      "loss for iteration 35: 1.4881766943870012\n",
      "hei\n",
      "loss for iteration 36: 1.4882865586508818\n",
      "hei\n",
      "loss for iteration 37: 1.4883976155229992\n",
      "hei\n",
      "loss for iteration 38: 1.4885098605353084\n",
      "hei\n",
      "loss for iteration 39: 1.488622807549645\n",
      "hei\n",
      "loss for iteration 40: 1.4887356774279197\n",
      "hei\n",
      "loss for iteration 41: 1.4888495208025092\n",
      "hei\n",
      "loss for iteration 42: 1.4889659671421789\n",
      "hei\n",
      "loss for iteration 43: 1.4890843787650845\n",
      "hei\n",
      "loss for iteration 44: 1.489203980973366\n",
      "hei\n",
      "loss for iteration 45: 1.4893241963723245\n",
      "hei\n",
      "loss for iteration 46: 1.4894458408794946\n",
      "hei\n",
      "loss for iteration 47: 1.4895707866203918\n",
      "hei\n",
      "loss for iteration 48: 1.4896994017615852\n",
      "hei\n",
      "loss for iteration 49: 1.4898295698691217\n",
      "hei\n",
      "loss for iteration 50: 1.4899622220774835\n",
      "hei\n",
      "loss for iteration 51: 1.4900968536202275\n",
      "hei\n",
      "loss for iteration 52: 1.4902316851283326\n",
      "hei\n",
      "loss for iteration 53: 1.4903683268772816\n",
      "hei\n",
      "loss for iteration 54: 1.4905053734944416\n",
      "hei\n",
      "loss for iteration 55: 1.4906437549266118\n",
      "hei\n",
      "loss for iteration 56: 1.4907837655327003\n",
      "hei\n",
      "loss for iteration 57: 1.4909212665503557\n",
      "hei\n",
      "loss for iteration 58: 1.4910510844771587\n",
      "hei\n",
      "loss for iteration 59: 1.491173177442911\n",
      "hei\n",
      "loss for iteration 60: 1.4912806146512239\n",
      "hei\n",
      "loss for iteration 61: 1.491372716828064\n",
      "hei\n",
      "loss for iteration 62: 1.4914460861448755\n",
      "hei\n",
      "loss for iteration 63: 1.4914906377023407\n",
      "hei\n",
      "loss for iteration 64: 1.4915025148129037\n",
      "hei\n",
      "loss for iteration 65: 1.4914755023586725\n",
      "hei\n",
      "loss for iteration 66: 1.4914104312477114\n",
      "hei\n",
      "loss for iteration 67: 1.4912879520672417\n",
      "hei\n",
      "loss for iteration 68: 1.491085684537367\n",
      "hei\n",
      "loss for iteration 69: 1.490803636190316\n",
      "hei\n",
      "loss for iteration 70: 1.4904431247330243\n",
      "hei\n",
      "loss for iteration 71: 1.489990272040545\n",
      "hei\n",
      "loss for iteration 72: 1.4894511779373159\n",
      "hei\n",
      "loss for iteration 73: 1.489007486430879\n",
      "hei\n",
      "loss for iteration 74: 1.4893230408365858\n",
      "hei\n",
      "loss for iteration 75: 1.4935172333063285\n",
      "hei\n",
      "loss for iteration 76: 1.4946141345916903\n",
      "hei\n",
      "loss for iteration 77: 1.496131245576469\n",
      "hei\n",
      "loss for iteration 78: 1.4977600881116329\n",
      "hei\n",
      "loss for iteration 79: 1.499591070125969\n",
      "hei\n",
      "loss for iteration 80: 1.5016499640559442\n",
      "hei\n",
      "loss for iteration 81: 1.5040148204827055\n",
      "hei\n",
      "loss for iteration 82: 1.5067912467977693\n",
      "hei\n",
      "loss for iteration 83: 1.5101025459723267\n",
      "hei\n",
      "loss for iteration 84: 1.5141189694910295\n",
      "hei\n",
      "loss for iteration 85: 1.5190790034989792\n",
      "hei\n",
      "loss for iteration 86: 1.5253158075027662\n",
      "hei\n",
      "loss for iteration 87: 1.5333296008814101\n",
      "hei\n",
      "loss for iteration 88: 1.5439397872920093\n",
      "hei\n",
      "loss for iteration 89: 1.558509930597396\n",
      "hei\n",
      "loss for iteration 90: 1.5794295970513137\n",
      "hei\n",
      "loss for iteration 91: 1.6113291381443062\n",
      "hei\n",
      "loss for iteration 92: 1.6644260774220097\n",
      "hei\n",
      "loss for iteration 93: 1.766961943379417\n",
      "hei\n",
      "loss for iteration 94: 2.034424754978505\n",
      "hei\n",
      "loss for iteration 95: 3.7792755398276436\n",
      "hei\n",
      "loss for iteration 96: nan\n",
      "hei\n",
      "loss for iteration 97: nan\n",
      "hei\n",
      "loss for iteration 98: nan\n",
      "hei\n",
      "loss for iteration 99: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tryme\\Documents\\GitHub\\TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter\\Prosjekt2-IndustriellMatematikk\\layers.py:137: RuntimeWarning: overflow encountered in exp\n",
      "  self.P = np.exp(x)\n",
      "c:\\Users\\tryme\\Documents\\GitHub\\TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter\\Prosjekt2-IndustriellMatematikk\\layers.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  self.Z = self.P/(Q+self.epsilon)\n",
      "c:\\Users\\tryme\\Documents\\GitHub\\TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter\\Prosjekt2-IndustriellMatematikk\\layers.py:178: RuntimeWarning: divide by zero encountered in log\n",
      "  self.Q = -np.log(self.P)\n",
      "c:\\Users\\tryme\\Documents\\GitHub\\TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter\\Prosjekt2-IndustriellMatematikk\\layers.py:148: RuntimeWarning: overflow encountered in exp\n",
      "  P = np.exp(self.x)\n",
      "c:\\Users\\tryme\\Documents\\GitHub\\TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter\\Prosjekt2-IndustriellMatematikk\\layers.py:150: RuntimeWarning: overflow encountered in multiply\n",
      "  S = P/(Q*Q+self.epsilon)\n",
      "c:\\Users\\tryme\\Documents\\GitHub\\TMA4320-Introduksjon-til-vitenskapelige-beregninger-prosjekter\\Prosjekt2-IndustriellMatematikk\\layers.py:150: RuntimeWarning: invalid value encountered in divide\n",
      "  S = P/(Q*Q+self.epsilon)\n"
     ]
    }
   ],
   "source": [
    "# feed_forward1 = FeedForward(d,p)\n",
    "# attention1 = Attention(d,k)\n",
    "# embed_pos = EmbedPosition(n_max,m,d)\n",
    "# un_embed_pos = LinearLayer(d,m)\n",
    "# softmax = Softmax()\n",
    "# layers = [embed_pos,attention1,feed_forward1,un_embed_pos, softmax]\n",
    "# nueralnet = NeuralNetwork(layers)\n",
    "\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "x = np.random.randint(0, m, (b,n_max))\n",
    "y = np.random.randint(0, m, (b,n_max-1))\n",
    "\n",
    "def algorithm_4(x, y, m , d, p, k, n_max):\n",
    "    n_iter = 100\n",
    "    loss = CrossEntropy()\n",
    "    L_arr = np.zeros(n_iter)\n",
    "    feed_forward1 = FeedForward(d,p)\n",
    "    attention1 = Attention(d,k)\n",
    "    embed_pos = EmbedPosition(n_max,m,d)\n",
    "    un_embed_pos = LinearLayer(d,m)\n",
    "    softmax = Softmax()\n",
    "    layers = [embed_pos, attention1,feed_forward1, un_embed_pos, softmax]\n",
    "    nueralnet = NeuralNetwork(layers)\n",
    "\n",
    "    alpha = 0.001\n",
    "    Losses = []\n",
    "    for j in range(1,n_iter):\n",
    "        X = onehot(x, m)\n",
    "        Z = nueralnet.forward(X) \n",
    "        Losses.append(loss.forward(Z,y))\n",
    "        dLdz = loss.backward()\n",
    "        nueralnet.backward(dLdz) \n",
    "        layers[0].step_gd(alpha)\n",
    "        layers[1].step_adam(j, alpha)\n",
    "        layers[2].step_gd(alpha)\n",
    "        layers[3].step_adam(j, alpha)\n",
    "        print(f'loss for iteration {j}: {loss.forward(Z,y)}') \n",
    "    return Losses   \n",
    "\n",
    "arr = algorithm_4(x, y, m , d, p, k, n_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,100,99),np.log(arr))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
